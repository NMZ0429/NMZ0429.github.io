{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome !! \u81ea\u5df1\u7d39\u4ecb\u3092\u66f8\u304f\u307b\u3069\u306e\u4eba\u9593\u3058\u3083\u306a\u3044\u3093\u3060\u3088\u306a\u3041 \u4f55\u30b3\u30ec \u5099\u5fd8\u9332 \u30b5\u30a4\u30c8\u69cb\u7bc9\u72b6\u6cc1 Success \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0 \u76ee\u6b21 \u691c\u7d22 GH action Attention Google analytics \u30d5\u30c3\u30bf\u30fc\u3000\u3044\u308b\u304b\uff1f \u30d5\u30a1\u30f4\u30a3\u30b3\u30f3 Vscode\u9023\u643a","title":"<i class=\"fa fa-arrow-circle-right\" aria-hidden=\"true\"></i> Welcome !!"},{"location":"#welcome","text":"\u81ea\u5df1\u7d39\u4ecb\u3092\u66f8\u304f\u307b\u3069\u306e\u4eba\u9593\u3058\u3083\u306a\u3044\u3093\u3060\u3088\u306a\u3041","title":" Welcome !!"},{"location":"#_1","text":"\u5099\u5fd8\u9332","title":"\u4f55\u30b3\u30ec"},{"location":"#_2","text":"Success \u30eb\u30fc\u30c6\u30a3\u30f3\u30b0 \u76ee\u6b21 \u691c\u7d22 GH action Attention Google analytics \u30d5\u30c3\u30bf\u30fc\u3000\u3044\u308b\u304b\uff1f \u30d5\u30a1\u30f4\u30a3\u30b3\u30f3 Vscode\u9023\u643a","title":"\u30b5\u30a4\u30c8\u69cb\u7bc9\u72b6\u6cc1"},{"location":"intro/","text":"Artificial Intelligence\u3092\u5b66\u3076\u65b9\u3078 AI\u306f\u69d8\u3005\u306a\u5206\u91ce\u306b\u307e\u305f\u304c\u3063\u3066\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5168\u4f53\u50cf\u3092\u4fef\u77b0\u3059\u308b\u306e\u304c\u96e3\u3057\u3044\u3002 \u4ee5\u4e0bpdf\u3067AI\u304c\u3069\u306e\u69d8\u306a\u5206\u91ce\u3067\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u304b\u306a\u3093\u3068\u306a\u304f\u308f\u304b\u308b\u304b\u3082 Conference map \u3082\u3063\u3068\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u65b9\u306f\u4e0b\u8a18\u306e\u30b9\u30e9\u30a4\u30c9\u3092\u4e00\u77a5\u3057\u3066\u307f\u308b\u3068\u3044\u3044\u3067\u3057\u3087\u3046\u3001\u3053\u3061\u3089\u306f\u4e00\u822c\u793e\u56e3\u6cd5\u4eba\u4eba\u5de5\u77e5\u80fd\u5b66\u4f1a\u304c\u4f5c\u6210\u3057\u305fAI\u7814\u7a76\u306e\u4fef\u77b0\u56f3\u3068\u305d\u306e\u8a73\u7d30\u3067\u3059\u3002 AI\u7814\u7a76\u521d\u5b66\u8005\u3068\u7570\u5206\u91ce\u7814\u7a76\u8005\u306e\u305f\u3081\u306e AI\u7814\u7a76\u306e\u4fef\u77b0\u56f3 \u6a5f\u68b0\u5b66\u7fd2\u306e\u5b66\u7fd2\u306e\u3059\u309d\u3081 \u73fe\u5728\u306e\u4e3b\u6d41\u3067\u3042\u308b \u6df1\u5c64\u5b66\u7fd2\uff08\u7279\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09 \u3092\u5b66\u3076\u524d\u306b\u7d71\u8a08\u30d9\u30fc\u30b9\u306e\u975e\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4e00\u901a\u308a\u5b66\u3076\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u306f\u7406\u8ad6\u304c\u6bd4\u8f03\u7684\u7c21\u5358\u306a\u306e\u3067\u521d\u5b66\u306b\u5411\u304d\u307e\u3059\u3057\u3001\u305d\u308c\u3092\u77e5\u3063\u305f\u4e0a\u3067\u6df1\u5c64\u5b66\u7fd2\u304c\u5fc5\u8981\u306b\u306a\u3063\u305f\u7d4c\u7def\u3092\u77e5\u308b\u3068\u8272\u3005\u3068\u7d0d\u5f97\u3067\u304d\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \u3053\u3061\u3089\u306e\u30b5\u30a4\u30c8\u3067\u6709\u540d\u30e2\u30c7\u30eb\u3078\u306e\u3042\u308b\u7a0b\u5ea6\u306e\u7406\u89e3\u304c\u3067\u304d\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \u9023\u8f09 Python\u3067\u5b66\u3076 \u57fa\u790e\u304b\u3089\u306e\u6a5f\u68b0\u5b66\u7fd2\u5165\u9580 Python + \u5fc5\u8981\u306a\u30b9\u30ad\u30eb\u306e\u5b66\u7fd2 \u30c8\u30d4\u30c3\u30af\u6bce\u306e100\u672c\u30ce\u30c3\u30af\u3092\u3059\u308b\u306e\u304c\u304a\u3059\u3059\u3081 \u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af \u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af 2020: https://nlp100.github.io/ja/ \u2190 NEW! \u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af 2015: http://www.cl.ecei.tohoku.ac.jp/nlp100/ \u2192 \u7b54\u3048\u3068\u89e3\u8aac \u8a00\u8a9e\u51e6\u7406\u3060\u3051\u3067\u306a\u304f\u4e00\u90e8UNIX\u57fa\u672c, \u6a5f\u68b0\u5b66\u7fd2\u3082\u542b\u3080 \u95a2\u9023: \u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u3092\u89e3\u304d\u59cb\u3081\u308b\u524d\u306b \u753b\u50cf\u51e6\u7406 100\u672c\u30ce\u30c3\u30af https://github.com/yoyoyo-yo/Gasyori100knock \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0 \u221e\u672c\u30ce\u30c3\u30af https://github.com/yoyoyo-yo/DeepLearningMugenKnock numpy100\u672c\u30ce\u30c3\u30af https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises.md pandas100\u672c\u30ce\u30c3\u30af https://www.kaggle.com/python10pm/pandas-100-tricks \u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9100\u672c\u30ce\u30c3\u30af https://github.com/The-Japan-DataScientist-Society/100knocks-preprocess YouTube\u30b3\u30f3\u30c6\u30f3\u30c4 \u52d5\u753b\u3067\u6982\u8981\u3092\u63b4\u3081\u308b\u30ca\u30a4\u30b9\u306a\u30c1\u30e3\u30f3\u30cd\u30eb\u7b49 Sony Neural Network Console Deep Learning \u5165\u9580\u518d\u751f\u30ea\u30b9\u30c8 Deep Learning\u5165\u9580\uff1aDeep Learning\u3067\u3067\u304d\u308b\u3053\u3068 Deep Learning\u5165\u9580\uff1a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u8a08\u306e\u57fa\u790e Alcia Solid Project \u30b9\u30bf\u30d3\u3058by\u30a6\u30de\u305f\u3093","title":"Artificial Intelligence\u3092\u5b66\u3076\u65b9\u3078"},{"location":"intro/#artificial-intelligence","text":"AI\u306f\u69d8\u3005\u306a\u5206\u91ce\u306b\u307e\u305f\u304c\u3063\u3066\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u5168\u4f53\u50cf\u3092\u4fef\u77b0\u3059\u308b\u306e\u304c\u96e3\u3057\u3044\u3002 \u4ee5\u4e0bpdf\u3067AI\u304c\u3069\u306e\u69d8\u306a\u5206\u91ce\u3067\u7814\u7a76\u3055\u308c\u3066\u3044\u308b\u304b\u306a\u3093\u3068\u306a\u304f\u308f\u304b\u308b\u304b\u3082 Conference map \u3082\u3063\u3068\u8a73\u3057\u304f\u77e5\u308a\u305f\u3044\u65b9\u306f\u4e0b\u8a18\u306e\u30b9\u30e9\u30a4\u30c9\u3092\u4e00\u77a5\u3057\u3066\u307f\u308b\u3068\u3044\u3044\u3067\u3057\u3087\u3046\u3001\u3053\u3061\u3089\u306f\u4e00\u822c\u793e\u56e3\u6cd5\u4eba\u4eba\u5de5\u77e5\u80fd\u5b66\u4f1a\u304c\u4f5c\u6210\u3057\u305fAI\u7814\u7a76\u306e\u4fef\u77b0\u56f3\u3068\u305d\u306e\u8a73\u7d30\u3067\u3059\u3002 AI\u7814\u7a76\u521d\u5b66\u8005\u3068\u7570\u5206\u91ce\u7814\u7a76\u8005\u306e\u305f\u3081\u306e AI\u7814\u7a76\u306e\u4fef\u77b0\u56f3","title":"Artificial Intelligence\u3092\u5b66\u3076\u65b9\u3078"},{"location":"intro/#_1","text":"\u73fe\u5728\u306e\u4e3b\u6d41\u3067\u3042\u308b \u6df1\u5c64\u5b66\u7fd2\uff08\u7279\u306b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\uff09 \u3092\u5b66\u3076\u524d\u306b\u7d71\u8a08\u30d9\u30fc\u30b9\u306e\u975e\u6df1\u5c64\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4e00\u901a\u308a\u5b66\u3076\u3053\u3068\u3092\u304a\u52e7\u3081\u3057\u307e\u3059\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u306f\u7406\u8ad6\u304c\u6bd4\u8f03\u7684\u7c21\u5358\u306a\u306e\u3067\u521d\u5b66\u306b\u5411\u304d\u307e\u3059\u3057\u3001\u305d\u308c\u3092\u77e5\u3063\u305f\u4e0a\u3067\u6df1\u5c64\u5b66\u7fd2\u304c\u5fc5\u8981\u306b\u306a\u3063\u305f\u7d4c\u7def\u3092\u77e5\u308b\u3068\u8272\u3005\u3068\u7d0d\u5f97\u3067\u304d\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \u3053\u3061\u3089\u306e\u30b5\u30a4\u30c8\u3067\u6709\u540d\u30e2\u30c7\u30eb\u3078\u306e\u3042\u308b\u7a0b\u5ea6\u306e\u7406\u89e3\u304c\u3067\u304d\u308b\u3068\u601d\u3044\u307e\u3059\u3002 \u9023\u8f09 Python\u3067\u5b66\u3076 \u57fa\u790e\u304b\u3089\u306e\u6a5f\u68b0\u5b66\u7fd2\u5165\u9580","title":"\u6a5f\u68b0\u5b66\u7fd2\u306e\u5b66\u7fd2\u306e\u3059\u309d\u3081"},{"location":"intro/#python","text":"\u30c8\u30d4\u30c3\u30af\u6bce\u306e100\u672c\u30ce\u30c3\u30af\u3092\u3059\u308b\u306e\u304c\u304a\u3059\u3059\u3081","title":"Python + \u5fc5\u8981\u306a\u30b9\u30ad\u30eb\u306e\u5b66\u7fd2"},{"location":"intro/#100","text":"\u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af 2020: https://nlp100.github.io/ja/ \u2190 NEW! \u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af 2015: http://www.cl.ecei.tohoku.ac.jp/nlp100/ \u2192 \u7b54\u3048\u3068\u89e3\u8aac \u8a00\u8a9e\u51e6\u7406\u3060\u3051\u3067\u306a\u304f\u4e00\u90e8UNIX\u57fa\u672c, \u6a5f\u68b0\u5b66\u7fd2\u3082\u542b\u3080 \u95a2\u9023: \u8a00\u8a9e\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u3092\u89e3\u304d\u59cb\u3081\u308b\u524d\u306b","title":"\u8a00\u8a9e\u51e6\u7406 100\u672c\u30ce\u30c3\u30af"},{"location":"intro/#100_1","text":"https://github.com/yoyoyo-yo/Gasyori100knock","title":"\u753b\u50cf\u51e6\u7406 100\u672c\u30ce\u30c3\u30af"},{"location":"intro/#_2","text":"https://github.com/yoyoyo-yo/DeepLearningMugenKnock","title":"\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0 \u221e\u672c\u30ce\u30c3\u30af"},{"location":"intro/#numpy100","text":"https://github.com/rougier/numpy-100/blob/master/100_Numpy_exercises.md","title":"numpy100\u672c\u30ce\u30c3\u30af"},{"location":"intro/#pandas100","text":"https://www.kaggle.com/python10pm/pandas-100-tricks","title":"pandas100\u672c\u30ce\u30c3\u30af"},{"location":"intro/#100_2","text":"https://github.com/The-Japan-DataScientist-Society/100knocks-preprocess","title":"\u30c7\u30fc\u30bf\u30b5\u30a4\u30a8\u30f3\u30b9100\u672c\u30ce\u30c3\u30af"},{"location":"intro/#youtube","text":"\u52d5\u753b\u3067\u6982\u8981\u3092\u63b4\u3081\u308b\u30ca\u30a4\u30b9\u306a\u30c1\u30e3\u30f3\u30cd\u30eb\u7b49 Sony Neural Network Console Deep Learning \u5165\u9580\u518d\u751f\u30ea\u30b9\u30c8 Deep Learning\u5165\u9580\uff1aDeep Learning\u3067\u3067\u304d\u308b\u3053\u3068 Deep Learning\u5165\u9580\uff1a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u8a08\u306e\u57fa\u790e Alcia Solid Project \u30b9\u30bf\u30d3\u3058by\u30a6\u30de\u305f\u3093","title":"YouTube\u30b3\u30f3\u30c6\u30f3\u30c4"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/","text":"\u753b\u50cf\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u7b54\u3048 Q1. \u30c1\u30e3\u30f3\u30cd\u30eb\u5165\u308c\u66ff\u3048 \u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\u3001BGR\u304b\u3089RGB\u3078\u3068\u5909\u63db\u3002 \u89e3\u7b54 import cv2 img = cv2.imread(\"./img/imori.jpeg\") rgb_img = img[:, :, [2,1,0]].copy() numpy\u914d\u5217\u306e\u64cd\u4f5c\u3067\u5909\u63db\u3067\u304d\u308b\u3002 cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \u3067\u5909\u63db\u53ef\u80fd Q2. \u753b\u50cf\u306e\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5316 Y= 0.2126 R + 0.7152 G + 0.0722 B\u3001\u3067\u8868\u3055\u308c\u308b\u3002 \u89e3\u7b54 img = cv2.imread(\"./img/imori.jpeg\") gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 opencv\u306e\u6a5f\u80fd\u3067 cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \u3067\u5909\u63db\u3067\u304d\u308b\u3002 \u884c\u5217\u8a08\u7b97\u3067\u3082\u51fa\u6765\u305d\u3046\u3002 Q3. \uff12\u5024\u5316 \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u753b\u50cf\u306e\uff12\u5024\u5316\u3002\u95be\u5024\u306f128 \u89e3\u7b54 img = cv2.imread(\"./img/imori.jpeg\") gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 ret, gray_img = cv2.threshold(gray_img,128,255,cv2.THRESH_BINARY) \u4e00\u500b\u4e00\u500b\u6761\u4ef6\u5206\u5c90\u3055\u305b\u308b\u3068\u8a08\u7b97\u91cf\u304c\u591a\u305d\u3046\u3002 Q4. \u5927\u6d25\u306e\u4e8c\u5024\u5316 \u4e8c\u5024\u5316\u306b\u304a\u3051\u308b\u5206\u96e2\u306e\u95be\u5024\u3092\u81ea\u52d5\u6c7a\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002 \u30af\u30e9\u30b9\u9593\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308c\u3070\u826f\u3044\u3002 ret, gray_img = cv2.threshold(gray_img,0,255,cv2.THRESH_OTSU) \u3067\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u3002 \u4e8b\u524d\u306e gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 \u304c\u539f\u56e0\u3002gray_img\u304cfloat\u3060\u3068\u3046\u307e\u304f\u3044\u304b\u306a\u3044\u3002 \u89e3\u7b54 gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) ret, th = cv2.threshold(gray_img, 0, 255, cv2.THRESH_OTSU) Q5. HSV\u5909\u63db HSV\u5909\u63db\u3068\u306f\u3001Hue(\u8272\u76f8)\u3001Saturation(\u5f69\u5ea6)\u3001Value(\u660e\u5ea6) \u3067\u8272\u3092\u8868\u73fe\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002 hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) hsv_img[:,:,0] = (hsv_img[:,:,0] + 180) % 360 hsv_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR) \u82e5\u5e72\u8272\u304c\u9055\u3046\u6c17\u304c\u3059\u308b\u3002 Hue\u306e\u7bc4\u56f2\u304c[0:179]\u3089\u3057\u3044\u3002 hsv[:,:,0] = (hsv[:,:,0] + 90) % 180 \u307e\u3060\u9055\u3046\u3002 opencv\u3092\u4f7f\u308f\u305a\u306b\u5b9f\u88c5\u3057\u305f\u3002 \u89e3\u7b54 import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") h,w,c = img.shape hsv = img / 255 for i in range(w): for j in range(h): b,g,r = img[i,j,:] / 255 max_val = max(b,g,r) min_val = min(b,g,r) val = max_val sat = max_val - min_val if max_val == min_val: hue = 0 elif min_val == b: hue = 60 * (g-r) / sat + 60 elif min_val == r: hue = 60 * (b-g) / sat + 180 else: hue = 60 * (r-b) / sat + 300 # print(hsv[i,j,:]) hsv[i,j,:] = [hue,sat,val] # print(hsv[i,j,:]) cv2.imwrite(\"./img/hsv_moto.jpeg\",hsv) hsv[:,:,0] = (hsv[:,:,0] + 180) % 360 revers_img = img/255 for i in range(w): for j in range(h): hue,sat,val = hsv[i,j,:] c = sat h_dot = hue / 60 x = c * (1 - abs(h_dot % 2 - 1)) if (0 <= h_dot) & (h_dot < 1): add_h = [c,x,0] elif (1<= h_dot) & (h_dot < 2): add_h = [x,c,0] elif (2<= h_dot) & (h_dot < 3): add_h = [0,c,x] elif (3<= h_dot) & (h_dot < 4): add_h = [0,x,c] elif (4<= h_dot) & (h_dot < 5): add_h = [x,0,c] elif (5<= h_dot) & (h_dot < 6): add_h = [c,0,x] else: add_h = [0,0,0] revers_img[i,j,:] = np.multiply([1,1,1], (val - c)) + add_h revers_img = revers_img * 255 revers_img = revers_img[:, :, [2,1,0]] cv2.imwrite(\"./img/hsv.jpeg\",revers_img) height,width,channel\u306e\u9806\u5e8f\u3092\u3082\u3063\u3068\u610f\u8b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u3082\u3063\u3068\u30b3\u30fc\u30c9\u3092\u77ed\u304f\u3067\u304d\u308b\u3068\u826f\u3044\u3068\u601d\u3046\u3002if\u306e\u5206\u5c90\u304c\u591a\u3059\u304e\u308b\u3002 Q6. \u6e1b\u8272\u51e6\u7406 \u3059\u306a\u308f\u3061R,G,B in {32, 96, 160, 224}\u306e\u54044\u5024\u306b\u6e1b\u8272\u305b\u3088\u3002 \u89e3\u7b54 img = cv2.imread(\"./img/imori.jpeg\") img = (img // 64 + 1) * 64 - 32 cv2.imwrite(\"./img/result_img.jpeg\",img) if\u6587\u3092\u4f7f\u308f\u305a\u306b\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002 Q7. \u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0 \u753b\u50cf\u3092\u30b0\u30ea\u30c3\u30c9\u5206\u5272(\u3042\u308b\u56fa\u5b9a\u9577\u306e\u9818\u57df\u306b\u5206\u3051\u308b)\u3057\u3001\u304b\u304f\u9818\u57df\u5185(\u30bb\u30eb)\u306e\u5e73\u5747\u5024\u3067\u305d\u306e\u9818\u57df\u5185\u306e\u5024\u3092\u57cb\u3081\u308b\u3002 imori.jpg\u306f128x128\u306a\u306e\u3067\u30018x8\u306b\u30b0\u30ea\u30c3\u30c9\u5206\u5272\u3057\u3001\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u305b\u3088\u3002 \u89e3\u7b54 import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") def average_pooling(img,karnel): pool_img = img.copy() height,width,channel = img.shape for i in range(0,height,karnel[0]): for j in range(0,width,karnel[1]): ave = np.mean(img[i:i+karnel[0],j:j+karnel[1],:],axis = 0) ave = np.mean(ave,axis = 0) pool_img[i:i+karnel[0],j:j+karnel[1],:] = ave return pool_img kar = (8,8) img = average_pooling(img,kar) cv2.imwrite(\"./img/pool_img.jpeg\",img) for\u6587\u30922\u56de\u4f7f\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002\u3053\u308c\u4ee5\u4e0a\u6e1b\u3089\u3059\u65b9\u6cd5\u304c\u601d\u3044\u3064\u304b\u306a\u304b\u3063\u305f\u3002 \u307e\u305f\u884c\u3068\u5217\u306e\u5e73\u5747\u30922\u56de\u306b\u5206\u3051\u3066\u6c42\u3081\u3066\u308b\u306e\u3067\u3053\u308c\u3092\u4e00\u5ea6\u306b\u3067\u304d\u308b\u65b9\u6cd5\u304c\u3042\u308c\u3070\u3088\u304b\u3063\u305f\u3002 def average_pooling(img,karnel) img\u306f\u753b\u50cf\u3001karnel\u306f\u30b0\u30ea\u30c3\u30c9\u306e\u5206\u5272\u306e\u7bc4\u56f2 Q8. Max\u30d7\u30fc\u30ea\u30f3\u30b0 \u3053\u3053\u3067\u306f\u5e73\u5747\u5024\u3067\u306a\u304f\u6700\u5927\u5024\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\u305b\u3088\u3002 \u89e3\u7b54 import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") def average_pooling(img,karnel): pool_img = img.copy() height,width,channel = img.shape for i in range(0,height,karnel[0]): for j in range(0,width,karnel[1]): ave = np.max(img[i:i+karnel[0],j:j+karnel[1],:],axis = 0) ave = np.max(ave,axis = 0) pool_img[i:i+karnel[0],j:j+karnel[1],:] = ave return pool_img kar = (8,8) img = average_pooling(img,kar) cv2.imwrite(\"./img/poolmax_img.jpeg\",img) \u5e73\u5747\u3092\u6c42\u3081\u308b\u90e8\u5206\u3092\u3001\u6700\u5927\u5024\u306b\u5909\u66f4\u3057\u305f\u3002 Q9. \u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf \u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf(3x3\u3001\u6a19\u6e96\u504f\u5dee1.3)\u3092\u5b9f\u88c5\u3057\u3001imori_noise.jpg\u306e\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u305b\u3088\u3002 \u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf\u306f\u6ce8\u76ee\u753b\u7d20\u306e\u5468\u8fba\u753b\u7d20\u3092\u3001\u30ac\u30a6\u30b9\u5206\u5e03\u306b\u3088\u308b\u91cd\u307f\u4ed8\u3051\u3067\u5e73\u6ed1\u5316\u3057\u3001\u6b21\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b\u3002 \u3053\u306e\u3088\u3046\u306a\u91cd\u307f\u306f\u30ab\u30fc\u30cd\u30eb\u3084\u30d5\u30a3\u30eb\u30bf\u3068\u547c\u3070\u308c\u308b\u3002 \u89e3\u7b54 import cv2 import numpy as np img = cv2.imread(\"./img/imori_noise.jpeg\") def gausu_filter(img,karnel,sigma): height,width,channel = img.shape pad = karnel // 2 pad_img = np.zeros((height + pad * 2,width + pad * 2, channel)) pad_img[pad:pad+height,pad:pad+width] = img weight = gausu(sigma,karnel,pad) gausu_img = img.copy() for i in range(height): for j in range(width): gausu_img[i,j,0] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,0]*weight) gausu_img[i,j,1] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,1]*weight) gausu_img[i,j,2] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,2]*weight) print(gausu_img) return gausu_img def gausu(sigma,karnel,pading): filt = np.zeros((karnel,karnel)) for x in range(pading * -1, pading + 1): for y in range(pading * -1, pading + 1): print(x,y) filt[x+pading,y+pading] = 1 / (2*np.pi*sigma*sigma) * np.exp((-1 * (x*x + y*y))/(2 * (sigma**2))) filt /= filt.sum() return filt kar = 3 sig = 1.3 img = gausu_filter(img,kar,sig) cv2.imwrite(\"./img/gausu_img.jpeg\",img) \u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u4f5c\u6210\u306b\u3082\u95a2\u6570\u3092\u7528\u3044\u305f\u3002 Q10. \u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf \u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u3057\u3001imori_noise.jpg\u306e\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u305b\u3088\u3002 \u3053\u308c\u306f\u6ce8\u76ee\u753b\u7d20\u306e3x3\u306e\u9818\u57df\u5185\u306e\u3001\u30e1\u30c7\u30a3\u30a2\u30f3\u5024(\u4e2d\u592e\u5024)\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308b\u3002 \u3053\u308c\u3082\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u305b\u3088\u3002 \u30b5\u30a4\u30c8 \u3092\u53c2\u8003\u306b\u3057\u3066\u3001for\u6587\u3092\u5c11\u306a\u304f\u66f8\u3044\u3066\u307f\u308b\u3002 \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori_noise.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 medhian = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='med') cv2.imwrite(\"./img/medhian_img.jpeg\",medhian) \u53c2\u8003\u306b\u3057\u305f\u7d50\u679cfor\u6587\u3092\u4f7f\u308f\u306a\u3044\u3067\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002 \u3057\u304b\u3057as_strided\u306e\u5f15\u6570strides\u306e\u5f79\u5272\u304c\u3088\u304f\u5206\u304b\u3063\u3066\u3044\u306a\u3044\u3002\u8abf\u3079\u305f\u3068\u3053\u308d\u30e1\u30e2\u30ea\u306e\u79fb\u52d5\u8ddd\u96e2\u306e\u3088\u3046\u3060\u3063\u305f\u3002 strides = (stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) \u306eshape\u304c (390, 3, 1, 390, 3) \u3068\u306a\u3063\u3066\u3044\u308b\u3002\u524d\u534a\u306e(390,3,1)\u306f(height,width,channel)\u3092\u8868\u3057\u3066\u3044\u3066\u3001\u52d5\u304b\u3059(height,width)\u3092\u3082\u3046\u4e00\u5ea6\u8ffd\u52a0\u3057\u3066\u3044\u308b\u306e\u304b\uff1f Q11. \u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf \u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf\u306f\u30d5\u30a3\u30eb\u30bf\u5185\u306e\u753b\u7d20\u306e\u5e73\u5747\u5024\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308b\u3002 \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 mean = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='avg') cv2.imwrite(\"./img/mean_img.jpeg\",mean) \u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf\u306e\u6700\u5f8c\u306e\u90e8\u5206\u3092\u5e73\u5747\u306b\u5909\u3048\u305f\u3060\u3051\u3067\u3042\u308b\u3002 Q12. \u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf \u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf\u3068\u306f\u5bfe\u89d2\u65b9\u5411\u306e\u5e73\u5747\u5024\u3092\u53d6\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u6b21\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b. [[1/3,0,0] [0,1/3,0] [0,0,1/3]] \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 motion = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='motion') print(motion.shape) cv2.imwrite(\"./img/motion_img.jpeg\",motion) \u91cd\u307f\u3092\u4f5c\u6210\u3057\u3066\u305d\u306e\u5024\u3092\u305d\u308c\u305e\u308c\u306b\u304b\u3051\u305f\u3002 Q13. MAX-MIN\u30d5\u30a3\u30eb\u30bf MAX-MIN\u30d5\u30a3\u30eb\u30bf\u3068\u306f\u30d5\u30a3\u30eb\u30bf\u5185\u306e\u753b\u7d20\u306e\u6700\u5927\u5024\u3068\u6700\u5c0f\u5024\u306e\u5dee\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u30a8\u30c3\u30b8\u691c\u51fa\u306e\u30d5\u30a3\u30eb\u30bf\u306e\u4e00\u3064\u3067\u3042\u308b\u3002 \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1) kernel_size = (kernel_size, kernel_size) # print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], ) + A.strides) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'min': return A_w.min(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) elif pool_mode == \"max_min\": max_pool = A_w.max(axis=(1,2)).reshape(output_shape) min_pool = A_w.min(axis=(1,2)).reshape(output_shape) return max_pool - min_pool img = cv2.imread(\"./img/imori.jpeg\") img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) height,width= img.shape karn = 3 padding = karn // 2 max_min = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='max_min') print(max_min.shape) cv2.imwrite(\"./img/min_max_img.jpeg\",max_min) \u4e8b\u524d\u306b\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u305f\u304c\u3001BGR\u753b\u50cf\u3067\u4f5c\u6210\u3057\u305f\u5f8c\u306b\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u3082\u540c\u3058\u306a\u306e\u304b\uff1f Q14. \u5fae\u5206\u30d5\u30a3\u30eb\u30bf \u5fae\u5206\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u5fae\u5206\u30d5\u30a3\u30eb\u30bf\u306f\u8f1d\u5ea6\u306e\u6025\u6fc0\u306a\u5909\u5316\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u90e8\u5206\u306e\u30a8\u30c3\u30b8\u3092\u53d6\u308a\u51fa\u3059\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u96a3\u308a\u5408\u3046\u753b\u7d20\u540c\u58eb\u306e\u5dee\u3092\u53d6\u308b\u3002 \u7e26\u306e\u30d5\u30a3\u30eb\u30bf [[0,0,0] [-1,1,0] [0,0,0]] \u6a2a\u306e\u30d5\u30a3\u30eb\u30bf [[0,-1,0] [0,1,0] [0,0,0]] ### \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) weight_w = [[0,0,0],[-1,1,0],[0,0,0]] weight_h = [[0,-1,0],[0,1,0],[0,0,0]] weight_w = np.array(weight_w).reshape(-1,3,3) weight_h = np.array(weight_h).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'min': return A_w.min(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) elif pool_mode == \"max_min\": max_pool = A_w.max(axis=(1,2)).reshape(output_shape) min_pool = A_w.min(axis=(1,2)).reshape(output_shape) return max_pool - min_pool elif pool_mode == \"diff_w\": return np.sum(A_w*weight_w,axis = (1,2)).reshape(output_shape) elif pool_mode == \"diff_h\": return np.sum(A_w*weight_h,axis = (1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 diff_w_img = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='diff_w') diff_h_img = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='diff_h') cv2.imwrite(\"./img/diff_h_img.jpeg\",diff_h_img) cv2.imwrite(\"./img/diff_w_img.jpeg\",diff_w_img) \u30d5\u30a3\u30eb\u30bf\u3054\u3068\u306b\u5206\u96e2\u3057\u3066\u304a\u3051\u3070\u51e6\u7406\u304c\u7c21\u5358\u306b\u3067\u304d\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\u3002 Q20. \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a matplotlib\u3092\u7528\u3044\u3066imori_dark.jpg\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u8868\u793a\u305b\u3088\u3002 \u89e3\u7b54 import numpy as np import cv2 import matplotlib.pyplot as plt img = cv2.imread(\"./img/imori_dark.jpeg\") gaso = np.array(img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) plt.show() \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306b\u3059\u308b\u3068\u304d\u306f\uff11\u6b21\u5143\u306b\u3057\u306a\u3044\u3068\u51e6\u7406\u3067\u304d\u306a\u3044\u3002 Q21. \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316\u3092\u5b9f\u88c5\u305b\u3088\u3002 [c,d]\u306e\u753b\u7d20\u5024\u3092\u6301\u3064\u753b\u50cf\u3092[a,b]\u306e\u30ec\u30f3\u30b8\u306b\u5909\u63db\u3059\u308b\u3002 \u89e3\u7b54 import numpy as np import cv2 import matplotlib.pyplot as plt def gray_scale_trans(img,a=0,b=255): out = img.copy() c = img.min() d = img.max() out = (b-a)/(d-c)*(out-c)+a np.where(out < a, a, out) np.where(b < out, b, out) return out img = cv2.imread(\"./img/imori_dark.jpeg\") trans_img = gray_scale_trans(img) gaso = np.array(trans_img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) cv2.imwrite(\"./img/trans_img.jpeg\",trans_img) plt.show() np.where\u3092\u7528\u3044\u3066\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316\u3092\u95a2\u6570\u3068\u3057\u3066\u8868\u3057\u305f\u3002 Q.22 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u64cd\u4f5c \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u5e73\u5747\u5024\u3092m0=128\u3001\u6a19\u6e96\u504f\u5dee\u3092s0=52\u306b\u306a\u308b\u3088\u3046\u306b\u64cd\u4f5c\u305b\u3088\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u5909\u66f4\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308b. \u89e3\u7b54 import numpy as np import cv2 import matplotlib.pyplot as plt def hist_heitan(img,m0=128,s0=52): out = img.copy() s = np.std(img) m = np.average(img) out = s0 / s * (out - m) + m0 return out img = cv2.imread(\"./img/imori_dark.jpeg\") trans_img = hist_heitan(img) gaso = np.array(trans_img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) cv2.imwrite(\"./img/trans_img_1.jpeg\",trans_img) plt.show() \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u305f\u3002 Q.23 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316\u3068\u306f\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u5909\u66f4\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308a\u3001\u4e0a\u8a18\u306e\u5e73\u5747\u5024\u3084\u6a19\u6e96\u504f\u5dee\u306a\u3069\u3092\u5fc5\u8981\u3068\u305b\u305a\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5024\u3092\u5747\u8861\u306b\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308b\u3002 import numpy as np import cv2 import matplotlib.pyplot as plt def hist_heitan_function(img, z_max=255): out = img.copy() height, width, channel = img.shape S = height * width * channel sum_h = 0 for i in range(1, 255): ind = np.where(img == i) sum_h += len(img[ind]) z_prime = z_max / S * sum_h out[ind] = z_prime return out img = cv2.imread(\"./img/imori.jpeg\") trans_img = hist_heitan_function(img) gaso = np.array(trans_img).flatten() plt.hist(gaso, bins=255, range=(0, 255), rwidth=0.8) cv2.imwrite(\"./img/trans_img_3.jpeg\", trans_img) plt.show() np.where\u3092\u7528\u3044\u3066for\u6587\u3092\u7121\u304f\u305d\u3046\u3068\u3057\u305f\u304c\u4e0a\u624b\u304f\u884c\u304b\u306a\u304b\u3063\u305f\u3002","title":"\u753b\u50cf\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u7b54\u3048"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#100","text":"","title":"\u753b\u50cf\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u7b54\u3048"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q1","text":"\u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f\u3001BGR\u304b\u3089RGB\u3078\u3068\u5909\u63db\u3002","title":"Q1.\u30c1\u30e3\u30f3\u30cd\u30eb\u5165\u308c\u66ff\u3048"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_1","text":"import cv2 img = cv2.imread(\"./img/imori.jpeg\") rgb_img = img[:, :, [2,1,0]].copy() numpy\u914d\u5217\u306e\u64cd\u4f5c\u3067\u5909\u63db\u3067\u304d\u308b\u3002 cv2.cvtColor(img,cv2.COLOR_BGR2RGB) \u3067\u5909\u63db\u53ef\u80fd","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q2","text":"Y= 0.2126 R + 0.7152 G + 0.0722 B\u3001\u3067\u8868\u3055\u308c\u308b\u3002","title":"Q2.\u753b\u50cf\u306e\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u5316"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_2","text":"img = cv2.imread(\"./img/imori.jpeg\") gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 opencv\u306e\u6a5f\u80fd\u3067 cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) \u3067\u5909\u63db\u3067\u304d\u308b\u3002 \u884c\u5217\u8a08\u7b97\u3067\u3082\u51fa\u6765\u305d\u3046\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q32","text":"\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u753b\u50cf\u306e\uff12\u5024\u5316\u3002\u95be\u5024\u306f128","title":"Q3.\uff12\u5024\u5316"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_3","text":"img = cv2.imread(\"./img/imori.jpeg\") gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 ret, gray_img = cv2.threshold(gray_img,128,255,cv2.THRESH_BINARY) \u4e00\u500b\u4e00\u500b\u6761\u4ef6\u5206\u5c90\u3055\u305b\u308b\u3068\u8a08\u7b97\u91cf\u304c\u591a\u305d\u3046\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q4","text":"\u4e8c\u5024\u5316\u306b\u304a\u3051\u308b\u5206\u96e2\u306e\u95be\u5024\u3092\u81ea\u52d5\u6c7a\u5b9a\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002 \u30af\u30e9\u30b9\u9593\u5206\u6563\u304c\u6700\u5927\u3068\u306a\u308c\u3070\u826f\u3044\u3002 ret, gray_img = cv2.threshold(gray_img,0,255,cv2.THRESH_OTSU) \u3067\u3046\u307e\u304f\u3044\u304b\u306a\u304b\u3063\u305f\u3002 \u4e8b\u524d\u306e gray_img = img[:,:,0] * 0.0722 + img[:,:,1] * 0.7152 + img[:,:,2] * 0.2126 \u304c\u539f\u56e0\u3002gray_img\u304cfloat\u3060\u3068\u3046\u307e\u304f\u3044\u304b\u306a\u3044\u3002","title":"Q4.\u5927\u6d25\u306e\u4e8c\u5024\u5316"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_4","text":"gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) ret, th = cv2.threshold(gray_img, 0, 255, cv2.THRESH_OTSU)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q5hsv","text":"HSV\u5909\u63db\u3068\u306f\u3001Hue(\u8272\u76f8)\u3001Saturation(\u5f69\u5ea6)\u3001Value(\u660e\u5ea6) \u3067\u8272\u3092\u8868\u73fe\u3059\u308b\u624b\u6cd5\u3067\u3042\u308b\u3002 hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) hsv_img[:,:,0] = (hsv_img[:,:,0] + 180) % 360 hsv_img = cv2.cvtColor(hsv_img, cv2.COLOR_HSV2BGR) \u82e5\u5e72\u8272\u304c\u9055\u3046\u6c17\u304c\u3059\u308b\u3002 Hue\u306e\u7bc4\u56f2\u304c[0:179]\u3089\u3057\u3044\u3002 hsv[:,:,0] = (hsv[:,:,0] + 90) % 180 \u307e\u3060\u9055\u3046\u3002 opencv\u3092\u4f7f\u308f\u305a\u306b\u5b9f\u88c5\u3057\u305f\u3002","title":"Q5.HSV\u5909\u63db"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_5","text":"import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") h,w,c = img.shape hsv = img / 255 for i in range(w): for j in range(h): b,g,r = img[i,j,:] / 255 max_val = max(b,g,r) min_val = min(b,g,r) val = max_val sat = max_val - min_val if max_val == min_val: hue = 0 elif min_val == b: hue = 60 * (g-r) / sat + 60 elif min_val == r: hue = 60 * (b-g) / sat + 180 else: hue = 60 * (r-b) / sat + 300 # print(hsv[i,j,:]) hsv[i,j,:] = [hue,sat,val] # print(hsv[i,j,:]) cv2.imwrite(\"./img/hsv_moto.jpeg\",hsv) hsv[:,:,0] = (hsv[:,:,0] + 180) % 360 revers_img = img/255 for i in range(w): for j in range(h): hue,sat,val = hsv[i,j,:] c = sat h_dot = hue / 60 x = c * (1 - abs(h_dot % 2 - 1)) if (0 <= h_dot) & (h_dot < 1): add_h = [c,x,0] elif (1<= h_dot) & (h_dot < 2): add_h = [x,c,0] elif (2<= h_dot) & (h_dot < 3): add_h = [0,c,x] elif (3<= h_dot) & (h_dot < 4): add_h = [0,x,c] elif (4<= h_dot) & (h_dot < 5): add_h = [x,0,c] elif (5<= h_dot) & (h_dot < 6): add_h = [c,0,x] else: add_h = [0,0,0] revers_img[i,j,:] = np.multiply([1,1,1], (val - c)) + add_h revers_img = revers_img * 255 revers_img = revers_img[:, :, [2,1,0]] cv2.imwrite(\"./img/hsv.jpeg\",revers_img) height,width,channel\u306e\u9806\u5e8f\u3092\u3082\u3063\u3068\u610f\u8b58\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u3082\u3063\u3068\u30b3\u30fc\u30c9\u3092\u77ed\u304f\u3067\u304d\u308b\u3068\u826f\u3044\u3068\u601d\u3046\u3002if\u306e\u5206\u5c90\u304c\u591a\u3059\u304e\u308b\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q6","text":"\u3059\u306a\u308f\u3061R,G,B in {32, 96, 160, 224}\u306e\u54044\u5024\u306b\u6e1b\u8272\u305b\u3088\u3002","title":"Q6.\u6e1b\u8272\u51e6\u7406"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_6","text":"img = cv2.imread(\"./img/imori.jpeg\") img = (img // 64 + 1) * 64 - 32 cv2.imwrite(\"./img/result_img.jpeg\",img) if\u6587\u3092\u4f7f\u308f\u305a\u306b\u5b9f\u88c5\u3057\u3066\u307f\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q7","text":"\u753b\u50cf\u3092\u30b0\u30ea\u30c3\u30c9\u5206\u5272(\u3042\u308b\u56fa\u5b9a\u9577\u306e\u9818\u57df\u306b\u5206\u3051\u308b)\u3057\u3001\u304b\u304f\u9818\u57df\u5185(\u30bb\u30eb)\u306e\u5e73\u5747\u5024\u3067\u305d\u306e\u9818\u57df\u5185\u306e\u5024\u3092\u57cb\u3081\u308b\u3002 imori.jpg\u306f128x128\u306a\u306e\u3067\u30018x8\u306b\u30b0\u30ea\u30c3\u30c9\u5206\u5272\u3057\u3001\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0\u305b\u3088\u3002","title":"Q7.\u5e73\u5747\u30d7\u30fc\u30ea\u30f3\u30b0"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_7","text":"import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") def average_pooling(img,karnel): pool_img = img.copy() height,width,channel = img.shape for i in range(0,height,karnel[0]): for j in range(0,width,karnel[1]): ave = np.mean(img[i:i+karnel[0],j:j+karnel[1],:],axis = 0) ave = np.mean(ave,axis = 0) pool_img[i:i+karnel[0],j:j+karnel[1],:] = ave return pool_img kar = (8,8) img = average_pooling(img,kar) cv2.imwrite(\"./img/pool_img.jpeg\",img) for\u6587\u30922\u56de\u4f7f\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u308b\u3002\u3053\u308c\u4ee5\u4e0a\u6e1b\u3089\u3059\u65b9\u6cd5\u304c\u601d\u3044\u3064\u304b\u306a\u304b\u3063\u305f\u3002 \u307e\u305f\u884c\u3068\u5217\u306e\u5e73\u5747\u30922\u56de\u306b\u5206\u3051\u3066\u6c42\u3081\u3066\u308b\u306e\u3067\u3053\u308c\u3092\u4e00\u5ea6\u306b\u3067\u304d\u308b\u65b9\u6cd5\u304c\u3042\u308c\u3070\u3088\u304b\u3063\u305f\u3002 def average_pooling(img,karnel) img\u306f\u753b\u50cf\u3001karnel\u306f\u30b0\u30ea\u30c3\u30c9\u306e\u5206\u5272\u306e\u7bc4\u56f2","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q8max","text":"\u3053\u3053\u3067\u306f\u5e73\u5747\u5024\u3067\u306a\u304f\u6700\u5927\u5024\u3067\u30d7\u30fc\u30ea\u30f3\u30b0\u305b\u3088\u3002","title":"Q8.Max\u30d7\u30fc\u30ea\u30f3\u30b0"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_8","text":"import cv2 import numpy as np img = cv2.imread(\"./img/imori.jpeg\") def average_pooling(img,karnel): pool_img = img.copy() height,width,channel = img.shape for i in range(0,height,karnel[0]): for j in range(0,width,karnel[1]): ave = np.max(img[i:i+karnel[0],j:j+karnel[1],:],axis = 0) ave = np.max(ave,axis = 0) pool_img[i:i+karnel[0],j:j+karnel[1],:] = ave return pool_img kar = (8,8) img = average_pooling(img,kar) cv2.imwrite(\"./img/poolmax_img.jpeg\",img) \u5e73\u5747\u3092\u6c42\u3081\u308b\u90e8\u5206\u3092\u3001\u6700\u5927\u5024\u306b\u5909\u66f4\u3057\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q9","text":"\u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf(3x3\u3001\u6a19\u6e96\u504f\u5dee1.3)\u3092\u5b9f\u88c5\u3057\u3001imori_noise.jpg\u306e\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u305b\u3088\u3002 \u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf\u306f\u6ce8\u76ee\u753b\u7d20\u306e\u5468\u8fba\u753b\u7d20\u3092\u3001\u30ac\u30a6\u30b9\u5206\u5e03\u306b\u3088\u308b\u91cd\u307f\u4ed8\u3051\u3067\u5e73\u6ed1\u5316\u3057\u3001\u6b21\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b\u3002 \u3053\u306e\u3088\u3046\u306a\u91cd\u307f\u306f\u30ab\u30fc\u30cd\u30eb\u3084\u30d5\u30a3\u30eb\u30bf\u3068\u547c\u3070\u308c\u308b\u3002","title":"Q9.\u30ac\u30a6\u30b7\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_9","text":"import cv2 import numpy as np img = cv2.imread(\"./img/imori_noise.jpeg\") def gausu_filter(img,karnel,sigma): height,width,channel = img.shape pad = karnel // 2 pad_img = np.zeros((height + pad * 2,width + pad * 2, channel)) pad_img[pad:pad+height,pad:pad+width] = img weight = gausu(sigma,karnel,pad) gausu_img = img.copy() for i in range(height): for j in range(width): gausu_img[i,j,0] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,0]*weight) gausu_img[i,j,1] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,1]*weight) gausu_img[i,j,2] = np.sum(pad_img[i:i+pad*2+1,j:j+pad*2+1,2]*weight) print(gausu_img) return gausu_img def gausu(sigma,karnel,pading): filt = np.zeros((karnel,karnel)) for x in range(pading * -1, pading + 1): for y in range(pading * -1, pading + 1): print(x,y) filt[x+pading,y+pading] = 1 / (2*np.pi*sigma*sigma) * np.exp((-1 * (x*x + y*y))/(2 * (sigma**2))) filt /= filt.sum() return filt kar = 3 sig = 1.3 img = gausu_filter(img,kar,sig) cv2.imwrite(\"./img/gausu_img.jpeg\",img) \u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u4f5c\u6210\u306b\u3082\u95a2\u6570\u3092\u7528\u3044\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q10","text":"\u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u3057\u3001imori_noise.jpg\u306e\u30ce\u30a4\u30ba\u3092\u9664\u53bb\u305b\u3088\u3002 \u3053\u308c\u306f\u6ce8\u76ee\u753b\u7d20\u306e3x3\u306e\u9818\u57df\u5185\u306e\u3001\u30e1\u30c7\u30a3\u30a2\u30f3\u5024(\u4e2d\u592e\u5024)\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308b\u3002 \u3053\u308c\u3082\u30bc\u30ed\u30d1\u30c7\u30a3\u30f3\u30b0\u305b\u3088\u3002 \u30b5\u30a4\u30c8 \u3092\u53c2\u8003\u306b\u3057\u3066\u3001for\u6587\u3092\u5c11\u306a\u304f\u66f8\u3044\u3066\u307f\u308b\u3002","title":"Q10.\u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_10","text":"import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori_noise.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 medhian = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='med') cv2.imwrite(\"./img/medhian_img.jpeg\",medhian) \u53c2\u8003\u306b\u3057\u305f\u7d50\u679cfor\u6587\u3092\u4f7f\u308f\u306a\u3044\u3067\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002 \u3057\u304b\u3057as_strided\u306e\u5f15\u6570strides\u306e\u5f79\u5272\u304c\u3088\u304f\u5206\u304b\u3063\u3066\u3044\u306a\u3044\u3002\u8abf\u3079\u305f\u3068\u3053\u308d\u30e1\u30e2\u30ea\u306e\u79fb\u52d5\u8ddd\u96e2\u306e\u3088\u3046\u3060\u3063\u305f\u3002 strides = (stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) \u306eshape\u304c (390, 3, 1, 390, 3) \u3068\u306a\u3063\u3066\u3044\u308b\u3002\u524d\u534a\u306e(390,3,1)\u306f(height,width,channel)\u3092\u8868\u3057\u3066\u3044\u3066\u3001\u52d5\u304b\u3059(height,width)\u3092\u3082\u3046\u4e00\u5ea6\u8ffd\u52a0\u3057\u3066\u3044\u308b\u306e\u304b\uff1f","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q11","text":"\u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf\u306f\u30d5\u30a3\u30eb\u30bf\u5185\u306e\u753b\u7d20\u306e\u5e73\u5747\u5024\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308b\u3002","title":"Q11.\u5e73\u6ed1\u5316\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_11","text":"import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 mean = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='avg') cv2.imwrite(\"./img/mean_img.jpeg\",mean) \u30e1\u30c7\u30a3\u30a2\u30f3\u30d5\u30a3\u30eb\u30bf\u306e\u6700\u5f8c\u306e\u90e8\u5206\u3092\u5e73\u5747\u306b\u5909\u3048\u305f\u3060\u3051\u3067\u3042\u308b\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q12","text":"\u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf\u3068\u306f\u5bfe\u89d2\u65b9\u5411\u306e\u5e73\u5747\u5024\u3092\u53d6\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u6b21\u5f0f\u3067\u5b9a\u7fa9\u3055\u308c\u308b. [[1/3,0,0] [0,1/3,0] [0,0,1/3]]","title":"Q12.\u30e2\u30fc\u30b7\u30e7\u30f3\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_12","text":"import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 motion = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='motion') print(motion.shape) cv2.imwrite(\"./img/motion_img.jpeg\",motion) \u91cd\u307f\u3092\u4f5c\u6210\u3057\u3066\u305d\u306e\u5024\u3092\u305d\u308c\u305e\u308c\u306b\u304b\u3051\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q13max-min","text":"MAX-MIN\u30d5\u30a3\u30eb\u30bf\u3068\u306f\u30d5\u30a3\u30eb\u30bf\u5185\u306e\u753b\u7d20\u306e\u6700\u5927\u5024\u3068\u6700\u5c0f\u5024\u306e\u5dee\u3092\u51fa\u529b\u3059\u308b\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u30a8\u30c3\u30b8\u691c\u51fa\u306e\u30d5\u30a3\u30eb\u30bf\u306e\u4e00\u3064\u3067\u3042\u308b\u3002","title":"Q13.MAX-MIN\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_13","text":"import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1) kernel_size = (kernel_size, kernel_size) # print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], ) + A.strides) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'min': return A_w.min(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) elif pool_mode == \"max_min\": max_pool = A_w.max(axis=(1,2)).reshape(output_shape) min_pool = A_w.min(axis=(1,2)).reshape(output_shape) return max_pool - min_pool img = cv2.imread(\"./img/imori.jpeg\") img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) height,width= img.shape karn = 3 padding = karn // 2 max_min = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='max_min') print(max_min.shape) cv2.imwrite(\"./img/min_max_img.jpeg\",max_min) \u4e8b\u524d\u306b\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u305f\u304c\u3001BGR\u753b\u50cf\u3067\u4f5c\u6210\u3057\u305f\u5f8c\u306b\u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\u306b\u3057\u3066\u3082\u540c\u3058\u306a\u306e\u304b\uff1f","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q14","text":"\u5fae\u5206\u30d5\u30a3\u30eb\u30bf(3x3)\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u5fae\u5206\u30d5\u30a3\u30eb\u30bf\u306f\u8f1d\u5ea6\u306e\u6025\u6fc0\u306a\u5909\u5316\u304c\u8d77\u3053\u3063\u3066\u3044\u308b\u90e8\u5206\u306e\u30a8\u30c3\u30b8\u3092\u53d6\u308a\u51fa\u3059\u30d5\u30a3\u30eb\u30bf\u3067\u3042\u308a\u3001\u96a3\u308a\u5408\u3046\u753b\u7d20\u540c\u58eb\u306e\u5dee\u3092\u53d6\u308b\u3002 \u7e26\u306e\u30d5\u30a3\u30eb\u30bf [[0,0,0] [-1,1,0] [0,0,0]] \u6a2a\u306e\u30d5\u30a3\u30eb\u30bf [[0,-1,0] [0,1,0] [0,0,0]] ### \u89e3\u7b54 import numpy as np import cv2 from numpy.lib.stride_tricks import as_strided def pool2d(A, kernel_size, stride, padding, pool_mode='max'): ''' 2D Pooling Parameters: A: input 2D array kernel_size: int, the size of the window stride: int, the stride of the window padding: int, implicit zero paddings on both sides of the input pool_mode: string, 'max' or 'avg' ''' # Padding A = np.pad(A, ((padding,padding),(padding,padding),(0,0)), mode='constant') # Window view of A output_shape = ((A.shape[0] - kernel_size)//stride + 1, (A.shape[1] - kernel_size)//stride + 1, A.shape[2]) kernel_size = (kernel_size, kernel_size) print((stride*A.strides[0],stride*A.strides[1],stride*A.strides[2]) + A.strides[0:2]) A_w = as_strided(A, shape = output_shape + kernel_size, strides = (stride*A.strides[0], stride*A.strides[1], stride*A.strides[2] ) + A.strides[0:2]) A_w = A_w.reshape(-1, *kernel_size) weight = [[1/3,0,0],[0,1/3,0],[0,0,1/3]] weight = np.array(weight).reshape(-1,3,3) weight_w = [[0,0,0],[-1,1,0],[0,0,0]] weight_h = [[0,-1,0],[0,1,0],[0,0,0]] weight_w = np.array(weight_w).reshape(-1,3,3) weight_h = np.array(weight_h).reshape(-1,3,3) # Return the result of pooling if pool_mode == 'max': return A_w.max(axis=(1,2)).reshape(output_shape) elif pool_mode == 'min': return A_w.min(axis=(1,2)).reshape(output_shape) elif pool_mode == 'avg': return A_w.mean(axis=(1,2)).reshape(output_shape) elif pool_mode == \"med\": return np.median(A_w,axis=(1,2)).reshape(output_shape) elif pool_mode == \"motion\": return np.sum(A_w*weight,axis = (1,2)).reshape(output_shape) elif pool_mode == \"max_min\": max_pool = A_w.max(axis=(1,2)).reshape(output_shape) min_pool = A_w.min(axis=(1,2)).reshape(output_shape) return max_pool - min_pool elif pool_mode == \"diff_w\": return np.sum(A_w*weight_w,axis = (1,2)).reshape(output_shape) elif pool_mode == \"diff_h\": return np.sum(A_w*weight_h,axis = (1,2)).reshape(output_shape) img = cv2.imread(\"./img/imori.jpeg\") height,width,channel = img.shape karn = 3 padding = karn // 2 diff_w_img = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='diff_w') diff_h_img = pool2d(img, kernel_size=karn, stride=1, padding=padding, pool_mode='diff_h') cv2.imwrite(\"./img/diff_h_img.jpeg\",diff_h_img) cv2.imwrite(\"./img/diff_w_img.jpeg\",diff_w_img) \u30d5\u30a3\u30eb\u30bf\u3054\u3068\u306b\u5206\u96e2\u3057\u3066\u304a\u3051\u3070\u51e6\u7406\u304c\u7c21\u5358\u306b\u3067\u304d\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\u3002","title":"Q14.\u5fae\u5206\u30d5\u30a3\u30eb\u30bf"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q20","text":"matplotlib\u3092\u7528\u3044\u3066imori_dark.jpg\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u8868\u793a\u305b\u3088\u3002","title":"Q20.\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u8868\u793a"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_14","text":"import numpy as np import cv2 import matplotlib.pyplot as plt img = cv2.imread(\"./img/imori_dark.jpeg\") gaso = np.array(img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) plt.show() \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306b\u3059\u308b\u3068\u304d\u306f\uff11\u6b21\u5143\u306b\u3057\u306a\u3044\u3068\u51e6\u7406\u3067\u304d\u306a\u3044\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q21","text":"\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316\u3092\u5b9f\u88c5\u305b\u3088\u3002 [c,d]\u306e\u753b\u7d20\u5024\u3092\u6301\u3064\u753b\u50cf\u3092[a,b]\u306e\u30ec\u30f3\u30b8\u306b\u5909\u63db\u3059\u308b\u3002","title":"Q21.\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_15","text":"import numpy as np import cv2 import matplotlib.pyplot as plt def gray_scale_trans(img,a=0,b=255): out = img.copy() c = img.min() d = img.max() out = (b-a)/(d-c)*(out-c)+a np.where(out < a, a, out) np.where(b < out, b, out) return out img = cv2.imread(\"./img/imori_dark.jpeg\") trans_img = gray_scale_trans(img) gaso = np.array(trans_img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) cv2.imwrite(\"./img/trans_img.jpeg\",trans_img) plt.show() np.where\u3092\u7528\u3044\u3066\u5b9f\u88c5\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u305f\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6b63\u898f\u5316\u3092\u95a2\u6570\u3068\u3057\u3066\u8868\u3057\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q22","text":"\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u306e\u5e73\u5747\u5024\u3092m0=128\u3001\u6a19\u6e96\u504f\u5dee\u3092s0=52\u306b\u306a\u308b\u3088\u3046\u306b\u64cd\u4f5c\u305b\u3088\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u5909\u66f4\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308b.","title":"Q.22\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u64cd\u4f5c"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#_16","text":"import numpy as np import cv2 import matplotlib.pyplot as plt def hist_heitan(img,m0=128,s0=52): out = img.copy() s = np.std(img) m = np.average(img) out = s0 / s * (out - m) + m0 return out img = cv2.imread(\"./img/imori_dark.jpeg\") trans_img = hist_heitan(img) gaso = np.array(trans_img).flatten() plt.hist(gaso,bins=255,range=(0,255),rwidth=0.8) cv2.imwrite(\"./img/trans_img_1.jpeg\",trans_img) plt.show() \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u3059\u308b\u95a2\u6570\u3092\u5b9f\u88c5\u3057\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/image/#q23","text":"\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316\u3092\u5b9f\u88c5\u305b\u3088\u3002 \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316\u3068\u306f\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5e73\u5766\u306b\u5909\u66f4\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308a\u3001\u4e0a\u8a18\u306e\u5e73\u5747\u5024\u3084\u6a19\u6e96\u504f\u5dee\u306a\u3069\u3092\u5fc5\u8981\u3068\u305b\u305a\u3001\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5024\u3092\u5747\u8861\u306b\u3059\u308b\u64cd\u4f5c\u3067\u3042\u308b\u3002 import numpy as np import cv2 import matplotlib.pyplot as plt def hist_heitan_function(img, z_max=255): out = img.copy() height, width, channel = img.shape S = height * width * channel sum_h = 0 for i in range(1, 255): ind = np.where(img == i) sum_h += len(img[ind]) z_prime = z_max / S * sum_h out[ind] = z_prime return out img = cv2.imread(\"./img/imori.jpeg\") trans_img = hist_heitan_function(img) gaso = np.array(trans_img).flatten() plt.hist(gaso, bins=255, range=(0, 255), rwidth=0.8) cv2.imwrite(\"./img/trans_img_3.jpeg\", trans_img) plt.show() np.where\u3092\u7528\u3044\u3066for\u6587\u3092\u7121\u304f\u305d\u3046\u3068\u3057\u305f\u304c\u4e0a\u624b\u304f\u884c\u304b\u306a\u304b\u3063\u305f\u3002","title":"Q.23\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u5e73\u5766\u5316"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/","text":"Numpy100\u672c\u30ce\u30c3\u30af\u7b54\u3048 numpy-100 \u306e\u554f\u984c\u3092\u89e3\u3044\u305f\u3082\u306e Q1. Import the numpy package under the name np numpy\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002 \u89e3\u7b54 import numpy as np Q2.Print the numpy version and the configuration numpy\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3059\u308b\u3002 \u89e3\u7b54 import numpy as np print(np.__version__) 1.20.2\u3060\u3063\u305f\u3002 Q3.Create a null vector of size 10 \u30b5\u30a4\u30ba\uff11\uff10\u306e\uff10\u306e\u914d\u5217\u4f5c\u6210 \u89e3\u7b54 import numpy as np a = np.zeros(10) print(a) Q4.How to find the memory size of any array \u914d\u5217\u306e\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u3092\u898b\u308b \u89e3\u7b54 a = np.zeros((10, 10)) print(a.nbytes)\u3000#800 \u4e00\u3064\u306e\u8981\u7d20\u306e\u30d0\u30a4\u30c8\u6570\u306f a.itemsize \u3067\u5f97\u3089\u308c\u308b\u3002 Q5.How to get the documentation of the numpy add function from the command line? numpy\u306eadd\u95a2\u6570\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304b\u3089\u898b\u308b\u65b9\u6cd5 \u89e3\u7b54 python -c \"import numpy; numpy.info(numpy.add)\" Q6.Create a null vector of size 10 but the fifth value which is 1 \u30b5\u30a4\u30ba10\u306e0\u30d9\u30af\u30c8\u30eb\u30675\u756a\u76ee\u306e\u5024\u3060\u30511\u306e\u3082\u306e\u3092\u4f5c\u6210\u3059\u308b\u3002 \u89e3\u7b54 a = np.zeros((10)) a[4] = 1 print(a) Q7.Create a vector with values ranging from 10 to 49 10~49\u306e\u7bc4\u56f2\u306e\u5024\u3092\u3082\u3064\u914d\u5217\u306e\u4f5c\u6210 \u89e3\u7b54 b = np.arange(10, 50) print(b) Q8. Reverse a vector (first element becomes last) \u30d9\u30af\u30c8\u30eb\u3092\u53cd\u8ee2 \u89e3\u7b54 b = np.arange(10, 50) print(b[::-1]) Q9.Create a 3x3 matrix with values ranging from 0 to 8 3\u00d73\u306e\u884c\u5217\u306b0~9\u3092\u5165\u308c\u308b \u89e3\u7b54 c = np.arange(0, 9) print(c.reshape(3, 3)) reshape\u306f\u91cd\u8981\u306a\u6c17\u304c\u3059\u308b Q10. Find indices of non-zero elements from [1,2,0,0,4,0] \u30d9\u30af\u30c8\u30eb\u304b\u3089\uff10\u3067\u306f\u306a\u3044\u30d9\u30af\u30c8\u30eb\u3092\u898b\u3064\u3051\u308d \u89e3\u7b54 d = np.array([1, 2, 0, 0, 4, 0]) print(np.where(d != 0)) Q11. Create a 3x3 identity matrix \u5358\u4f4d\u884c\u5217\u3092\u4f5c\u308c \u89e3\u7b54 e = np.eye(3) print(e) Q12. Create a 3x3x3 array with random values \u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u3092\u4f5c\u308c \u89e3\u7b54 f = np.random.random((3, 3, 3)) print(f) Q13. Create a 10x10 array with random values and find the minimum and maximum values \u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u304b\u3089\u6700\u5927\u3001\u6700\u5c0f\u306e\u5024\u3092\u3068\u308c \u89e3\u7b54 f = np.random.random((10, 10)) print(np.max(f), np.min(f)) Q14.Create a random vector of size 30 and find the mean value \u30e9\u30f3\u30c0\u30e0\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u5e73\u5747\u5024\u3092\u3068\u308c \u89e3\u7b54 g = np.random.random(30) print(np.mean(g)) Q15. Create a 2d array with 1 on the border and 0 inside \u5883\u754c\u304c0\u3001\u4e2d\u304c\uff11\u306e\uff12\u6b21\u5143\u306e\u884c\u5217\u3092\u4f5c\u6210\u305b\u3088 \u89e3\u7b54 size = 5 h = np.zeros((size, size)) h[1:size-1, 1:size-1] = 1 print(h) Q16.How to add a border (filled with 0's) around an existing array? \u5468\u308a\u306b0\u306e\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0\u305b\u3088 \u89e3\u7b54 i = np.ones((5, 5)) print(np.pad(i, 1)) pad\u3082\u6163\u308c\u3066\u3044\u304f\u5fc5\u8981\u304c\u3042\u308b Q17.What is the result of the following expression? \u89e3\u7b54 print(0 * np.nan) print(np.nan == np.nan) print(np.inf > np.nan) print(np.nan - np.nan) print(np.nan in set([np.nan])) print(0.3 == 3 * 0.1) \u5b9f\u884c\u7d50\u679c nan False False nan True False np.nan\u306e\u7279\u5fb4\u3092\u3057\u3063\u304b\u308a\u628a\u63e1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b Q18.Create a 5x5 matrix with values 1,2,3,4 just below the diagonal \u5bfe\u89d2\u306e\u4e0b\u306b1,2,3,4\u306e\u5024\u3092\u5165\u308c\u308b \u89e3\u7b54 j = np.diag(np.arange(4) + 1, k=-1) print(j) Q19.Create a 8x8 matrix and fill it with a checkerboard pattern \u884c\u5217\u3092\u30c1\u30a7\u30c3\u30ab\u30fc\u30dc\u30fc\u30c9\u30d1\u30bf\u30fc\u30f3\u306b\u3057\u307e\u3057\u3087\u3046 \u89e3\u7b54 j = np.zeros((8, 8)) j[1::2, ::2] = 1 j[::2, 1::2] = 1 print(j) \u914d\u5217\u64cd\u4f5c\u306e::\u306e\u610f\u5473\u3092\u77e5\u3089\u306a\u304b\u3063\u305f\u3002 Q20.Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100 th element 100\u756a\u76ee\u306e\u8981\u7d20\u3092\u53d6\u308a\u51fa\u305b \u89e3\u7b54 print(np.unravel_index(99, (6, 7, 8))) Q21. Create a checkerboard 8x8 matrix using the tile function tile\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u30c1\u30a7\u30c3\u30ab\u30fc\u30dc\u30fc\u30c9\u30d1\u30bf\u30fc\u30f3\u3092\u4f5c\u6210\u305b\u3088 \u89e3\u7b54 j = np.tile(((0, 1), (1, 0)), (4, 4)) print(j) Q22. Normalize a 5x5 random matrix \u884c\u5217\u3092\u6b63\u898f\u5316\u305b\u3088 \u89e3\u7b54 k = np.random.random((5, 5)) k = (k - np.mean(k))/np.std(k) print(k) Q23. Create a custom dtype that describes a color as four unsigned bytes (RGBA) \u30ab\u30b9\u30bf\u30e0dtype\u3092\u4f5c\u6210\u305b\u3088 \u89e3\u7b54 color = np.dtype([(\"r\", np.ubyte, 1), (\"g\", np.ubyte, 1), (\"b\", np.ubyte, 1), (\"a\", np.ubyte, 1)]) \u81ea\u5206\u3067dtype\u3092\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\u3053\u306e\u6a5f\u80fd\u3092\u4f7f\u3046\u6a5f\u4f1a\u306f\u3042\u308b\u306e\u3060\u308d\u3046\u304b\uff1f Q24.Multiply a 5x3 matrix by a 3x2 matrix (real matrix product) \u884c\u5217\u7a4d\u3092\u884c\u3046 \u89e3\u7b54 p = np.ones((5, 3)) m = np.ones((3, 2)) n = np.dot(p, m) print(n) Q25 Given a 1D array, negate all elements which are between 3 and 8, in place. 3-8\u307e\u3067\u306e\u8981\u7d20\u3092\u8ca0\u306e\u5024 \u89e3\u7b54 q = np.arange(10) q[(3 < q) & (q <= 8)] *= -1 print(q) \u753b\u50cf\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u3067\u3082\u3088\u304f\u898b\u305f\u3002 Q26.What is the output of the following script? \u4ee5\u4e0b\u306e\u5b9f\u884c\u7d50\u679c\u3092\u78ba\u8a8d\u305b\u3088 print(sum(range(5),-1)) from numpy import * print(sum(range(5),-1)) \u7d50\u679c 9 10 \u4e0a\u306esum\u306f-1\u3092\u8a08\u7b97\u306b\u5165\u308c\u3066\u3044\u3066\u3001\u4e0b\u306esum\u306f\u8ef8\u3092-1\u306b\u6307\u5b9a\u3057\u3066\u3044\u308b\u3068\u306a\u3063\u3066\u3044\u308b Q27.Consider an integer vector Z, which of these expressions are legal? \u4ee5\u4e0b\u306e\u5f0f\u306f\u6709\u52b9\u304b?(Z\u306f\u6574\u6570) Z = np.arange(5) Z**Z 2 << Z >> 2 Z <- Z 1j*Z Z/1/1 Z<Z>Z \u7d50\u679c [ 1 1 4 27 256] [0 1 2 4 8] [False False False False False] [0.+0.j 0.+1.j 0.+2.j 0.+3.j 0.+4.j] [0. 1. 2. 3. 4.] error \u30fb\u5404\u8981\u7d20\u5024\u3054\u3068\u306e\u7d2f\u4e57\u8a08\u7b97\u306e\u7d50\u679c\u3092\u793a\u3057\u3066\u3044\u308b [0^0,1^1,2^2,3^3,4^4] \u30fb\u30d3\u30c3\u30c8\u6f14\u7b97\u3092\u3057\u3066\u3044\u308b \u30fb\u5404\u8981\u7d20\u306e\u5927\u5c0f\u95a2\u4fc2 \u30fb\u865a\u6570\u306b\u3057\u3066\u3044\u308b \u30fb2\u56de\uff11\u3067\u5272\u3063\u3066\u3044\u308b \u30fb2\u3064\u306e\u8a08\u7b97\u306f\u3067\u304d\u306a","title":"Numpy100\u672c\u30ce\u30c3\u30af\u7b54\u3048"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#numpy100","text":"numpy-100 \u306e\u554f\u984c\u3092\u89e3\u3044\u305f\u3082\u306e","title":"Numpy100\u672c\u30ce\u30c3\u30af\u7b54\u3048"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q1-import-the-numpy-package-under-the-name-np","text":"numpy\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u3002","title":"Q1. Import the numpy package under the name np"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_1","text":"import numpy as np","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q2print-the-numpy-version-and-the-configuration","text":"numpy\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u78ba\u8a8d\u3059\u308b\u3002","title":"Q2.Print the numpy version and the configuration"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_2","text":"import numpy as np print(np.__version__) 1.20.2\u3060\u3063\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q3create-a-null-vector-of-size-10","text":"\u30b5\u30a4\u30ba\uff11\uff10\u306e\uff10\u306e\u914d\u5217\u4f5c\u6210","title":"Q3.Create a null vector of size 10"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_3","text":"import numpy as np a = np.zeros(10) print(a)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q4how-to-find-the-memory-size-of-any-array","text":"\u914d\u5217\u306e\u30e1\u30e2\u30ea\u30b5\u30a4\u30ba\u3092\u898b\u308b","title":"Q4.How to find the memory size of any array"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_4","text":"a = np.zeros((10, 10)) print(a.nbytes)\u3000#800 \u4e00\u3064\u306e\u8981\u7d20\u306e\u30d0\u30a4\u30c8\u6570\u306f a.itemsize \u3067\u5f97\u3089\u308c\u308b\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q5how-to-get-the-documentation-of-the-numpy-add-function-from-the-command-line","text":"numpy\u306eadd\u95a2\u6570\u306e\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3092\u30b3\u30de\u30f3\u30c9\u30e9\u30a4\u30f3\u304b\u3089\u898b\u308b\u65b9\u6cd5","title":"Q5.How to get the documentation of the numpy add function from the command line?"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_5","text":"python -c \"import numpy; numpy.info(numpy.add)\"","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q6create-a-null-vector-of-size-10-but-the-fifth-value-which-is-1","text":"\u30b5\u30a4\u30ba10\u306e0\u30d9\u30af\u30c8\u30eb\u30675\u756a\u76ee\u306e\u5024\u3060\u30511\u306e\u3082\u306e\u3092\u4f5c\u6210\u3059\u308b\u3002","title":"Q6.Create a null vector of size 10 but the fifth value which is 1"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_6","text":"a = np.zeros((10)) a[4] = 1 print(a)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q7create-a-vector-with-values-ranging-from-10-to-49","text":"10~49\u306e\u7bc4\u56f2\u306e\u5024\u3092\u3082\u3064\u914d\u5217\u306e\u4f5c\u6210","title":"Q7.Create a vector with values ranging from 10 to 49"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_7","text":"b = np.arange(10, 50) print(b)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q8-reverse-a-vector-first-element-becomes-last","text":"\u30d9\u30af\u30c8\u30eb\u3092\u53cd\u8ee2","title":"Q8. Reverse a vector (first element becomes last)"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_8","text":"b = np.arange(10, 50) print(b[::-1])","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q9create-a-3x3-matrix-with-values-ranging-from-0-to-8","text":"3\u00d73\u306e\u884c\u5217\u306b0~9\u3092\u5165\u308c\u308b","title":"Q9.Create a 3x3 matrix with values ranging from 0 to 8"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_9","text":"c = np.arange(0, 9) print(c.reshape(3, 3)) reshape\u306f\u91cd\u8981\u306a\u6c17\u304c\u3059\u308b","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q10-find-indices-of-non-zero-elements-from-120040","text":"\u30d9\u30af\u30c8\u30eb\u304b\u3089\uff10\u3067\u306f\u306a\u3044\u30d9\u30af\u30c8\u30eb\u3092\u898b\u3064\u3051\u308d","title":"Q10. Find indices of non-zero elements from [1,2,0,0,4,0]"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_10","text":"d = np.array([1, 2, 0, 0, 4, 0]) print(np.where(d != 0))","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q11-create-a-3x3-identity-matrix","text":"\u5358\u4f4d\u884c\u5217\u3092\u4f5c\u308c","title":"Q11. Create a 3x3 identity matrix"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_11","text":"e = np.eye(3) print(e)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q12-create-a-3x3x3-array-with-random-values","text":"\u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u3092\u4f5c\u308c","title":"Q12. Create a 3x3x3 array with random values"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_12","text":"f = np.random.random((3, 3, 3)) print(f)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q13-create-a-10x10-array-with-random-values-and-find-the-minimum-and-maximum-values","text":"\u30e9\u30f3\u30c0\u30e0\u306a\u884c\u5217\u304b\u3089\u6700\u5927\u3001\u6700\u5c0f\u306e\u5024\u3092\u3068\u308c","title":"Q13. Create a 10x10 array with random values and find the minimum and maximum values"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_13","text":"f = np.random.random((10, 10)) print(np.max(f), np.min(f))","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q14create-a-random-vector-of-size-30-and-find-the-mean-value","text":"\u30e9\u30f3\u30c0\u30e0\u306a\u30d9\u30af\u30c8\u30eb\u304b\u3089\u5e73\u5747\u5024\u3092\u3068\u308c","title":"Q14.Create a random vector of size 30 and find the mean value"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_14","text":"g = np.random.random(30) print(np.mean(g))","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q15-create-a-2d-array-with-1-on-the-border-and-0-inside","text":"\u5883\u754c\u304c0\u3001\u4e2d\u304c\uff11\u306e\uff12\u6b21\u5143\u306e\u884c\u5217\u3092\u4f5c\u6210\u305b\u3088","title":"Q15. Create a 2d array with 1 on the border and 0 inside"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_15","text":"size = 5 h = np.zeros((size, size)) h[1:size-1, 1:size-1] = 1 print(h)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q16how-to-add-a-border-filled-with-0s-around-an-existing-array","text":"\u5468\u308a\u306b0\u306e\u30d1\u30c7\u30a3\u30f3\u30b0\u3092\u8ffd\u52a0\u305b\u3088","title":"Q16.How to add a border (filled with 0's) around an existing array?"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_16","text":"i = np.ones((5, 5)) print(np.pad(i, 1)) pad\u3082\u6163\u308c\u3066\u3044\u304f\u5fc5\u8981\u304c\u3042\u308b","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q17what-is-the-result-of-the-following-expression","text":"","title":"Q17.What is the result of the following expression?"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_17","text":"print(0 * np.nan) print(np.nan == np.nan) print(np.inf > np.nan) print(np.nan - np.nan) print(np.nan in set([np.nan])) print(0.3 == 3 * 0.1) \u5b9f\u884c\u7d50\u679c nan False False nan True False np.nan\u306e\u7279\u5fb4\u3092\u3057\u3063\u304b\u308a\u628a\u63e1\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q18create-a-5x5-matrix-with-values-1234-just-below-the-diagonal","text":"\u5bfe\u89d2\u306e\u4e0b\u306b1,2,3,4\u306e\u5024\u3092\u5165\u308c\u308b","title":"Q18.Create a 5x5 matrix with values 1,2,3,4 just below the diagonal"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_18","text":"j = np.diag(np.arange(4) + 1, k=-1) print(j)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q19create-a-8x8-matrix-and-fill-it-with-a-checkerboard-pattern","text":"\u884c\u5217\u3092\u30c1\u30a7\u30c3\u30ab\u30fc\u30dc\u30fc\u30c9\u30d1\u30bf\u30fc\u30f3\u306b\u3057\u307e\u3057\u3087\u3046","title":"Q19.Create a 8x8 matrix and fill it with a checkerboard pattern"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_19","text":"j = np.zeros((8, 8)) j[1::2, ::2] = 1 j[::2, 1::2] = 1 print(j) \u914d\u5217\u64cd\u4f5c\u306e::\u306e\u610f\u5473\u3092\u77e5\u3089\u306a\u304b\u3063\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q20consider-a-678-shape-array-what-is-the-index-xyz-of-the-100th-element","text":"100\u756a\u76ee\u306e\u8981\u7d20\u3092\u53d6\u308a\u51fa\u305b","title":"Q20.Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_20","text":"print(np.unravel_index(99, (6, 7, 8)))","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q21-create-a-checkerboard-8x8-matrix-using-the-tile-function","text":"tile\u95a2\u6570\u3092\u4f7f\u7528\u3057\u3066\u30c1\u30a7\u30c3\u30ab\u30fc\u30dc\u30fc\u30c9\u30d1\u30bf\u30fc\u30f3\u3092\u4f5c\u6210\u305b\u3088","title":"Q21. Create a checkerboard 8x8 matrix using the tile function"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_21","text":"j = np.tile(((0, 1), (1, 0)), (4, 4)) print(j)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q22-normalize-a-5x5-random-matrix","text":"\u884c\u5217\u3092\u6b63\u898f\u5316\u305b\u3088","title":"Q22. Normalize a 5x5 random matrix"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_22","text":"k = np.random.random((5, 5)) k = (k - np.mean(k))/np.std(k) print(k)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q23-create-a-custom-dtype-that-describes-a-color-as-four-unsigned-bytes-rgba","text":"\u30ab\u30b9\u30bf\u30e0dtype\u3092\u4f5c\u6210\u305b\u3088","title":"Q23. Create a custom dtype that describes a color as four unsigned bytes (RGBA)"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_23","text":"color = np.dtype([(\"r\", np.ubyte, 1), (\"g\", np.ubyte, 1), (\"b\", np.ubyte, 1), (\"a\", np.ubyte, 1)]) \u81ea\u5206\u3067dtype\u3092\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002\u3053\u306e\u6a5f\u80fd\u3092\u4f7f\u3046\u6a5f\u4f1a\u306f\u3042\u308b\u306e\u3060\u308d\u3046\u304b\uff1f","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q24multiply-a-5x3-matrix-by-a-3x2-matrix-real-matrix-product","text":"\u884c\u5217\u7a4d\u3092\u884c\u3046","title":"Q24.Multiply a 5x3 matrix by a 3x2 matrix (real matrix product)"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_24","text":"p = np.ones((5, 3)) m = np.ones((3, 2)) n = np.dot(p, m) print(n)","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q25-given-a-1d-array-negate-all-elements-which-are-between-3-and-8-in-place","text":"3-8\u307e\u3067\u306e\u8981\u7d20\u3092\u8ca0\u306e\u5024","title":"Q25 Given a 1D array, negate all elements which are between 3 and 8, in place."},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_25","text":"q = np.arange(10) q[(3 < q) & (q <= 8)] *= -1 print(q) \u753b\u50cf\u51e6\u7406100\u672c\u30ce\u30c3\u30af\u3067\u3082\u3088\u304f\u898b\u305f\u3002","title":"\u89e3\u7b54"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q26what-is-the-output-of-the-following-script","text":"\u4ee5\u4e0b\u306e\u5b9f\u884c\u7d50\u679c\u3092\u78ba\u8a8d\u305b\u3088 print(sum(range(5),-1)) from numpy import * print(sum(range(5),-1))","title":"Q26.What is the output of the following script?"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_26","text":"9 10 \u4e0a\u306esum\u306f-1\u3092\u8a08\u7b97\u306b\u5165\u308c\u3066\u3044\u3066\u3001\u4e0b\u306esum\u306f\u8ef8\u3092-1\u306b\u6307\u5b9a\u3057\u3066\u3044\u308b\u3068\u306a\u3063\u3066\u3044\u308b","title":"\u7d50\u679c"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#q27consider-an-integer-vector-z-which-of-these-expressions-are-legal","text":"\u4ee5\u4e0b\u306e\u5f0f\u306f\u6709\u52b9\u304b?(Z\u306f\u6574\u6570) Z = np.arange(5) Z**Z 2 << Z >> 2 Z <- Z 1j*Z Z/1/1 Z<Z>Z","title":"Q27.Consider an integer vector Z, which of these expressions are legal?"},{"location":"100%E6%9C%AC%E3%83%8E%E3%83%83%E3%82%AF/numpy/#_27","text":"[ 1 1 4 27 256] [0 1 2 4 8] [False False False False False] [0.+0.j 0.+1.j 0.+2.j 0.+3.j 0.+4.j] [0. 1. 2. 3. 4.] error \u30fb\u5404\u8981\u7d20\u5024\u3054\u3068\u306e\u7d2f\u4e57\u8a08\u7b97\u306e\u7d50\u679c\u3092\u793a\u3057\u3066\u3044\u308b [0^0,1^1,2^2,3^3,4^4] \u30fb\u30d3\u30c3\u30c8\u6f14\u7b97\u3092\u3057\u3066\u3044\u308b \u30fb\u5404\u8981\u7d20\u306e\u5927\u5c0f\u95a2\u4fc2 \u30fb\u865a\u6570\u306b\u3057\u3066\u3044\u308b \u30fb2\u56de\uff11\u3067\u5272\u3063\u3066\u3044\u308b \u30fb2\u3064\u306e\u8a08\u7b97\u306f\u3067\u304d\u306a","title":"\u7d50\u679c"},{"location":"PyTorch/PyTorch/pytorch/","text":"pytorch\u306b\u3088\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u5b9f\u88c5\u306e\u6d41\u308c \u524d\u51e6\u7406\u3001\u5f8c\u51e6\u7406\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u78ba\u8a8d Dataset\u306e\u4f5c\u6210 DataLoader\u306e\u4f5c\u6210 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u9806\u4f1d\u64ad(forward)\u306e\u5b9a\u7fa9 \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9 \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a \u5b66\u7fd2\u691c\u8a3c\u306e\u5b9f\u65bd \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u63a8\u8ad6 1~9\u307e\u3067\u3056\u3063\u304f\u308a\u3068\u307e\u3068\u3081\u3068\u304f\u3067 Dataset\u3068DataLoader\u306b\u3064\u3044\u3066 Dataset\u30af\u30e9\u30b9 \u5165\u529b\u3059\u308b\u30c7\u30fc\u30bf\u3068\u305d\u306e\u30e9\u30d9\u30eb\u306a\u3069\u3092\u30da\u30a2\u306b\u3057\u3066\u4fdd\u6301\u3057\u305f\u30af\u30e9\u30b9 \u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u524d\u51e6\u7406\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4e0e\u3048\u3001\u5bfe\u8c61\u30c7\u30fc\u30bf\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u969b\u306b\u524d\u51e6\u7406\u3092\u81ea\u52d5\u3067\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b DataLoader\u30af\u30e9\u30b9 Dataset\u304b\u3089\u3069\u306e\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u306e\u304b\u3092\u8a2d\u5b9a\u3059\u308b\u30af\u30e9\u30b9 Dataset\u304b\u3089\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u308a\u51fa\u3057\u3084\u3059\u3044 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u30bc\u30ed\u304b\u3089\u5168\u3066\u81ea\u5206\u3067\u4f5c\u6210\u3059\u308b\u30b1\u30fc\u30b9 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\u3057\u3066\u7528\u3044\u308b\u30b1\u30fc\u30b9 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u81ea\u5206\u3067\u6539\u5909\u3059\u308b\u30b1\u30fc\u30b9 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u3067\u306f\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u81ea\u5206\u3067\u6539\u5909\u3059\u308b\u30b1\u30fc\u30b9\u304c\u591a\u3044 \u9806\u4f1d\u64ad(forward)\u306b\u3064\u3044\u3066 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e2\u30c7\u30eb\u304c\u9014\u4e2d\u3067\u5206\u5c90\u3057\u305f\u308a\u3059\u308b\u305f\u3081\u3001\u9806\u4f1d\u64ad\u304c\u8907\u96d1\u306a\u5834\u5408\u304c\u591a\u3044 \u5358\u7d14\u306a\u30cd\u30c3\u30c8\u30fc\u30af\u30e2\u30c7\u30eb\u306f\u524d\u304b\u3089\u5f8c\u308d\u306b\u6d41\u308c\u308b\u3060\u3051\u3060\u304c\u3001\u305d\u3046\u306f\u3044\u304b\u306a\u3044\u306e\u3067\u304d\u3061\u3093\u3068\u9806\u4f1d\u64ad\u95a2\u6570(forward)\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068 \u640d\u5931\u95a2\u6570\u306b\u3064\u3044\u3066 \u8aa4\u5dee\u9006\u4f1d\u64ad(Backpropagation)\u3092\u3059\u308b\u305f\u3081\u306b\u5b9a\u7fa9\u3059\u308b \u5358\u7d14\u306a\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u624b\u6cd5\u3067\u3042\u308c\u3070\uff12\u4e57\u8aa4\u5dee\u306a\u3069\u5358\u7d14\u306a\u95a2\u6570\u3060\u304c\u3001\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u3067\u306f\u3082\u3063\u3068\u8907\u96d1\u306a\u3082\u306e\u304c\u4f7f\u308f\u308c\u308b \u6700\u9069\u5316\u624b\u6cd5\u306b\u3064\u3044\u3066 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306b\u4f7f\u3046\u3082\u306e \u8aa4\u5dee\u9006\u4f1d\u64ad\u306b\u3088\u3063\u3066\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8aa4\u5dee\u306b\u5bfe\u3059\u308b\u52fe\u914d\u304c\u6c42\u307e\u308b\u306e\u3067\u3001\u305d\u306e\u52fe\u914d\u3092\u4f7f\u3063\u3066\u3001\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u4fee\u6b63\u91cf\u3092\u3069\u306e\u3088\u3046\u306b\u8a08\u7b97\u3059\u308b\u306e\u304b\u3092\u8a2d\u5b9a\u3059\u308b Momentum SGD\u3001Adam\u306a\u3069\u304c\u3042\u308b \u5b66\u7fd2\u3068\u691c\u8a3c\u3001\u63a8\u8ad6\u306b\u3064\u3044\u3066 \u57fa\u672c\u7684\u306b\u306fepoch\u3054\u3068\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3067\u306e\u6027\u80fd\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u3067\u306e\u6027\u80fd\u3092\u78ba\u8a8d\u3059\u308b \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6027\u80fd\u304c\u5411\u4e0a\u3057\u306a\u304f\u306a\u3063\u305f\u3089\u3001\u305d\u306e\u5f8c\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u304b\u5b66\u7fd2\u306b\u9665\u3063\u3066\u3044\u304f\u305f\u3081\u3001\u305d\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u5b66\u7fd2\u3092\u7d42\u4e86\u3055\u305b\u308b\u3053\u3068\u304c\u591a\u3044 early stopping \u5b66\u7fd2\u304c\u7d42\u4e86\u5f8c\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u63a8\u8ad6\u3092\u884c\u3046 \u8ee2\u79fb\u5b66\u7fd2\u306e\u5b9f\u88c5 \u753b\u50cf\u30c7\u30fc\u30bf\u304b\u3089Dataset\u3092\u4f5c\u6210\u3059\u308b Dataset\u304b\u3089DataLoader\u3092\u4f5c\u6210 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u4efb\u610f\u306e\u5f62\u306b\u5909\u66f4 \u51fa\u529b\u5c64\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u307f\u3092\u5b66\u7fd2\u3055\u305b\u3001\u8ee2\u79fb\u5b66\u7fd2\u3092\u5b9f\u88c5 \u8ee2\u79fb\u5b66\u7fd2 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u3001\u6700\u7d42\u306e\u51fa\u529b\u5c64\u3092\u4ed8\u3051\u66ff\u3048\u3066\u5b66\u7fd2\u3055\u305b\u308b\u624b\u6cd5 \u6700\u7d42\u51fa\u529b\u5c64\u3092\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u5fdc\u3057\u305f\u51fa\u529b\u5c64\u306b\u4ed8\u3051\u66ff\u3048\u3066\u3001\u4ed8\u3051\u66ff\u3048\u305f\u51fa\u529b\u5c64\u3078\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u624b\u5143\u306b\u3042\u308b\u5c11\u91cf\u306e\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\u3057\u76f4\u3059\u3068\u3044\u3046\u3053\u3068 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u3068\u3059\u308b\u306e\u3067\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u304c\u5c11\u91cf\u3067\u3082\u6027\u80fd\u306e\u3044\u3044\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u73fe\u3057\u3084\u3059\u3044\u3068\u3044\u3046\u30e1\u30ea\u30c3\u30c8\u304c\uff01 \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u51fa\u529b\u5c64\u306a\u3069\u3092\u5909\u66f4\u3057\u305f\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3001\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30fb\u30e2\u30c7\u30eb\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\u624b\u6cd5 \u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5024\u306b\u306f\u5b66\u7fd2\u6e08\u307f\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5229\u7528\u3059\u308b \u8ee2\u79fb\u5b66\u7fd2\u3068\u306f\u7570\u306a\u308a\u3001\u51fa\u529b\u5c64\u30fb\u51fa\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u3060\u3051\u3067\u306a\u304f\u3001\u5168\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u518d\u5b66\u7fd2\u3055\u305b\u308b \u5165\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u5b66\u7fd2\u7387\u3092\u5c0f\u3055\u304f\u8a2d\u5b9a\u3057\u3001\u51fa\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u5b66\u7fd2\u7387\u3092\u5927\u304d\u304f\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u304c\u4e00\u822c\u7684 \u8ee2\u79fb\u5b66\u7fd2\u3068\u540c\u3058\u3067\u3001\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u304c\u5c11\u91cf\u3067\u3082\u6027\u80fd\u306e\u3044\u3044\u3067\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u73fe\u3057\u3084\u3059\u3044\u3068\u3044\u3046\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308b \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a\u90e8\u5206\u304c\u8ee2\u79fb\u5b66\u7fd2\u3068\u7570\u306a\u308b Dataset\u3092\u4f5c\u6210 Dataset\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u3001torchvision.datasets.ImageFolder\u30af\u30e9\u30b9\u3092\u5229\u7528\u3059\u308b\u624b\u6cd5\u304c\u7c21\u5358 \u4e0a\u306e\u3084\u308a\u65b9\u306f\u7c21\u5358\u3060\u304c\u3001Dataset\u306f\u81ea\u5206\u3067\u3082\u4f5c\u308c\u308b Data Augmentation \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30f3\u30c0\u30e0\u306b\u7570\u306a\u308b\u753b\u50cf\u5909\u63db\u3092\u9069\u7528\u3057\u3001\u30c7\u30fc\u30bf\u3092\u6c34\u5897\u3057\u3059\u308b\u624b\u6cd5\u3002\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30af\u30e9\u30b9\u304c\u7528\u3044\u3089\u308c\u308b\u3002 randomresizedcrop \uff1a\u6307\u5b9a\u3055\u308c\u305fPIL\u30a4\u30e1\u30fc\u30b8\u3092\u30e9\u30f3\u30c0\u30e0\u306a\u30b5\u30a4\u30ba\u3068\u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u306b\u30c8\u30ea\u30df\u30f3\u30b0\u3059\u308b\u30af\u30e9\u30b9 \uff08\u4f7f\u7528\u4f8b\uff09 RandomResizedCrop(resize, scale=(0.5~1.0)) 0.5~1.0\u306e\u5927\u304d\u3055\u3067\u62e1\u5927\u7e2e\u5c0f \u3055\u3089\u306b\u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u3092\u00be\u304b\u30894/3\u306e\u9593\u306e\u3044\u305a\u308c\u304b\u3067\u5909\u66f4\u3057\u3066\u753b\u50cf\u3092\u6a2a\u3082\u3057\u304f\u306f\u7e26\u306b\u5f15\u304d\u4f38\u3070\u3059 \u6700\u5f8c\u306bresize\u3067\u6307\u5b9a\u3057\u305f\u5927\u304d\u3055\u3067\u753b\u50cf\u3092\u5207\u308a\u51fa\u3059 RandomHorizontalFlip \uff1a\u6307\u5b9a\u3055\u308c\u305f\u78ba\u7387\u3067\u3001\u6307\u5b9a\u3055\u308c\u305fPIL\u753b\u50cf\u3092\u6c34\u5e73\u65b9\u5411\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u53cd\u8ee2\u3059\u308b\u30af\u30e9\u30b9 \uff08\u4f7f\u7528\u4f8b\uff09 RandomHorizontalFlip() \u753b\u50cf\u306e\u5de6\u53f3\u309250%\u306e\u78ba\u7387\u3067\u53cd\u8ee2\u3055\u305b\u308b\u64cd\u4f5c torchvision.transforms albumentations\u3082\u3048\u3048\u305e \u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u3092\u884c\u3044\u591a\u69d8\u306a\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3067\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u6027\u80fd\uff08\u6c4e\u5316\u6027\u80fd\uff09\u304c\u5411\u4e0a\u3057\u3084\u3059\u304f\u306a\u308b\uff01 DataLoader\u3092\u4f5c\u6210 Dataset\u3092\u5229\u7528\u3057\u3066\u4f5c\u6210\u3059\u308b torch.utils.data.DataLoader shuffle=True \u753b\u50cf\u3092\u53d6\u308a\u51fa\u3059\u9806\u756a\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b \u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9 \u901a\u5e38\u306e\u30af\u30e9\u30b9\u5206\u985e\u306f\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570\u3092\u4f7f\u7528 \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570 \u5168\u7d50\u5408\u5c64\u304b\u3089\u306e\u51fa\u529b\u306b\u5bfe\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3042\u3068\u3001\u30af\u30e9\u30b9\u5206\u985e\u306e\u640d\u5931\u95a2\u6570\u3067\u3042\u308bThe negative log likelihood loss(\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6\u640d\u5931?)\u3092\u8a08\u7b97\u3059\u308b \u6700\u9069\u5316\u624b\u6cd5\u3092\u8a2d\u5b9a (\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0)[https://qiita.com/ZoneTsuyoshi/items/8ef6fa1e154d176e25b8] requires_grad \u81ea\u52d5\u5fae\u5206\u306e\u5bfe\u8c61\u306e\u52fe\u914d\u306e\u8a08\u7b97\u3092\u3059\u308b\u304b\u3057\u306a\u3044\u304b\u3092\u8a2d\u5b9a\u3059\u308b\u3082\u306e requires_grad = True \u8aa4\u5dee\u9006\u4f1d\u64ad\u3067\u52fe\u914d\u304c\u8a08\u7b97\u3055\u308c\u3001\u5b66\u7fd2\u6642\u306b\u5024\u304c\u5909\u5316\u3059\u308b(\u81ea\u52d5\u5fae\u5206\u3092\u884c\u3046) requires_grad = False \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u56fa\u5b9a\u3055\u305b\u3001\u66f4\u65b0\u3057\u305f\u304f\u306a\u3044\u6642\u306b\u4f7f\u3046(\u81ea\u52d5\u5fae\u5206\u3092\u884c\u308f\u306a\u3044) \u5b66\u7fd2\u30fb\u691c\u8a3c\u3092\u5b9f\u65bd Dropout\u3084\u52fe\u914d\u8a08\u7b97\u306f\u8a13\u7df4\u6642\u306b\u306e\u307f\u884c\u3044\u3001\u4e88\u6e2c\u6642\u306b\u306f\u4f7f\u7528\u3057\u306a\u3044\u306e\u304c\u901a\u5e38 \u306a\u306e\u3067\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u3001\u691c\u8a3c\u30e2\u30fc\u30c9\u306b\u3057\u3066\u308f\u3051\u308b \uff08\u4f8b\uff09net.train(), net.eval() \u691c\u8a3c\u6642\u306b\u306f\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u306a\u3044\u306e\u3067\u3001if\u6587\u3067\u5834\u5408\u5206\u3051\u3092\u884c\u3046 \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u88c5 \u6700\u9069\u5316\u306e\u65b9\u6cd5\u304c\u8ee2\u79fb\u5b66\u7fd2\u3068\u7570\u306a\u308b \u5168\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306boptimizer\u3092\u8a2d\u5b9a","title":"pytorch\u306b\u3088\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u5b9f\u88c5\u306e\u6d41\u308c"},{"location":"PyTorch/PyTorch/pytorch/#pytorch","text":"\u524d\u51e6\u7406\u3001\u5f8c\u51e6\u7406\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u78ba\u8a8d Dataset\u306e\u4f5c\u6210 DataLoader\u306e\u4f5c\u6210 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u9806\u4f1d\u64ad(forward)\u306e\u5b9a\u7fa9 \u640d\u5931\u95a2\u6570\u306e\u5b9a\u7fa9 \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a \u5b66\u7fd2\u691c\u8a3c\u306e\u5b9f\u65bd \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u63a8\u8ad6 1~9\u307e\u3067\u3056\u3063\u304f\u308a\u3068\u307e\u3068\u3081\u3068\u304f\u3067","title":"pytorch\u306b\u3088\u308b\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u5b9f\u88c5\u306e\u6d41\u308c"},{"location":"PyTorch/PyTorch/pytorch/#datasetdataloader","text":"Dataset\u30af\u30e9\u30b9 \u5165\u529b\u3059\u308b\u30c7\u30fc\u30bf\u3068\u305d\u306e\u30e9\u30d9\u30eb\u306a\u3069\u3092\u30da\u30a2\u306b\u3057\u3066\u4fdd\u6301\u3057\u305f\u30af\u30e9\u30b9 \u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u524d\u51e6\u7406\u30af\u30e9\u30b9\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u4e0e\u3048\u3001\u5bfe\u8c61\u30c7\u30fc\u30bf\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u8aad\u307f\u8fbc\u3080\u969b\u306b\u524d\u51e6\u7406\u3092\u81ea\u52d5\u3067\u9069\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b DataLoader\u30af\u30e9\u30b9 Dataset\u304b\u3089\u3069\u306e\u3088\u3046\u306b\u30c7\u30fc\u30bf\u3092\u53d6\u308a\u51fa\u3059\u306e\u304b\u3092\u8a2d\u5b9a\u3059\u308b\u30af\u30e9\u30b9 Dataset\u304b\u3089\u30df\u30cb\u30d0\u30c3\u30c1\u3092\u53d6\u308a\u51fa\u3057\u3084\u3059\u3044","title":"Dataset\u3068DataLoader\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#_1","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u4f5c\u6210 \u30bc\u30ed\u304b\u3089\u5168\u3066\u81ea\u5206\u3067\u4f5c\u6210\u3059\u308b\u30b1\u30fc\u30b9 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\u3057\u3066\u7528\u3044\u308b\u30b1\u30fc\u30b9 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u81ea\u5206\u3067\u6539\u5909\u3059\u308b\u30b1\u30fc\u30b9 \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u3067\u306f\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u81ea\u5206\u3067\u6539\u5909\u3059\u308b\u30b1\u30fc\u30b9\u304c\u591a\u3044","title":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#forward","text":"\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u306f\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e2\u30c7\u30eb\u304c\u9014\u4e2d\u3067\u5206\u5c90\u3057\u305f\u308a\u3059\u308b\u305f\u3081\u3001\u9806\u4f1d\u64ad\u304c\u8907\u96d1\u306a\u5834\u5408\u304c\u591a\u3044 \u5358\u7d14\u306a\u30cd\u30c3\u30c8\u30fc\u30af\u30e2\u30c7\u30eb\u306f\u524d\u304b\u3089\u5f8c\u308d\u306b\u6d41\u308c\u308b\u3060\u3051\u3060\u304c\u3001\u305d\u3046\u306f\u3044\u304b\u306a\u3044\u306e\u3067\u304d\u3061\u3093\u3068\u9806\u4f1d\u64ad\u95a2\u6570(forward)\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068","title":"\u9806\u4f1d\u64ad(forward)\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#_2","text":"\u8aa4\u5dee\u9006\u4f1d\u64ad(Backpropagation)\u3092\u3059\u308b\u305f\u3081\u306b\u5b9a\u7fa9\u3059\u308b \u5358\u7d14\u306a\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u624b\u6cd5\u3067\u3042\u308c\u3070\uff12\u4e57\u8aa4\u5dee\u306a\u3069\u5358\u7d14\u306a\u95a2\u6570\u3060\u304c\u3001\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u306e\u5fdc\u7528\u624b\u6cd5\u3067\u306f\u3082\u3063\u3068\u8907\u96d1\u306a\u3082\u306e\u304c\u4f7f\u308f\u308c\u308b","title":"\u640d\u5931\u95a2\u6570\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#_3","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\u969b\u306b\u4f7f\u3046\u3082\u306e \u8aa4\u5dee\u9006\u4f1d\u64ad\u306b\u3088\u3063\u3066\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8aa4\u5dee\u306b\u5bfe\u3059\u308b\u52fe\u914d\u304c\u6c42\u307e\u308b\u306e\u3067\u3001\u305d\u306e\u52fe\u914d\u3092\u4f7f\u3063\u3066\u3001\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u4fee\u6b63\u91cf\u3092\u3069\u306e\u3088\u3046\u306b\u8a08\u7b97\u3059\u308b\u306e\u304b\u3092\u8a2d\u5b9a\u3059\u308b Momentum SGD\u3001Adam\u306a\u3069\u304c\u3042\u308b","title":"\u6700\u9069\u5316\u624b\u6cd5\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#_4","text":"\u57fa\u672c\u7684\u306b\u306fepoch\u3054\u3068\u306b\u8a13\u7df4\u30c7\u30fc\u30bf\u3067\u306e\u6027\u80fd\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u3067\u306e\u6027\u80fd\u3092\u78ba\u8a8d\u3059\u308b \u691c\u8a3c\u30c7\u30fc\u30bf\u306e\u6027\u80fd\u304c\u5411\u4e0a\u3057\u306a\u304f\u306a\u3063\u305f\u3089\u3001\u305d\u306e\u5f8c\u306f\u8a13\u7df4\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u304b\u5b66\u7fd2\u306b\u9665\u3063\u3066\u3044\u304f\u305f\u3081\u3001\u305d\u306e\u30bf\u30a4\u30df\u30f3\u30b0\u3067\u5b66\u7fd2\u3092\u7d42\u4e86\u3055\u305b\u308b\u3053\u3068\u304c\u591a\u3044 early stopping \u5b66\u7fd2\u304c\u7d42\u4e86\u5f8c\u306b\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u63a8\u8ad6\u3092\u884c\u3046","title":"\u5b66\u7fd2\u3068\u691c\u8a3c\u3001\u63a8\u8ad6\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch/pytorch/#_5","text":"\u753b\u50cf\u30c7\u30fc\u30bf\u304b\u3089Dataset\u3092\u4f5c\u6210\u3059\u308b Dataset\u304b\u3089DataLoader\u3092\u4f5c\u6210 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u5c64\u3092\u4efb\u610f\u306e\u5f62\u306b\u5909\u66f4 \u51fa\u529b\u5c64\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u307f\u3092\u5b66\u7fd2\u3055\u305b\u3001\u8ee2\u79fb\u5b66\u7fd2\u3092\u5b9f\u88c5 \u8ee2\u79fb\u5b66\u7fd2 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u3001\u6700\u7d42\u306e\u51fa\u529b\u5c64\u3092\u4ed8\u3051\u66ff\u3048\u3066\u5b66\u7fd2\u3055\u305b\u308b\u624b\u6cd5 \u6700\u7d42\u51fa\u529b\u5c64\u3092\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u306b\u5bfe\u5fdc\u3057\u305f\u51fa\u529b\u5c64\u306b\u4ed8\u3051\u66ff\u3048\u3066\u3001\u4ed8\u3051\u66ff\u3048\u305f\u51fa\u529b\u5c64\u3078\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u624b\u5143\u306b\u3042\u308b\u5c11\u91cf\u306e\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2\u3057\u76f4\u3059\u3068\u3044\u3046\u3053\u3068 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u3068\u3059\u308b\u306e\u3067\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u304c\u5c11\u91cf\u3067\u3082\u6027\u80fd\u306e\u3044\u3044\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u73fe\u3057\u3084\u3059\u3044\u3068\u3044\u3046\u30e1\u30ea\u30c3\u30c8\u304c\uff01 \u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0 \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d9\u30fc\u30b9\u306b\u51fa\u529b\u5c64\u306a\u3069\u3092\u5909\u66f4\u3057\u305f\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u3001\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u3067\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30fb\u30e2\u30c7\u30eb\u306e\u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u308b\u624b\u6cd5 \u7d50\u5408\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5024\u306b\u306f\u5b66\u7fd2\u6e08\u307f\u306e\u30e2\u30c7\u30eb\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5229\u7528\u3059\u308b \u8ee2\u79fb\u5b66\u7fd2\u3068\u306f\u7570\u306a\u308a\u3001\u51fa\u529b\u5c64\u30fb\u51fa\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u3060\u3051\u3067\u306a\u304f\u3001\u5168\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u518d\u5b66\u7fd2\u3055\u305b\u308b \u5165\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u5b66\u7fd2\u7387\u3092\u5c0f\u3055\u304f\u8a2d\u5b9a\u3057\u3001\u51fa\u529b\u5c64\u306b\u8fd1\u3044\u90e8\u5206\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u5b66\u7fd2\u7387\u3092\u5927\u304d\u304f\u8a2d\u5b9a\u3059\u308b\u3053\u3068\u304c\u4e00\u822c\u7684 \u8ee2\u79fb\u5b66\u7fd2\u3068\u540c\u3058\u3067\u3001\u81ea\u524d\u306e\u30c7\u30fc\u30bf\u304c\u5c11\u91cf\u3067\u3082\u6027\u80fd\u306e\u3044\u3044\u3067\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u5b9f\u73fe\u3057\u3084\u3059\u3044\u3068\u3044\u3046\u30e1\u30ea\u30c3\u30c8\u304c\u3042\u308b \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a\u90e8\u5206\u304c\u8ee2\u79fb\u5b66\u7fd2\u3068\u7570\u306a\u308b","title":"\u8ee2\u79fb\u5b66\u7fd2\u306e\u5b9f\u88c5"},{"location":"PyTorch/PyTorch/pytorch/#dataset","text":"Dataset\u3092\u4f5c\u6210\u3059\u308b\u5834\u5408\u3001torchvision.datasets.ImageFolder\u30af\u30e9\u30b9\u3092\u5229\u7528\u3059\u308b\u624b\u6cd5\u304c\u7c21\u5358 \u4e0a\u306e\u3084\u308a\u65b9\u306f\u7c21\u5358\u3060\u304c\u3001Dataset\u306f\u81ea\u5206\u3067\u3082\u4f5c\u308c\u308b Data Augmentation \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u30e9\u30f3\u30c0\u30e0\u306b\u7570\u306a\u308b\u753b\u50cf\u5909\u63db\u3092\u9069\u7528\u3057\u3001\u30c7\u30fc\u30bf\u3092\u6c34\u5897\u3057\u3059\u308b\u624b\u6cd5\u3002\u4ee5\u4e0b\u306e\u3088\u3046\u306a\u30af\u30e9\u30b9\u304c\u7528\u3044\u3089\u308c\u308b\u3002 randomresizedcrop \uff1a\u6307\u5b9a\u3055\u308c\u305fPIL\u30a4\u30e1\u30fc\u30b8\u3092\u30e9\u30f3\u30c0\u30e0\u306a\u30b5\u30a4\u30ba\u3068\u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u306b\u30c8\u30ea\u30df\u30f3\u30b0\u3059\u308b\u30af\u30e9\u30b9 \uff08\u4f7f\u7528\u4f8b\uff09 RandomResizedCrop(resize, scale=(0.5~1.0)) 0.5~1.0\u306e\u5927\u304d\u3055\u3067\u62e1\u5927\u7e2e\u5c0f \u3055\u3089\u306b\u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u3092\u00be\u304b\u30894/3\u306e\u9593\u306e\u3044\u305a\u308c\u304b\u3067\u5909\u66f4\u3057\u3066\u753b\u50cf\u3092\u6a2a\u3082\u3057\u304f\u306f\u7e26\u306b\u5f15\u304d\u4f38\u3070\u3059 \u6700\u5f8c\u306bresize\u3067\u6307\u5b9a\u3057\u305f\u5927\u304d\u3055\u3067\u753b\u50cf\u3092\u5207\u308a\u51fa\u3059 RandomHorizontalFlip \uff1a\u6307\u5b9a\u3055\u308c\u305f\u78ba\u7387\u3067\u3001\u6307\u5b9a\u3055\u308c\u305fPIL\u753b\u50cf\u3092\u6c34\u5e73\u65b9\u5411\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u53cd\u8ee2\u3059\u308b\u30af\u30e9\u30b9 \uff08\u4f7f\u7528\u4f8b\uff09 RandomHorizontalFlip() \u753b\u50cf\u306e\u5de6\u53f3\u309250%\u306e\u78ba\u7387\u3067\u53cd\u8ee2\u3055\u305b\u308b\u64cd\u4f5c torchvision.transforms albumentations\u3082\u3048\u3048\u305e \u30c7\u30fc\u30bf\u306e\u6c34\u5897\u3057\u3092\u884c\u3044\u591a\u69d8\u306a\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3059\u308b\u3053\u3068\u3067\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u6027\u80fd\uff08\u6c4e\u5316\u6027\u80fd\uff09\u304c\u5411\u4e0a\u3057\u3084\u3059\u304f\u306a\u308b\uff01","title":"Dataset\u3092\u4f5c\u6210"},{"location":"PyTorch/PyTorch/pytorch/#dataloader","text":"Dataset\u3092\u5229\u7528\u3057\u3066\u4f5c\u6210\u3059\u308b torch.utils.data.DataLoader shuffle=True \u753b\u50cf\u3092\u53d6\u308a\u51fa\u3059\u9806\u756a\u304c\u30e9\u30f3\u30c0\u30e0\u306b\u306a\u308b\u3088\u3046\u306b\u3059\u308b","title":"DataLoader\u3092\u4f5c\u6210"},{"location":"PyTorch/PyTorch/pytorch/#_6","text":"\u901a\u5e38\u306e\u30af\u30e9\u30b9\u5206\u985e\u306f\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570\u3092\u4f7f\u7528 \u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u95a2\u6570 \u5168\u7d50\u5408\u5c64\u304b\u3089\u306e\u51fa\u529b\u306b\u5bfe\u3057\u3066\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u95a2\u6570\u3092\u9069\u7528\u3057\u305f\u3042\u3068\u3001\u30af\u30e9\u30b9\u5206\u985e\u306e\u640d\u5931\u95a2\u6570\u3067\u3042\u308bThe negative log likelihood loss(\u8ca0\u306e\u5bfe\u6570\u5c24\u5ea6\u640d\u5931?)\u3092\u8a08\u7b97\u3059\u308b","title":"\u640d\u5931\u95a2\u6570\u3092\u5b9a\u7fa9"},{"location":"PyTorch/PyTorch/pytorch/#_7","text":"(\u6df1\u5c64\u5b66\u7fd2\u306e\u6700\u9069\u5316\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0)[https://qiita.com/ZoneTsuyoshi/items/8ef6fa1e154d176e25b8] requires_grad \u81ea\u52d5\u5fae\u5206\u306e\u5bfe\u8c61\u306e\u52fe\u914d\u306e\u8a08\u7b97\u3092\u3059\u308b\u304b\u3057\u306a\u3044\u304b\u3092\u8a2d\u5b9a\u3059\u308b\u3082\u306e requires_grad = True \u8aa4\u5dee\u9006\u4f1d\u64ad\u3067\u52fe\u914d\u304c\u8a08\u7b97\u3055\u308c\u3001\u5b66\u7fd2\u6642\u306b\u5024\u304c\u5909\u5316\u3059\u308b(\u81ea\u52d5\u5fae\u5206\u3092\u884c\u3046) requires_grad = False \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u56fa\u5b9a\u3055\u305b\u3001\u66f4\u65b0\u3057\u305f\u304f\u306a\u3044\u6642\u306b\u4f7f\u3046(\u81ea\u52d5\u5fae\u5206\u3092\u884c\u308f\u306a\u3044)","title":"\u6700\u9069\u5316\u624b\u6cd5\u3092\u8a2d\u5b9a"},{"location":"PyTorch/PyTorch/pytorch/#_8","text":"Dropout\u3084\u52fe\u914d\u8a08\u7b97\u306f\u8a13\u7df4\u6642\u306b\u306e\u307f\u884c\u3044\u3001\u4e88\u6e2c\u6642\u306b\u306f\u4f7f\u7528\u3057\u306a\u3044\u306e\u304c\u901a\u5e38 \u306a\u306e\u3067\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u3001\u691c\u8a3c\u30e2\u30fc\u30c9\u306b\u3057\u3066\u308f\u3051\u308b \uff08\u4f8b\uff09net.train(), net.eval() \u691c\u8a3c\u6642\u306b\u306f\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u5fc5\u8981\u304c\u306a\u3044\u306e\u3067\u3001if\u6587\u3067\u5834\u5408\u5206\u3051\u3092\u884c\u3046","title":"\u5b66\u7fd2\u30fb\u691c\u8a3c\u3092\u5b9f\u65bd"},{"location":"PyTorch/PyTorch/pytorch/#_9","text":"\u6700\u9069\u5316\u306e\u65b9\u6cd5\u304c\u8ee2\u79fb\u5b66\u7fd2\u3068\u7570\u306a\u308b \u5168\u5c64\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306boptimizer\u3092\u8a2d\u5b9a","title":"\u30d5\u30a1\u30a4\u30f3\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\u306e\u5b9f\u88c5"},{"location":"PyTorch/PyTorch/video/","text":"PyTorch Dataset API \u3067\u52d5\u753b\u30c7\u30fc\u30bf\u3092\u6271\u3046\u65b9\u6cd5 \u4ecaVideo Recognition\u306e\u5206\u91ce\u304c\u30a2\u30c4\u3044\uff1f YouTube\u3084TikTok\u7b49\u5927\u898f\u6a21\u306a\u52d5\u753b\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u3084\u3059\u304f\u306a\u3063\u305f\u6628\u4eca\u3001 3DCNN \u3084 ViT \u3092\u7528\u3044\u305f\u52d5\u753b\u5185\u3067\u306e\u59ff\u52e2\u63a8\u5b9a\u3084\u30af\u30e9\u30b9\u5206\u985e\u304c\u76db\u3093\u306b\u7814\u7a76\u3055\u308c\u3066\u3044\u307e\u3059\u3002SoTA\u30e2\u30c7\u30eb\u3060\u3068\u5b9f\u88c5\u304c\u5927\u5909\u3067\u3059\u304c\u5b9f\u306f\u65e2\u5b58\u306eCNN+LSTM\u306a\u3069\u3067\u3082\u7d50\u69cb\u3044\u3044\u7cbe\u5ea6\u304c\u3060\u305b\u305f\u308a\u3057\u307e\u3059\u3002 \u4eca\u56de\u306f\u305d\u3093\u306aVideo Recognition\u3092\u304a\u624b\u8efd\u306b\u8a66\u3059\u4e0a\u3067\u4ee5\u5916\u3068\u9762\u5012\u304f\u3055\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u308a\u65b9\u3092\u66f8\u3044\u3066\u3044\u304d\u307e\u3059\u3002 \u306a\u304a\u3001\u52d5\u753b\u30c7\u30fc\u30bf\u306f\u5404\u30d5\u30ec\u30fc\u30e0\u3092\u753b\u50cf\u30c7\u30fc\u30bf\u3068\u3057\u3066\u305d\u308c\u3089\u306e\u96c6\u5408\u3068\u3057\u3066\u6271\u3046\u306e\u304c\u4e00\u822c\u7684\u3067\u3059\u306e\u3067\u305d\u3053\u307e\u3067\u306e\u5909\u63db\u306f\u5225\u9014\u884c\u3063\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002 ffmpeg \u3084 torchvision.io \u7b49\u3067\u8abf\u3079\u308b\u3068\u5e78\u305b\u306b\u306a\u308c\u307e\u3059\u3002 \u3084\u308a\u65b9 \u4eca\u56de\u306f\u3001 \u6570\u30d5\u30ec\u30fc\u30e0\u306e\u52d5\u753b\u3092\u5b66\u7fd2\u3057\u3001\u305d\u308c\u304c\u306a\u3093\u306e\u52d5\u4f5c\u304b\u4e88\u6e2c\u3059\u308b \u3068\u3044\u3046\u30bf\u30b9\u30af\u3092\u4eee\u5b9a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u52d5\u753b\u306e3\u30d5\u30ec\u30fc\u30e0\u3092\u53d7\u3051\u53d6\u308a\u3001\u305d\u308c\u304c\u306a\u3093\u306e\u52d5\u4f5c\u306a\u306e\u304b\u3092\u63a8\u8ad6\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u4efb\u610f\u306e\u4e00\u9023\u306e4\u30d5\u30ec\u30fc\u30e0\u3068\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3067\u304d\u308c\u3070\u304ak\u3002 import torch class VideoDataset ( torch . utils . data . Dataset ): def __init__ ( self , frames ): self . idxs = [ 0 , 1 , 2 , 3 , 4 ] self . data = \"a b c d e\" . split () self . labels = \"a b c d e\" . split () self . frames = frames def __len__ ( self ): return 5 - ( self . frames - 1 ) def __getitem__ ( self , idx ): res = [ self . data [ i ] for i in range ( idx , idx + self . frames )] return res , self . labels [ idx + ( self . frames - 1 )] \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u52d5\u753b\u306e\u5404\u30d5\u30ec\u30fc\u30e0\u753b\u50cf\u306e\u30d1\u30b9\u3092abcde\u3068\u3057\u3066\u4e00\u9023\u306e\u4e09\u679a\u3068\u3001\u305d\u306e3\u679a\u76ee\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3057\u307e\u3059\u3002 train = VideoDataset ( frames = 3 ) for x , t in train : print ( x , t ) # out [ 'a' , 'b' , 'c' ] c [ 'b' , 'c' , 'd' ] d [ 'c' , 'd' , 'e' ] e __getitem__ \u306e\u6700\u7d42\u884c\u3092\u5e30\u308c\u3070\u51fa\u529b\u3059\u308b\u30e9\u30d9\u30eb\u306e\u4f4d\u7f6e\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4e00\u9023\u306e\u30d5\u30ec\u30fc\u30e0\u306b\u5bfe\u3057\u3066\u3069\u3053\u3092\u30e9\u30d9\u30eb\u3068\u3059\u308b\u304b\u306f\u554f\u984c\u8a2d\u5b9a\u306b\u3088\u3063\u3066\u5909\u308f\u3063\u3066\u6765\u307e\u3059\u304c\u3001\u6700\u521d\u306e\u30d5\u30ec\u30fc\u30e0\u3092\u30e9\u30d9\u30eb\u306b\u3057\u3066\u3057\u307e\u3046\u3068\u3042\u308b\u610f\u5473\u672a\u6765\u306e\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u5927\u62b5\u306e\u5834\u5408\u306f\u6700\u5f8c\u306e\u30d5\u30ec\u30fc\u30e0\u3092\u6559\u5e2b\u30c7\u30fc\u30bf\u306b\u3057\u307e\u3059\u3002 \u3053\u3053\u3089\u8fba\u306f\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u8a18\u4e8b\u3067\u6539\u3081\u3066\u89e6\u308c\u307e\u3059\u3002 \u6b21\u56de\u306f\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f\u30af\u30e9\u30b9\u5206\u985e\u30e2\u30c7\u30eb\u3092\u66f8\u304d\u307e\u3059\u3002","title":"PyTorch Dataset API \u3067\u52d5\u753b\u30c7\u30fc\u30bf\u3092\u6271\u3046\u65b9\u6cd5"},{"location":"PyTorch/PyTorch/video/#pytorch-dataset-api","text":"","title":"PyTorch Dataset API \u3067\u52d5\u753b\u30c7\u30fc\u30bf\u3092\u6271\u3046\u65b9\u6cd5"},{"location":"PyTorch/PyTorch/video/#video-recognition","text":"YouTube\u3084TikTok\u7b49\u5927\u898f\u6a21\u306a\u52d5\u753b\u30c7\u30fc\u30bf\u3092\u53ce\u96c6\u3057\u3084\u3059\u304f\u306a\u3063\u305f\u6628\u4eca\u3001 3DCNN \u3084 ViT \u3092\u7528\u3044\u305f\u52d5\u753b\u5185\u3067\u306e\u59ff\u52e2\u63a8\u5b9a\u3084\u30af\u30e9\u30b9\u5206\u985e\u304c\u76db\u3093\u306b\u7814\u7a76\u3055\u308c\u3066\u3044\u307e\u3059\u3002SoTA\u30e2\u30c7\u30eb\u3060\u3068\u5b9f\u88c5\u304c\u5927\u5909\u3067\u3059\u304c\u5b9f\u306f\u65e2\u5b58\u306eCNN+LSTM\u306a\u3069\u3067\u3082\u7d50\u69cb\u3044\u3044\u7cbe\u5ea6\u304c\u3060\u305b\u305f\u308a\u3057\u307e\u3059\u3002 \u4eca\u56de\u306f\u305d\u3093\u306aVideo Recognition\u3092\u304a\u624b\u8efd\u306b\u8a66\u3059\u4e0a\u3067\u4ee5\u5916\u3068\u9762\u5012\u304f\u3055\u3044\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u4f5c\u308a\u65b9\u3092\u66f8\u3044\u3066\u3044\u304d\u307e\u3059\u3002 \u306a\u304a\u3001\u52d5\u753b\u30c7\u30fc\u30bf\u306f\u5404\u30d5\u30ec\u30fc\u30e0\u3092\u753b\u50cf\u30c7\u30fc\u30bf\u3068\u3057\u3066\u305d\u308c\u3089\u306e\u96c6\u5408\u3068\u3057\u3066\u6271\u3046\u306e\u304c\u4e00\u822c\u7684\u3067\u3059\u306e\u3067\u305d\u3053\u307e\u3067\u306e\u5909\u63db\u306f\u5225\u9014\u884c\u3063\u3066\u304a\u3044\u3066\u304f\u3060\u3055\u3044\u3002 ffmpeg \u3084 torchvision.io \u7b49\u3067\u8abf\u3079\u308b\u3068\u5e78\u305b\u306b\u306a\u308c\u307e\u3059\u3002","title":"\u4ecaVideo Recognition\u306e\u5206\u91ce\u304c\u30a2\u30c4\u3044\uff1f"},{"location":"PyTorch/PyTorch/video/#_1","text":"\u4eca\u56de\u306f\u3001 \u6570\u30d5\u30ec\u30fc\u30e0\u306e\u52d5\u753b\u3092\u5b66\u7fd2\u3057\u3001\u305d\u308c\u304c\u306a\u3093\u306e\u52d5\u4f5c\u304b\u4e88\u6e2c\u3059\u308b \u3068\u3044\u3046\u30bf\u30b9\u30af\u3092\u4eee\u5b9a\u3057\u307e\u3059\u3002\u4f8b\u3048\u3070\u52d5\u753b\u306e3\u30d5\u30ec\u30fc\u30e0\u3092\u53d7\u3051\u53d6\u308a\u3001\u305d\u308c\u304c\u306a\u3093\u306e\u52d5\u4f5c\u306a\u306e\u304b\u3092\u63a8\u8ad6\u3057\u307e\u3059\u3002\u3064\u307e\u308a\u4efb\u610f\u306e\u4e00\u9023\u306e4\u30d5\u30ec\u30fc\u30e0\u3068\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3067\u304d\u308c\u3070\u304ak\u3002 import torch class VideoDataset ( torch . utils . data . Dataset ): def __init__ ( self , frames ): self . idxs = [ 0 , 1 , 2 , 3 , 4 ] self . data = \"a b c d e\" . split () self . labels = \"a b c d e\" . split () self . frames = frames def __len__ ( self ): return 5 - ( self . frames - 1 ) def __getitem__ ( self , idx ): res = [ self . data [ i ] for i in range ( idx , idx + self . frames )] return res , self . labels [ idx + ( self . frames - 1 )] \u4e0a\u8a18\u306e\u4f8b\u3067\u306f\u52d5\u753b\u306e\u5404\u30d5\u30ec\u30fc\u30e0\u753b\u50cf\u306e\u30d1\u30b9\u3092abcde\u3068\u3057\u3066\u4e00\u9023\u306e\u4e09\u679a\u3068\u3001\u305d\u306e3\u679a\u76ee\u306b\u5bfe\u5fdc\u3059\u308b\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3057\u307e\u3059\u3002 train = VideoDataset ( frames = 3 ) for x , t in train : print ( x , t ) # out [ 'a' , 'b' , 'c' ] c [ 'b' , 'c' , 'd' ] d [ 'c' , 'd' , 'e' ] e __getitem__ \u306e\u6700\u7d42\u884c\u3092\u5e30\u308c\u3070\u51fa\u529b\u3059\u308b\u30e9\u30d9\u30eb\u306e\u4f4d\u7f6e\u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4e00\u9023\u306e\u30d5\u30ec\u30fc\u30e0\u306b\u5bfe\u3057\u3066\u3069\u3053\u3092\u30e9\u30d9\u30eb\u3068\u3059\u308b\u304b\u306f\u554f\u984c\u8a2d\u5b9a\u306b\u3088\u3063\u3066\u5909\u308f\u3063\u3066\u6765\u307e\u3059\u304c\u3001\u6700\u521d\u306e\u30d5\u30ec\u30fc\u30e0\u3092\u30e9\u30d9\u30eb\u306b\u3057\u3066\u3057\u307e\u3046\u3068\u3042\u308b\u610f\u5473\u672a\u6765\u306e\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3055\u305b\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u306e\u3067\u5927\u62b5\u306e\u5834\u5408\u306f\u6700\u5f8c\u306e\u30d5\u30ec\u30fc\u30e0\u3092\u6559\u5e2b\u30c7\u30fc\u30bf\u306b\u3057\u307e\u3059\u3002 \u3053\u3053\u3089\u8fba\u306f\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u95a2\u3059\u308b\u8a18\u4e8b\u3067\u6539\u3081\u3066\u89e6\u308c\u307e\u3059\u3002 \u6b21\u56de\u306f\u3053\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f7f\u3063\u305f\u30af\u30e9\u30b9\u5206\u985e\u30e2\u30c7\u30eb\u3092\u66f8\u304d\u307e\u3059\u3002","title":"\u3084\u308a\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/checkpoint/","text":"PyTorch Lightning\u306eCheckpointCallback\u306e\u4fbf\u5229\u6a5f\u80fd \u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u4e2d\u306b\u306a\u3093\u3067\u3082\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u3081\u3066\u30bb\u30fc\u30d6\u3067\u304d\u308b\u6a5f\u80fd\u304c\u3042\u308b\u307f\u305f\u3044\u306a\u306e\u3067\u30e1\u30e2\u3002 on_save_checkpoint def on_save_checkpoint ( self , checkpoint ): # 99% of use cases you don't need to implement this method checkpoint [ 'something_cool_i_want_to_save' ] = my_cool_pickable_object \u30b3\u30ec\u3092\u4f7f\u3046\u3068parameter\u4ee5\u5916\u306b\u63a8\u8ad6\u6642\u306b\u4f7f\u3046\u60c5\u5831\u3092checkpoint\u3068\u3057\u3066\u4fdd\u6301\u3067\u304d\u308b\u306e\u3067\u63a8\u8ad6\u6642\u306b\u624b\u52d5\u3067\u30ed\u30fc\u30c9\u3057\u305f\u308a\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u4fdd\u5b58\u3057\u3066\u304a\u304f\u624b\u9593\u304c\u7701\u3051\u308b\u3002 def on_load_checkpoint \u3053\u308c\u3067\u30ed\u30fc\u30c9\u3067\u304d\u308b def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ] Use case \u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u3068\u304b\u3092\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3068\u4e00\u7dd2\u306b\u4fdd\u5b58\u3067\u304d\u308b\u3002","title":"PyTorch Lightning\u306eCheckpointCallback\u306e\u4fbf\u5229\u6a5f\u80fd"},{"location":"PyTorch/PyTorch%20Lightning/checkpoint/#pytorch-lightningcheckpointcallback","text":"\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u4e2d\u306b\u306a\u3093\u3067\u3082\u30c1\u30a7\u30c3\u30af\u30dd\u30a4\u30f3\u30c8\u30d5\u30a1\u30a4\u30eb\u306b\u542b\u3081\u3066\u30bb\u30fc\u30d6\u3067\u304d\u308b\u6a5f\u80fd\u304c\u3042\u308b\u307f\u305f\u3044\u306a\u306e\u3067\u30e1\u30e2\u3002","title":"PyTorch Lightning\u306eCheckpointCallback\u306e\u4fbf\u5229\u6a5f\u80fd"},{"location":"PyTorch/PyTorch%20Lightning/checkpoint/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint ): # 99% of use cases you don't need to implement this method checkpoint [ 'something_cool_i_want_to_save' ] = my_cool_pickable_object \u30b3\u30ec\u3092\u4f7f\u3046\u3068parameter\u4ee5\u5916\u306b\u63a8\u8ad6\u6642\u306b\u4f7f\u3046\u60c5\u5831\u3092checkpoint\u3068\u3057\u3066\u4fdd\u6301\u3067\u304d\u308b\u306e\u3067\u63a8\u8ad6\u6642\u306b\u624b\u52d5\u3067\u30ed\u30fc\u30c9\u3057\u305f\u308a\u5225\u306e\u30d5\u30a1\u30a4\u30eb\u3067\u4fdd\u5b58\u3057\u3066\u304a\u304f\u624b\u9593\u304c\u7701\u3051\u308b\u3002","title":"on_save_checkpoint"},{"location":"PyTorch/PyTorch%20Lightning/checkpoint/#def-on_load_checkpoint","text":"\u3053\u308c\u3067\u30ed\u30fc\u30c9\u3067\u304d\u308b def on_load_checkpoint ( self , checkpoint ): # 99% of the time you don't need to implement this method self . something_cool_i_want_to_save = checkpoint [ 'something_cool_i_want_to_save' ]","title":"def on_load_checkpoint"},{"location":"PyTorch/PyTorch%20Lightning/checkpoint/#use-case","text":"\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u5171\u5206\u6563\u884c\u5217\u3068\u304b\u3092\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3068\u4e00\u7dd2\u306b\u4fdd\u5b58\u3067\u304d\u308b\u3002","title":"Use case"},{"location":"PyTorch/PyTorch%20Lightning/data_module/","text":"\u3010PyTorch Lightning\u3011LightningDataModule\u306b\u3064\u3044\u3066 \u3061\u3083\u3093\u3068\u3057\u305f\u65e5\u672c\u8a9e\u89e3\u8aac\u304c\u7121\u304b\u3063\u305f\u306e\u3067\u4eca\u5f8c\u306e\u53c2\u8003\u306b\u306a\u308c\u3070\u3068\u601d\u3044\u30e1\u30e2\u3057\u3066\u304a\u304d\u307e\u3059\u3002 \u6982\u8981 PyTorch Lightning\u3067\u30e2\u30c7\u30eb\u3092\u52d5\u304b\u3059\u6642\u306eDataLoader(\u5834\u5408\u306b\u3088\u3063\u3066\u306fDataset\u3082)\u3068\u306a\u308b\u30af\u30e9\u30b9\u3001 PyTorch\u306e\u8a72\u5f53\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u4e92\u63db\u6027\u304c\u3042\u308b\u3002\u3053\u308c\u3092\u542b\u3081\u3066\u3001 LightningModule \u304c\u30e2\u30c7\u30eb LightningDataModule \u304c\u30c7\u30fc\u30bf \u305d\u306e\u4ed6\u5fc5\u8981\u306a\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\uff08 Callbacks API , LR_FINDER \u7b49\uff09 \u3092\u66f8\u3051\u3070\u304ak LightningDataModule\u306e\u66f8\u304d\u65b9 init \u4ee5\u5916\u306b3\u3064\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5b9f\u88c5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 prepare_data \uff08\u7121\u304f\u3066\u3082\u52d5\u304f\uff09 setup ~_dataloader 0. __init__ \u5fc5\u8981\u306aparameters\u3092\u4f5c\u308b\u3002Dataset\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u306e\u3067\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u8a13\u7df4\u30c7\u30fc\u30bf\u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u5225\u308c\u3066\u308b\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002 import pytorch - lightning as pl from torch.utils.data import random_split , DataLoader from torchvision import transforms class DataModule ( pl . LightningDataModule ): def __init__ ( self , train_dir = './train' , test_dir = './test' , batch_size = 64 ): super () . __init__ () self . train_dir = train_dir self . test_dir = test_dir self . batch_size = batch_size self . transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]) self . data_augmentation = transforms . Compose ([ transforms . ToTensor (), # ... some data augmentations... transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]) \u4f59\u8ac7\uff1aalbumentations\u3068\u3044\u3046DA\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u4fbf\u5229\u3067\u3059\u3001torchvision\u306etransform\u3068\u4e92\u63db\u6027\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3053\u3053\u3067\u3082\u4f7f\u3048\u307e\u3059\u3002 1. prepare_data \u6700\u521d\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u306a\u3069 GPU\u6570\u306b\u304b\u304b\u308f\u3089\u305a\u4e00\u56de\u884c\u3044\u305f\u3044\u51e6\u7406\u3092\u66f8\u304f \u3002 \u3053\u3053\u306b\u66f8\u304f\u3053\u3068\u3067\u30de\u30eb\u30c1GPU\u3067\u3082\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u51e6\u7406\u3092\u3088\u3057\u306a\u306b\u3084\u3063\u3066\u304f\u308c\u308b\u307f\u305f\u3044\u3067\u3059\u3002 \u4f8b\u3048\u3070\u3001MNIST\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u5834\u5408 def prepare_data ( self ): # download MNIST ( self . data_dir , train = True , download = True ) MNIST ( self . data_dir , train = False , download = True ) 2. setup \uff12\u756a\u76ee\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u3059\u3002 Trainer.fit() \u3068 Trainer.test() \u304c\u547c\u3070\u308c\u305f\u6642\u306b\u7570\u306a\u308bDataset\u3092\u6d41\u3059\u51e6\u7406\u3092\u3053\u3053\u306b\u66f8\u304d\u307e\u3059\u3002 DA\u306e\u6709\u7121\u7b49\u3082\u3053\u3053\u3067\u30b9\u30a4\u30c3\u30c1\u3059\u308b\u306e\u304c\u3044\u3044\u3067\u3057\u3087\u3046\u3002 \u4f55\u304b\u3057\u3089\u306eDataset\u30af\u30e9\u30b9\u3092\u5225\u306b\u4f5c\u3063\u3066\u304a\u304f\u3068\u8aad\u307f\u3084\u3059\u3044\u3068\u601d\u3044\u307e\u3059\u3002 \u6ce8\u610f\uff1aTrainer\u304b\u3089stage\u5f15\u6570\u306b\u30e2\u30fc\u30c9\u304c\u6587\u5b57\u5217\u3068\u3057\u3066\u6e21\u3055\u308c\u3066\u304f\u308b\u3088\u3046\u3067\u3059\u304c\u3001None\u306b\u306a\u3063\u305f\u6642\u306e\u51e6\u7406\u3092 \u66f8\u3044\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002setup\u3092\u624b\u52d5\u3067\u547c\u3076\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 \u6ce8\u610f\uff12\uff1a\u30de\u30eb\u30c1GPU\u306e\u5834\u5408\u5404GPU\u304b\u3089\u4e00\u56de\u3065\u3064\u547c\u3070\u308c\u307e\u3059\u3002 def setup ( self , stage = None ): if stage == 'fit' or stage is None : self . train_set = MyDataset ( self . train_dir , transform = self . data_augmentation ) size = len ( self . train_set ) t , v = ( int ( size * 0.9 ), int ( size * 0.1 )) # if using holdout method t += ( t + v != size ) self . train_set , self . valid_set = random_split ( self . train_set , [ t , v ]) if stage == 'test' or stage is None : self . test_set = MyDataset ( self . test_dir , transform = self . transform ) 3. ~_dataloader \u6700\u5f8c\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u3001Dataloader\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u8fd4\u3057\u307e\u3059\u3002 \u8a13\u7df4\u3001\u691c\u8a3c\u3001\u30c6\u30b9\u30c8\u3088\u3046\u306b\u4e09\u3064\u66f8\u304d\u307e\u3059\u3002 def train_dataloader ( self ): return DataLoader ( self . train_set , batch_size = self . batch_size , ) def val_dataloader ( self ): return DataLoader ( self . test_set , batch_size = self . batch_size , ) def test_dataloader ( self ): return DataLoader ( self . valid_set , batch_size = self . batch_size , ) \u5fc5\u8981\u306a\u30e1\u30bd\u30c3\u30c9\u306f\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002 EXTRA: LightningDataModule\u3092\u4f7f\u3046 \u901a\u5e38\u306e\u5834\u5408, dm = DataModule () model = Model () trainer . fit ( model , dm ) trainer . test ( datamodule = dm ) \u3067\u4e0a\u8a18\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u52dd\u624b\u306b\u547c\u3093\u3067\u8a13\u7df4\u307e\u3067\u884c\u3063\u3066\u304f\u308c\u307e\u3059\u3002 \u304c\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u30e2\u30c7\u30eb\u3092\u751f\u6210\u3059\u308b\u6642\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831\uff08\u30af\u30e9\u30b9\u6570\u3084\u753b\u50cf\u30b5\u30a4\u30ba\u3001\u3061\u3083\u3093\u306d\u308b\u6570\uff09\u304c \u5fc5\u8981\u306b\u306a\u308b\u306e\u3067\u305d\u306e\u6642\u306f setup \u5185\u306b\u5fc5\u8981\u306a\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308b\u51e6\u7406\u3092\u8a18\u8f09\u3057\u3066\u304b\u3089 dm = DataModule () dm . prepare_data () dm . setup ( 'fit' ) \u3000 # \u30a2\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u306b\u60c5\u5831\u3092\u683c\u7d0d\u3057\u3066\u304a\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u304a\u304f\u3053\u3068 model = Model ( num_classes = dm . num_classes , width = dm .= img_size ) trainer . fit ( model , dm )","title":"\u3010PyTorch Lightning\u3011LightningDataModule\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#pytorch-lightninglightningdatamodule","text":"\u3061\u3083\u3093\u3068\u3057\u305f\u65e5\u672c\u8a9e\u89e3\u8aac\u304c\u7121\u304b\u3063\u305f\u306e\u3067\u4eca\u5f8c\u306e\u53c2\u8003\u306b\u306a\u308c\u3070\u3068\u601d\u3044\u30e1\u30e2\u3057\u3066\u304a\u304d\u307e\u3059\u3002","title":"\u3010PyTorch Lightning\u3011LightningDataModule\u306b\u3064\u3044\u3066"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#_1","text":"PyTorch Lightning\u3067\u30e2\u30c7\u30eb\u3092\u52d5\u304b\u3059\u6642\u306eDataLoader(\u5834\u5408\u306b\u3088\u3063\u3066\u306fDataset\u3082)\u3068\u306a\u308b\u30af\u30e9\u30b9\u3001 PyTorch\u306e\u8a72\u5f53\u30e2\u30b8\u30e5\u30fc\u30eb\u3068\u4e92\u63db\u6027\u304c\u3042\u308b\u3002\u3053\u308c\u3092\u542b\u3081\u3066\u3001 LightningModule \u304c\u30e2\u30c7\u30eb LightningDataModule \u304c\u30c7\u30fc\u30bf \u305d\u306e\u4ed6\u5fc5\u8981\u306a\u30ab\u30b9\u30bf\u30de\u30a4\u30ba\uff08 Callbacks API , LR_FINDER \u7b49\uff09 \u3092\u66f8\u3051\u3070\u304ak","title":"\u6982\u8981"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#lightningdatamodule","text":"init \u4ee5\u5916\u306b3\u3064\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u5b9f\u88c5\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 prepare_data \uff08\u7121\u304f\u3066\u3082\u52d5\u304f\uff09 setup ~_dataloader","title":"LightningDataModule\u306e\u66f8\u304d\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#0-__init__","text":"\u5fc5\u8981\u306aparameters\u3092\u4f5c\u308b\u3002Dataset\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u4f5c\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u306e\u3067\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 \u4ee5\u4e0b\u306e\u4f8b\u3067\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u8a13\u7df4\u30c7\u30fc\u30bf\u304c\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3067\u5225\u308c\u3066\u308b\u3068\u4eee\u5b9a\u3057\u307e\u3059\u3002 import pytorch - lightning as pl from torch.utils.data import random_split , DataLoader from torchvision import transforms class DataModule ( pl . LightningDataModule ): def __init__ ( self , train_dir = './train' , test_dir = './test' , batch_size = 64 ): super () . __init__ () self . train_dir = train_dir self . test_dir = test_dir self . batch_size = batch_size self . transform = transforms . Compose ([ transforms . ToTensor (), transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ]) self . data_augmentation = transforms . Compose ([ transforms . ToTensor (), # ... some data augmentations... transforms . Normalize (( 0.1307 ,), ( 0.3081 ,)) ])","title":"0. __init__"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#albumentationsdatorchvisiontransform","text":"","title":"\u4f59\u8ac7\uff1aalbumentations\u3068\u3044\u3046DA\u30e9\u30a4\u30d6\u30e9\u30ea\u304c\u4fbf\u5229\u3067\u3059\u3001torchvision\u306etransform\u3068\u4e92\u63db\u6027\u304c\u3042\u308a\u307e\u3059\u306e\u3067\u3053\u3053\u3067\u3082\u4f7f\u3048\u307e\u3059\u3002"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#1-prepare_data","text":"\u6700\u521d\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u30c7\u30fc\u30bf\u306e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u306a\u3069 GPU\u6570\u306b\u304b\u304b\u308f\u3089\u305a\u4e00\u56de\u884c\u3044\u305f\u3044\u51e6\u7406\u3092\u66f8\u304f \u3002 \u3053\u3053\u306b\u66f8\u304f\u3053\u3068\u3067\u30de\u30eb\u30c1GPU\u3067\u3082\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u51e6\u7406\u3092\u3088\u3057\u306a\u306b\u3084\u3063\u3066\u304f\u308c\u308b\u307f\u305f\u3044\u3067\u3059\u3002 \u4f8b\u3048\u3070\u3001MNIST\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u5834\u5408 def prepare_data ( self ): # download MNIST ( self . data_dir , train = True , download = True ) MNIST ( self . data_dir , train = False , download = True )","title":"1. prepare_data"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#2-setup","text":"\uff12\u756a\u76ee\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u3059\u3002 Trainer.fit() \u3068 Trainer.test() \u304c\u547c\u3070\u308c\u305f\u6642\u306b\u7570\u306a\u308bDataset\u3092\u6d41\u3059\u51e6\u7406\u3092\u3053\u3053\u306b\u66f8\u304d\u307e\u3059\u3002 DA\u306e\u6709\u7121\u7b49\u3082\u3053\u3053\u3067\u30b9\u30a4\u30c3\u30c1\u3059\u308b\u306e\u304c\u3044\u3044\u3067\u3057\u3087\u3046\u3002 \u4f55\u304b\u3057\u3089\u306eDataset\u30af\u30e9\u30b9\u3092\u5225\u306b\u4f5c\u3063\u3066\u304a\u304f\u3068\u8aad\u307f\u3084\u3059\u3044\u3068\u601d\u3044\u307e\u3059\u3002 \u6ce8\u610f\uff1aTrainer\u304b\u3089stage\u5f15\u6570\u306b\u30e2\u30fc\u30c9\u304c\u6587\u5b57\u5217\u3068\u3057\u3066\u6e21\u3055\u308c\u3066\u304f\u308b\u3088\u3046\u3067\u3059\u304c\u3001None\u306b\u306a\u3063\u305f\u6642\u306e\u51e6\u7406\u3092 \u66f8\u3044\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002setup\u3092\u624b\u52d5\u3067\u547c\u3076\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002 \u6ce8\u610f\uff12\uff1a\u30de\u30eb\u30c1GPU\u306e\u5834\u5408\u5404GPU\u304b\u3089\u4e00\u56de\u3065\u3064\u547c\u3070\u308c\u307e\u3059\u3002 def setup ( self , stage = None ): if stage == 'fit' or stage is None : self . train_set = MyDataset ( self . train_dir , transform = self . data_augmentation ) size = len ( self . train_set ) t , v = ( int ( size * 0.9 ), int ( size * 0.1 )) # if using holdout method t += ( t + v != size ) self . train_set , self . valid_set = random_split ( self . train_set , [ t , v ]) if stage == 'test' or stage is None : self . test_set = MyDataset ( self . test_dir , transform = self . transform )","title":"2. setup"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#3-_dataloader","text":"\u6700\u5f8c\u306b\u547c\u3070\u308c\u308b\u30e1\u30bd\u30c3\u30c9\u3067\u3001Dataloader\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u8fd4\u3057\u307e\u3059\u3002 \u8a13\u7df4\u3001\u691c\u8a3c\u3001\u30c6\u30b9\u30c8\u3088\u3046\u306b\u4e09\u3064\u66f8\u304d\u307e\u3059\u3002 def train_dataloader ( self ): return DataLoader ( self . train_set , batch_size = self . batch_size , ) def val_dataloader ( self ): return DataLoader ( self . test_set , batch_size = self . batch_size , ) def test_dataloader ( self ): return DataLoader ( self . valid_set , batch_size = self . batch_size , ) \u5fc5\u8981\u306a\u30e1\u30bd\u30c3\u30c9\u306f\u4ee5\u4e0a\u306b\u306a\u308a\u307e\u3059\u3002","title":"3. ~_dataloader"},{"location":"PyTorch/PyTorch%20Lightning/data_module/#extra-lightningdatamodule","text":"\u901a\u5e38\u306e\u5834\u5408, dm = DataModule () model = Model () trainer . fit ( model , dm ) trainer . test ( datamodule = dm ) \u3067\u4e0a\u8a18\u306e\u30e1\u30bd\u30c3\u30c9\u3092\u52dd\u624b\u306b\u547c\u3093\u3067\u8a13\u7df4\u307e\u3067\u884c\u3063\u3066\u304f\u308c\u307e\u3059\u3002 \u304c\u3001\u5834\u5408\u306b\u3088\u3063\u3066\u306f\u30e2\u30c7\u30eb\u3092\u751f\u6210\u3059\u308b\u6642\u306b\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u60c5\u5831\uff08\u30af\u30e9\u30b9\u6570\u3084\u753b\u50cf\u30b5\u30a4\u30ba\u3001\u3061\u3083\u3093\u306d\u308b\u6570\uff09\u304c \u5fc5\u8981\u306b\u306a\u308b\u306e\u3067\u305d\u306e\u6642\u306f setup \u5185\u306b\u5fc5\u8981\u306a\u60c5\u5831\u3092\u53ce\u96c6\u3059\u308b\u51e6\u7406\u3092\u8a18\u8f09\u3057\u3066\u304b\u3089 dm = DataModule () dm . prepare_data () dm . setup ( 'fit' ) \u3000 # \u30a2\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u306b\u60c5\u5831\u3092\u683c\u7d0d\u3057\u3066\u304a\u3051\u308b\u3088\u3046\u306b\u3057\u3066\u304a\u304f\u3053\u3068 model = Model ( num_classes = dm . num_classes , width = dm .= img_size ) trainer . fit ( model , dm )","title":"EXTRA: LightningDataModule\u3092\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/","text":"PyTorch Lightning Bolts\u306e\u4f7f\u3044\u65b9 \u4fbf\u5229\u306a\u6642\u4ee3\u306b\u306a\u308a\u307e\u3057\u305f\u306d\u3002 Bolts\u3068\u306f\uff1f PyTorch Lightning\u7528\u306e\u4fbf\u5229\u30b3\u30fc\u30c9\u304c\u8a70\u307e\u3063\u305f\u516c\u5f0f\u30e9\u30a4\u30d6\u30e9\u30ea\u3002 \u8a13\u7df4\u6e08\u307fSOTA\u30e2\u30c7\u30eb \u3088\u304f\u4f7f\u3046\u30e2\u30c7\u30eb\u30b3\u30f3\u30dd\u30cd\u30fc\u30c8 Callback\u7528\u306e\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30d5\u30c3\u30af \u30ed\u30b9\u95a2\u6570 \u6709\u540d\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3053\u308c\u3089\u304cPyTorch Lightning\u3067\u76f4\u3050\u306b\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u3066\u3068\u3063\u3066\u3082\u4fbf\u5229\u3002 \u4ee5\u4e0b\u3001\u4f7f\u3044\u65b9\u306e\u4f8b 1.\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u305d\u306e\u307e\u307e\u4f7f\u3046 \u6700\u65b0\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u6642\u306a\u3069\u306f from pl_bolts.models.self_supervised import SwAV weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar' # weight file of ImageNet swav = SwAV . load_from_checkpoint ( weight_path , strict = True ) swav . freeze () \u4ee5\u964d swav \u306f\u901a\u5e38\u306e nn.Module \u3068\u3057\u3066\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002 2.\u30b3\u30f3\u30dd\u30cd\u30fc\u30c8\u5358\u4f4d\u3067\u4f7f\u3046 \u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u306b\u3057\u305f\u308a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\u3060\u3051\u63a1\u7528\u3057\u305f\u308a\u3082\u3067\u304d\u308b\u3002 \u4eca\u56de\u306f ResNet152 \u306e\u5165\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\u30923\u304b\u30894\u306b\u3057\u3066\u307f\u305f\u3002 from pl_bolts.models.self_supervised.resnets import resnet152 model = resnet152 ( pretrained = True ) temp_weight = model . conv1 . weight . data . clone () # \u65e2\u5b58\u306e\u91cd\u307f\u3092\u9000\u907f model . conv1 = nn . Conv2d ( 4 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) # input_channel\u3060\u3051\u5897\u3084\u3059 model . conv1 . weight . data [:, : 3 ] = temp_weight # 3 channel\u307e\u3067\u306f\u65e2\u5b58\u306e\u91cd\u307f model . conv1 . weight . data [:, 3 ] = model . conv1 . weight . data [:, 0 ] # R\u306e\u91cd\u307f\u30924\u3064\u3081\u306e\u91cd\u307f\u3068\u3057\u3066\u63a1\u7528 \u9006\u306b model.conv1 \u3060\u3051\u3092\u5225\u306e\u30e2\u30c7\u30eb\u306b\u4f7f\u3063\u305f\u308a\u3082\u3067\u304d\u308b\u3002 3.\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f7f\u3046 PyTorch Lightning \u306e Callback \u3000API\u7528\u306e\u4fbf\u5229\u306a\u51e6\u7406\u304c\u8272\u3005\u63c3\u3063\u3066\u308b\u3002 \u5fc5\u8981\u306aCallback\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u5ba3\u8a00\u3057\u3066\u305d\u308c\u3089\u306e\u30ea\u30b9\u30c8\u3092\u30c8\u30ec\u30a4\u30ca\u30fc\u306b\u6e21\u3059\u3060\u3051\u3067\u4f7f\u3048\u308b \u3002 \u3053\u3053\u3067\u306f\u4e8c\u3064\u7d39\u4ecb\u3002 \u30a8\u30dd\u30c3\u30af\u6bce\u306b\u30ed\u30b9\u3092\u8868\u793a\u3059\u308b from pl_bolts.callbacks import PrintTableMetricsCallback print_callback = PrintTableMetricsCallback () trainer pl . Trainer ( callback = [ print_callback ]) trainer . fit ( model ) GAN\u306eforward\u6642\u306b\u751f\u6210\u3057\u305f\u753b\u50cf\u3092TensorBoard\u306b\u8868\u793a\u3059\u308b model . img_dim = ( 1 , 28 , 28 ) # model forward must work for sampling z = torch . rand ( batch_size , latent_dim ) img_samples = GAN ( z ) from pl_bolts.callbacks import TensorboardGenerativeModelImageSampler trainer = Trainer ( callbacks = [ TensorboardGenerativeModelImageSampler ()]) trainer . fit ( GAN ) 4.\u30ed\u30b9\u95a2\u6570\u3092\u4f7f\u3046 \u30bf\u30b9\u30af\u6bce\u306b\u3044\u304f\u3064\u304b\u306e\u95a2\u6570\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u304c\u307e\u3060\u6570\u304c\u5c11\u306a\u3044\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u3068PyTorch\u306e\u30ed\u30b9\u95a2\u6570\u306b\u306a\u308b\u306e\u3067\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3059\u308b\u6642\u306b lossfun \u30e1\u30bd\u30c3\u30c9\u306b\u6e21\u3057\u3066\u3042\u3052\u308c\u3070\u3044\u3044\u3002 \u4e0b\u8a18\u306f\u7269\u4f53\u691c\u77e5\u7528\u306eGeneralizedIoU\u3002 >>> import torch >>> from pl_bolts.losses.object_detection import giou_loss >>> preds = torch . tensor ([[ 100 , 100 , 200 , 200 ]]) >>> target = torch . tensor ([[ 150 , 150 , 250 , 250 ]]) >>> giou_loss ( preds , target ) tensor ([[ 1.0794 ]]) def lossfun ( self , y , t ): # method of a network return giou_loss ( y , t ) 5.\u30c7\u30fc\u30bf\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046 LightningDataModule \u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u63c3\u3063\u3066\u3044\u308b\u3002 \u6307\u5b9a\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u6a5f\u80fd\u3082\u3042\u308b\u306e\u3067\u30e2\u30c7\u30eb\u306e\u30c6\u30b9\u30c8\u3092\u76f4\u3050\u306b\u59cb\u3081\u3089\u308c\u308b\u3002\u30de\u30eb\u30c1GPU\u5bfe\u5fdc\u3002 DA\u3092\u81ea\u5206\u3067\u5909\u66f4\u3057\u305f\u308a\u3067\u304d\u308b from pl_bolts.datamodules import CIFAR10DataModule dm = CIFAR10DataModule ( 'PATH_to_download/to_load' ) dm . train_transforms = ... # \u3053\u3053\u306bCompose\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u305b\u3070\u304ak dm . test_transforms = ... dm . val_transforms = ... Numpy\u3067x,y\u3092\u6e21\u3059\u3060\u3051\u3067 LightningDataModule \u5316\u3057\u3066\u304f\u308c\u3066\u51c4\u3044\u3068\u601d\u3044\u307e\u3057\u305f\uff08\u8a9e\u5f59\u529b\uff09 >>> from sklearn.datasets import load_boston >>> from pl_bolts.datamodules import SklearnDataset ... >>> X , y = load_boston ( return_X_y = True ) >>> dataset = SklearnDataset ( X , y ) # \u30b7\u30a7\u30a4\u30d7\u304c\u3042\u3063\u3066\u308c\u3070\u306a\u3093\u3067\u3082\u6e21\u305b\u308b","title":"PyTorch Lightning Bolts\u306e\u4f7f\u3044\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#pytorch-lightning-bolts","text":"\u4fbf\u5229\u306a\u6642\u4ee3\u306b\u306a\u308a\u307e\u3057\u305f\u306d\u3002","title":"PyTorch Lightning Bolts\u306e\u4f7f\u3044\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#bolts","text":"PyTorch Lightning\u7528\u306e\u4fbf\u5229\u30b3\u30fc\u30c9\u304c\u8a70\u307e\u3063\u305f\u516c\u5f0f\u30e9\u30a4\u30d6\u30e9\u30ea\u3002 \u8a13\u7df4\u6e08\u307fSOTA\u30e2\u30c7\u30eb \u3088\u304f\u4f7f\u3046\u30e2\u30c7\u30eb\u30b3\u30f3\u30dd\u30cd\u30fc\u30c8 Callback\u7528\u306e\u30d5\u30a9\u30ef\u30fc\u30c9\u3001\u30d0\u30c3\u30af\u30d5\u30c3\u30af \u30ed\u30b9\u95a2\u6570 \u6709\u540d\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 \u3053\u308c\u3089\u304cPyTorch Lightning\u3067\u76f4\u3050\u306b\u4f7f\u3048\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u3066\u3068\u3063\u3066\u3082\u4fbf\u5229\u3002 \u4ee5\u4e0b\u3001\u4f7f\u3044\u65b9\u306e\u4f8b","title":"Bolts\u3068\u306f\uff1f"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#1","text":"\u6700\u65b0\u306e\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u3092\u4f7f\u3063\u3066\u307f\u305f\u3044\u6642\u306a\u3069\u306f from pl_bolts.models.self_supervised import SwAV weight_path = 'https://pl-bolts-weights.s3.us-east-2.amazonaws.com/swav/swav_imagenet/swav_imagenet.pth.tar' # weight file of ImageNet swav = SwAV . load_from_checkpoint ( weight_path , strict = True ) swav . freeze () \u4ee5\u964d swav \u306f\u901a\u5e38\u306e nn.Module \u3068\u3057\u3066\u6271\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002","title":"1.\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u305d\u306e\u307e\u307e\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#2","text":"\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u306b\u3057\u305f\u308a\u30a8\u30f3\u30b3\u30fc\u30c0\u30fc\u90e8\u5206\u3060\u3051\u63a1\u7528\u3057\u305f\u308a\u3082\u3067\u304d\u308b\u3002 \u4eca\u56de\u306f ResNet152 \u306e\u5165\u529b\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\u30923\u304b\u30894\u306b\u3057\u3066\u307f\u305f\u3002 from pl_bolts.models.self_supervised.resnets import resnet152 model = resnet152 ( pretrained = True ) temp_weight = model . conv1 . weight . data . clone () # \u65e2\u5b58\u306e\u91cd\u307f\u3092\u9000\u907f model . conv1 = nn . Conv2d ( 4 , 64 , kernel_size = 7 , stride = 2 , padding = 3 , bias = False ) # input_channel\u3060\u3051\u5897\u3084\u3059 model . conv1 . weight . data [:, : 3 ] = temp_weight # 3 channel\u307e\u3067\u306f\u65e2\u5b58\u306e\u91cd\u307f model . conv1 . weight . data [:, 3 ] = model . conv1 . weight . data [:, 0 ] # R\u306e\u91cd\u307f\u30924\u3064\u3081\u306e\u91cd\u307f\u3068\u3057\u3066\u63a1\u7528 \u9006\u306b model.conv1 \u3060\u3051\u3092\u5225\u306e\u30e2\u30c7\u30eb\u306b\u4f7f\u3063\u305f\u308a\u3082\u3067\u304d\u308b\u3002","title":"2.\u30b3\u30f3\u30dd\u30cd\u30fc\u30c8\u5358\u4f4d\u3067\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#3","text":"PyTorch Lightning \u306e Callback \u3000API\u7528\u306e\u4fbf\u5229\u306a\u51e6\u7406\u304c\u8272\u3005\u63c3\u3063\u3066\u308b\u3002 \u5fc5\u8981\u306aCallback\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u5ba3\u8a00\u3057\u3066\u305d\u308c\u3089\u306e\u30ea\u30b9\u30c8\u3092\u30c8\u30ec\u30a4\u30ca\u30fc\u306b\u6e21\u3059\u3060\u3051\u3067\u4f7f\u3048\u308b \u3002 \u3053\u3053\u3067\u306f\u4e8c\u3064\u7d39\u4ecb\u3002 \u30a8\u30dd\u30c3\u30af\u6bce\u306b\u30ed\u30b9\u3092\u8868\u793a\u3059\u308b from pl_bolts.callbacks import PrintTableMetricsCallback print_callback = PrintTableMetricsCallback () trainer pl . Trainer ( callback = [ print_callback ]) trainer . fit ( model ) GAN\u306eforward\u6642\u306b\u751f\u6210\u3057\u305f\u753b\u50cf\u3092TensorBoard\u306b\u8868\u793a\u3059\u308b model . img_dim = ( 1 , 28 , 28 ) # model forward must work for sampling z = torch . rand ( batch_size , latent_dim ) img_samples = GAN ( z ) from pl_bolts.callbacks import TensorboardGenerativeModelImageSampler trainer = Trainer ( callbacks = [ TensorboardGenerativeModelImageSampler ()]) trainer . fit ( GAN )","title":"3.\u30b3\u30fc\u30eb\u30d0\u30c3\u30af\u3092\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#4","text":"\u30bf\u30b9\u30af\u6bce\u306b\u3044\u304f\u3064\u304b\u306e\u95a2\u6570\u304c\u5b9f\u88c5\u3055\u308c\u3066\u3044\u308b\u304c\u307e\u3060\u6570\u304c\u5c11\u306a\u3044\u3002 \u30a4\u30f3\u30dd\u30fc\u30c8\u3059\u308b\u3068PyTorch\u306e\u30ed\u30b9\u95a2\u6570\u306b\u306a\u308b\u306e\u3067\u30e2\u30c7\u30eb\u30af\u30e9\u30b9\u3092\u4f5c\u6210\u3059\u308b\u6642\u306b lossfun \u30e1\u30bd\u30c3\u30c9\u306b\u6e21\u3057\u3066\u3042\u3052\u308c\u3070\u3044\u3044\u3002 \u4e0b\u8a18\u306f\u7269\u4f53\u691c\u77e5\u7528\u306eGeneralizedIoU\u3002 >>> import torch >>> from pl_bolts.losses.object_detection import giou_loss >>> preds = torch . tensor ([[ 100 , 100 , 200 , 200 ]]) >>> target = torch . tensor ([[ 150 , 150 , 250 , 250 ]]) >>> giou_loss ( preds , target ) tensor ([[ 1.0794 ]]) def lossfun ( self , y , t ): # method of a network return giou_loss ( y , t )","title":"4.\u30ed\u30b9\u95a2\u6570\u3092\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/lightning_bolts/#5","text":"LightningDataModule \u5316\u3055\u308c\u305f\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u63c3\u3063\u3066\u3044\u308b\u3002 \u6307\u5b9a\u3057\u305f\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3059\u308b\u6a5f\u80fd\u3082\u3042\u308b\u306e\u3067\u30e2\u30c7\u30eb\u306e\u30c6\u30b9\u30c8\u3092\u76f4\u3050\u306b\u59cb\u3081\u3089\u308c\u308b\u3002\u30de\u30eb\u30c1GPU\u5bfe\u5fdc\u3002 DA\u3092\u81ea\u5206\u3067\u5909\u66f4\u3057\u305f\u308a\u3067\u304d\u308b from pl_bolts.datamodules import CIFAR10DataModule dm = CIFAR10DataModule ( 'PATH_to_download/to_load' ) dm . train_transforms = ... # \u3053\u3053\u306bCompose\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u6e21\u305b\u3070\u304ak dm . test_transforms = ... dm . val_transforms = ... Numpy\u3067x,y\u3092\u6e21\u3059\u3060\u3051\u3067 LightningDataModule \u5316\u3057\u3066\u304f\u308c\u3066\u51c4\u3044\u3068\u601d\u3044\u307e\u3057\u305f\uff08\u8a9e\u5f59\u529b\uff09 >>> from sklearn.datasets import load_boston >>> from pl_bolts.datamodules import SklearnDataset ... >>> X , y = load_boston ( return_X_y = True ) >>> dataset = SklearnDataset ( X , y ) # \u30b7\u30a7\u30a4\u30d7\u304c\u3042\u3063\u3066\u308c\u3070\u306a\u3093\u3067\u3082\u6e21\u305b\u308b","title":"5.\u30c7\u30fc\u30bf\u30e2\u30b8\u30e5\u30fc\u30eb\u3092\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/","text":"PyTorchVideo \u4f7f\u3044\u65b9 \u8272\u3005\u4fbf\u5229\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u30e1\u30e2 \u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u3046 1. \u30e2\u30c7\u30eb\u306e\u30ed\u30fc\u30c9\uff08torch hub\u7d4c\u7531\uff09 master branch\u306b\u5c02\u7528\u306e hubconf.py \u304c\u3042\u308b\u306e\u3067\u305d\u308c\u3078\u306e\u30d1\u30b9\u3092\u8a18\u8ff0\u3057\u3066,string\u3067\u30e2\u30c7\u30eb\u3092\u6307\u5b9a\u3057\u3066\u30ed\u30fc\u30c9\u3059\u308b\u3002 model_name = \"slow_r50\" path = 'path/to/directory/of/hubconf.py' model = torch . hub . load ( path , source = \"local\" , model = model_name , pretrained = True ) \u3061\u306a\u307f\u306b\u30e2\u30c7\u30eb\u306f 'pytorchvideo.models.net.Net' \u3068\u3044\u3046\u578b\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u30e9\u30a4\u30c8\u30cb\u30f3\u30b0\u306b\u7d44\u307f\u8fbc\u3080\u3068\u304d\u306f\u30a2\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u306b\u3059\u308b\u3002 ### 2. \u5165\u529b\u52d5\u753b\u3092\u898f\u5b9a\u306e\u5f62\u306b\u3067\u304d\u308b\u69d8Transform\u3092\u6e96\u5099\u3059\u308b \u52d5\u753b\u306e\u30e2\u30c7\u30eb\u3054\u3068\u306e\u898f\u683c\u3092\u5408\u308f\u305b\u308b\u3002 slow_50 \u306f256\u56db\u65b9\u304b\u3064RGB\u304c\u6a19\u6e96\u5316\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u307e\u305f\u3001\uff11\u30a4\u30f3\u30d7\u30c3\u30c8\u304c\u4f55\u30d5\u30ec\u30fc\u30e0\u304b\u3082\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u9055\u3046\u306e\u3067\u3042\u3089\u304b\u3058\u3081\u66f8\u3044\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002 \u8a73\u3057\u304f\u306f \u3053\u3061\u3089 ```python from pytorchvideo.transforms import ( ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample ) side_size = 256 mean = [0.45, 0.45, 0.45] std = [0.225, 0.225, 0.225] crop_size = 256 num_frames = 8 sampling_rate = 8 frames_per_second = 30 clip_duration = (num_frames * sampling_rate)/frames_per_second transform = ApplyTransformToKey( key=\"video\", transform=Compose( [ UniformTemporalSubsample(num_frames), Lambda(lambda x: x/255.0), NormalizeVideo(mean, std), ShortSideScale( size=side_size ), CenterCropVideo(crop_size=(crop_size, crop_size)) ] ), ) ``` 3. \u52d5\u753b\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b \u9069\u5f53\u306a\u52d5\u753b\u3092\u7528\u610f\u3057\u3066\u304a\u3051\u3070\u30a8\u30f3\u30b3\u30fc\u30c9\u3082\u4efb\u305b\u3089\u308c\u308b\u3002 .avi \u3067\u3082\u3044\u3051\u307e\u3057\u305f\u3002 \u624b\u9806\u3068\u3057\u3066\u306f 1. \u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b 2. \u79d2\u3092\u6307\u5b9a\u3057\u3066\u5207\u308a\u53d6\u308b 3. \u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30e0\u3092\u901a\u3059 from pytorchvideo.data.encoded_video import EncodedVideo sample_path = 'sample.avi' # 1 video = EncodedVideo . from_path ( sample_path ) # 2 video_cliped = video . get_clip ( start_sec = 0 , end_sec = 10 ) # 3 video_data = transform ( video_data ) 4. \u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b \u5909\u63db\u3057\u305f\u52d5\u753b\u306f\u8f9e\u66f8\u578b\u306b\u306a\u3063\u3066\u3066 video \u3067\u52d5\u753b\uff08C, T, H, W\uff09 audio \u3067\u97f3\u58f0, video_name \u3067\u5143\u306e\u30d1\u30b9\u304c\u53d6\u5f97\u3067\u304d\u308b\u3002 \u30e2\u30c7\u30eb\u306f (batch_size, C, T, H, W) shape\u306etensor\u3092\u3068\u308b\u306e\u3067 input = video_data [ 'video' ] prediction = model ( input . unsqueeze ( 0 )) \u304a\u307e\u3051 Lightning\u7d44\u307f\u8fbc\u3080 \u3084\u308b\u3053\u3068\u306f\u4e8c\u70b9 model\u3092\u7d44\u307f\u8fbc\u3080 DataModule\u3092\u66f8\u304f(Transform + label\u4ed8\u4e0e) class VideoClassification ( pytorch_lightning . LightningModule ): def __init__ ( self , path ): super () . __init__ () self . model = torch . hub . load ( path , source = \"local\" , model = model_name , pretrained = True ) def configure_optimizers ( self ): return torch . optim . Adam ( self . parameters (), lr = 1e-1 ) def forward ( self , x ): return self . model ( x ) def training_step ( self , batch , batch_idx ): y = self . model ( batch [ \"video\" ]) t = batch [ \"label\" ] loss = F . cross_entropy ( y , t ) return loss from pytorchvideo.transforms import ( ApplyTransformToKey , RandomShortSideScale , RemoveKey , ShortSideScale , UniformTemporalSubsample ) from torchvision.transforms import ( Compose , Normalize , RandomCrop , RandomHorizontalFlip ) class KineticsDataModule ( pytorch_lightning . LightningDataModule ): def setup ( self ); self . train_dataset = pytorchvideo . data . Kinetics ( data_path = os . path . join ( self . _DATA_PATH , \"train.csv\" ), clip_sampler = pytorchvideo . data . make_clip_sampler ( \"random\" , self . _CLIP_DURATION ), transform = train_transform ) def train_dataloader ( self ): train_transform = Compose ( [ ApplyTransformToKey ( key = \"video\" , transform = Compose ( [ UniformTemporalSubsample ( 8 ), Normalize (( 0.45 , 0.45 , 0.45 ), ( 0.225 , 0.225 , 0.225 )), RandomShortSideScale ( min_size = 256 , max_size = 320 ), RandomCrop ( 244 ), RandomHorizontalFlip ( p = 0.5 ), ] ), ), ] ) return torch . utils . data . DataLoader ( train_dataset , batch_size = self . _BATCH_SIZE , num_workers = self . _NUM_WORKERS , )","title":"PyTorchVideo \u4f7f\u3044\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#pytorchvideo","text":"\u8272\u3005\u4fbf\u5229\u305d\u3046\u3060\u3063\u305f\u306e\u3067\u30e1\u30e2","title":"PyTorchVideo \u4f7f\u3044\u65b9"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#_1","text":"","title":"\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u4f7f\u3046"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#1-torch-hub","text":"master branch\u306b\u5c02\u7528\u306e hubconf.py \u304c\u3042\u308b\u306e\u3067\u305d\u308c\u3078\u306e\u30d1\u30b9\u3092\u8a18\u8ff0\u3057\u3066,string\u3067\u30e2\u30c7\u30eb\u3092\u6307\u5b9a\u3057\u3066\u30ed\u30fc\u30c9\u3059\u308b\u3002 model_name = \"slow_r50\" path = 'path/to/directory/of/hubconf.py' model = torch . hub . load ( path , source = \"local\" , model = model_name , pretrained = True ) \u3061\u306a\u307f\u306b\u30e2\u30c7\u30eb\u306f 'pytorchvideo.models.net.Net' \u3068\u3044\u3046\u578b\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u30e9\u30a4\u30c8\u30cb\u30f3\u30b0\u306b\u7d44\u307f\u8fbc\u3080\u3068\u304d\u306f\u30a2\u30c8\u30ea\u30d3\u30e5\u30fc\u30c8\u306b\u3059\u308b\u3002 ### 2. \u5165\u529b\u52d5\u753b\u3092\u898f\u5b9a\u306e\u5f62\u306b\u3067\u304d\u308b\u69d8Transform\u3092\u6e96\u5099\u3059\u308b \u52d5\u753b\u306e\u30e2\u30c7\u30eb\u3054\u3068\u306e\u898f\u683c\u3092\u5408\u308f\u305b\u308b\u3002 slow_50 \u306f256\u56db\u65b9\u304b\u3064RGB\u304c\u6a19\u6e96\u5316\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\u307e\u305f\u3001\uff11\u30a4\u30f3\u30d7\u30c3\u30c8\u304c\u4f55\u30d5\u30ec\u30fc\u30e0\u304b\u3082\u30e2\u30c7\u30eb\u306b\u3088\u3063\u3066\u9055\u3046\u306e\u3067\u3042\u3089\u304b\u3058\u3081\u66f8\u3044\u3066\u304a\u304f\u5fc5\u8981\u304c\u3042\u308b\u3002 \u8a73\u3057\u304f\u306f \u3053\u3061\u3089 ```python from pytorchvideo.transforms import ( ApplyTransformToKey, ShortSideScale, UniformTemporalSubsample ) side_size = 256 mean = [0.45, 0.45, 0.45] std = [0.225, 0.225, 0.225] crop_size = 256 num_frames = 8 sampling_rate = 8 frames_per_second = 30 clip_duration = (num_frames * sampling_rate)/frames_per_second transform = ApplyTransformToKey( key=\"video\", transform=Compose( [ UniformTemporalSubsample(num_frames), Lambda(lambda x: x/255.0), NormalizeVideo(mean, std), ShortSideScale( size=side_size ), CenterCropVideo(crop_size=(crop_size, crop_size)) ] ), ) ```","title":"1. \u30e2\u30c7\u30eb\u306e\u30ed\u30fc\u30c9\uff08torch hub\u7d4c\u7531\uff09"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#3","text":"\u9069\u5f53\u306a\u52d5\u753b\u3092\u7528\u610f\u3057\u3066\u304a\u3051\u3070\u30a8\u30f3\u30b3\u30fc\u30c9\u3082\u4efb\u305b\u3089\u308c\u308b\u3002 .avi \u3067\u3082\u3044\u3051\u307e\u3057\u305f\u3002 \u624b\u9806\u3068\u3057\u3066\u306f 1. \u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b 2. \u79d2\u3092\u6307\u5b9a\u3057\u3066\u5207\u308a\u53d6\u308b 3. \u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30e0\u3092\u901a\u3059 from pytorchvideo.data.encoded_video import EncodedVideo sample_path = 'sample.avi' # 1 video = EncodedVideo . from_path ( sample_path ) # 2 video_cliped = video . get_clip ( start_sec = 0 , end_sec = 10 ) # 3 video_data = transform ( video_data )","title":"3. \u52d5\u753b\u3092\u30a8\u30f3\u30b3\u30fc\u30c9\u3059\u308b"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#4","text":"\u5909\u63db\u3057\u305f\u52d5\u753b\u306f\u8f9e\u66f8\u578b\u306b\u306a\u3063\u3066\u3066 video \u3067\u52d5\u753b\uff08C, T, H, W\uff09 audio \u3067\u97f3\u58f0, video_name \u3067\u5143\u306e\u30d1\u30b9\u304c\u53d6\u5f97\u3067\u304d\u308b\u3002 \u30e2\u30c7\u30eb\u306f (batch_size, C, T, H, W) shape\u306etensor\u3092\u3068\u308b\u306e\u3067 input = video_data [ 'video' ] prediction = model ( input . unsqueeze ( 0 ))","title":"4. \u30e2\u30c7\u30eb\u306b\u5165\u529b\u3059\u308b"},{"location":"PyTorch/PyTorch%20Lightning/pytorchvideo/#lightning","text":"\u3084\u308b\u3053\u3068\u306f\u4e8c\u70b9 model\u3092\u7d44\u307f\u8fbc\u3080 DataModule\u3092\u66f8\u304f(Transform + label\u4ed8\u4e0e) class VideoClassification ( pytorch_lightning . LightningModule ): def __init__ ( self , path ): super () . __init__ () self . model = torch . hub . load ( path , source = \"local\" , model = model_name , pretrained = True ) def configure_optimizers ( self ): return torch . optim . Adam ( self . parameters (), lr = 1e-1 ) def forward ( self , x ): return self . model ( x ) def training_step ( self , batch , batch_idx ): y = self . model ( batch [ \"video\" ]) t = batch [ \"label\" ] loss = F . cross_entropy ( y , t ) return loss from pytorchvideo.transforms import ( ApplyTransformToKey , RandomShortSideScale , RemoveKey , ShortSideScale , UniformTemporalSubsample ) from torchvision.transforms import ( Compose , Normalize , RandomCrop , RandomHorizontalFlip ) class KineticsDataModule ( pytorch_lightning . LightningDataModule ): def setup ( self ); self . train_dataset = pytorchvideo . data . Kinetics ( data_path = os . path . join ( self . _DATA_PATH , \"train.csv\" ), clip_sampler = pytorchvideo . data . make_clip_sampler ( \"random\" , self . _CLIP_DURATION ), transform = train_transform ) def train_dataloader ( self ): train_transform = Compose ( [ ApplyTransformToKey ( key = \"video\" , transform = Compose ( [ UniformTemporalSubsample ( 8 ), Normalize (( 0.45 , 0.45 , 0.45 ), ( 0.225 , 0.225 , 0.225 )), RandomShortSideScale ( min_size = 256 , max_size = 320 ), RandomCrop ( 244 ), RandomHorizontalFlip ( p = 0.5 ), ] ), ), ] ) return torch . utils . data . DataLoader ( train_dataset , batch_size = self . _BATCH_SIZE , num_workers = self . _NUM_WORKERS , )","title":"\u304a\u307e\u3051 Lightning\u7d44\u307f\u8fbc\u3080"},{"location":"UofT/game_theory/","text":"# Game Theory \u307e\u3068\u3081 \u6614\u306e\u8a18\u4e8b 1 2 Substraction Game Terminal = P State connected to P state is N State connected to only N state is P Return to 2 Sum of Combinatorial Games Given two distinct games, SG function of the combination of the two games is nim sum of SG functions of each game. \\(x \\bigoplus y = 0\\) IFF \\(x = y\\) Nim-sum is commutative and associative Zero-Sum Game Solution of 2 by 2 If exists, the saddle point is the solution Otherwise, MINMAX strategy of 1 st player gives the optimal player Value of Game is the expected payoff of 1 st player Solution of 2 by M If saddle point exists, done Otherwise, try to reduce into 2 by 2 by domination If not possible, use test domination. Plot \\(M\\) lines in terms of p such that each line is the expected payoff for each column. The point which maximizes the minimum of those \\(M\\) lines is MINMAX. If M by 2, take transpose and multiply negative one. Then second player becomes first player. Or plot for each row and minimize maximum. Solution of N by N If no reduction is possible, use indifference. That assumes that all column expectation is equivalent to the value of the game. This translates the problem into solving linear equation. Use calculator. NTU 1. PNE Max of the column for first player Max of the row for second player 2. MNE Reduce to 2 by 2. Consider one's expected gain in terms of opponent's move For each case, consider the best reaction of one's move. 3. TU feasible set and NTU feasible set A feasible payoff vector, \\((v_1,v_2)\\) , is said to be Pareto optimal if the only feasible payoff vector \\((v_1\u2032 ,v_2\u2032 )\\) such that \\(v_1\u2032 \u2265 v_1\\) and \\(v_2\u2032 \u2265 v_2\\) is the vector \\((v_1\u2032 ,v_2\u2032 ) = (v_1,v_2)\\) . It is the right top edge of the set. 4. Solution of NTU Game Find safety values for each player One of NME, NPE or MINMAX strategy is the solution. TU The game is inessential if $sum(v(i)) = v(n), otherwise essential (if inessential, only Imputation is \\(v(i)\\) ) 5. Solution of TU Game \\(\u03c3\\) = max( sum of each entry of bi-matrix) \\((D_1, D_2)\\) = \\((Val(A), Val(B))\\) Optimal payoff line = \\((D_1, \u03c3 - D_2)\\) and \\((\u03c3 - D_1, D_2)\\) \\(\u03b4\\) = \\(Val(A - B)\\) \\(\u03c6\\) = \\(\u03c3+\u03b4 / 2 , \u03c3-\u03b4 / 2\\) 6. Imputation Possible distributions of \\(v(N)\\) Convert to coalitional form Find the payoff vector with \\(sum(x) = v(N)\\) and \\(x_i \\geq v(i)\\) 7. Core Subset of Imputation with \\(sum(x_S) = v(S)\\) . The core is also called Stable imputation . Other imputations not core is said to be Unstable . 8. Shapley Shapley is unique Shapley vector must satisfy Efficiency : \\(sum(\u03c6_i) = v(N)\\) Symmetry : if characteristic function is symmetric for some two players, Shapley is also equivalent. Dummy Axiom : If player \\(x\\) does not increase values of any coalition with \\(x\\) , \\(\u03c6_x = 0\\) Additivity : sum of Shapley values of two distinct games equals to Shapley of sum of two game. Calculated as Summation occurs for all coalitions including the player \\(i\\)","title":"Game theory"},{"location":"UofT/game_theory/#substraction-game","text":"Terminal = P State connected to P state is N State connected to only N state is P Return to 2","title":"Substraction Game"},{"location":"UofT/game_theory/#sum-of-combinatorial-games","text":"Given two distinct games, SG function of the combination of the two games is nim sum of SG functions of each game. \\(x \\bigoplus y = 0\\) IFF \\(x = y\\) Nim-sum is commutative and associative","title":"Sum of Combinatorial Games"},{"location":"UofT/game_theory/#zero-sum-game","text":"","title":"Zero-Sum Game"},{"location":"UofT/game_theory/#solution-of-2-by-2","text":"If exists, the saddle point is the solution Otherwise, MINMAX strategy of 1 st player gives the optimal player Value of Game is the expected payoff of 1 st player","title":"Solution of 2 by 2"},{"location":"UofT/game_theory/#solution-of-2-by-m","text":"If saddle point exists, done Otherwise, try to reduce into 2 by 2 by domination If not possible, use test domination. Plot \\(M\\) lines in terms of p such that each line is the expected payoff for each column. The point which maximizes the minimum of those \\(M\\) lines is MINMAX. If M by 2, take transpose and multiply negative one. Then second player becomes first player. Or plot for each row and minimize maximum.","title":"Solution of 2 by M"},{"location":"UofT/game_theory/#solution-of-n-by-n","text":"If no reduction is possible, use indifference. That assumes that all column expectation is equivalent to the value of the game. This translates the problem into solving linear equation. Use calculator.","title":"Solution of N by N"},{"location":"UofT/game_theory/#ntu","text":"","title":"NTU"},{"location":"UofT/game_theory/#1-pne","text":"Max of the column for first player Max of the row for second player","title":"1. PNE"},{"location":"UofT/game_theory/#2-mne","text":"Reduce to 2 by 2. Consider one's expected gain in terms of opponent's move For each case, consider the best reaction of one's move.","title":"2. MNE"},{"location":"UofT/game_theory/#3-tu-feasible-set-and-ntu-feasible-set","text":"A feasible payoff vector, \\((v_1,v_2)\\) , is said to be Pareto optimal if the only feasible payoff vector \\((v_1\u2032 ,v_2\u2032 )\\) such that \\(v_1\u2032 \u2265 v_1\\) and \\(v_2\u2032 \u2265 v_2\\) is the vector \\((v_1\u2032 ,v_2\u2032 ) = (v_1,v_2)\\) . It is the right top edge of the set.","title":"3. TU feasible set and NTU feasible set"},{"location":"UofT/game_theory/#4-solution-of-ntu-game","text":"Find safety values for each player One of NME, NPE or MINMAX strategy is the solution.","title":"4. Solution of NTU Game"},{"location":"UofT/game_theory/#tu","text":"The game is inessential if $sum(v(i)) = v(n), otherwise essential (if inessential, only Imputation is \\(v(i)\\) )","title":"TU"},{"location":"UofT/game_theory/#5-solution-of-tu-game","text":"\\(\u03c3\\) = max( sum of each entry of bi-matrix) \\((D_1, D_2)\\) = \\((Val(A), Val(B))\\) Optimal payoff line = \\((D_1, \u03c3 - D_2)\\) and \\((\u03c3 - D_1, D_2)\\) \\(\u03b4\\) = \\(Val(A - B)\\) \\(\u03c6\\) = \\(\u03c3+\u03b4 / 2 , \u03c3-\u03b4 / 2\\)","title":"5. Solution of TU Game"},{"location":"UofT/game_theory/#6-imputation","text":"Possible distributions of \\(v(N)\\) Convert to coalitional form Find the payoff vector with \\(sum(x) = v(N)\\) and \\(x_i \\geq v(i)\\)","title":"6. Imputation"},{"location":"UofT/game_theory/#7-core","text":"Subset of Imputation with \\(sum(x_S) = v(S)\\) . The core is also called Stable imputation . Other imputations not core is said to be Unstable .","title":"7. Core"},{"location":"UofT/game_theory/#8-shapley","text":"Shapley is unique Shapley vector must satisfy Efficiency : \\(sum(\u03c6_i) = v(N)\\) Symmetry : if characteristic function is symmetric for some two players, Shapley is also equivalent. Dummy Axiom : If player \\(x\\) does not increase values of any coalition with \\(x\\) , \\(\u03c6_x = 0\\) Additivity : sum of Shapley values of two distinct games equals to Shapley of sum of two game. Calculated as Summation occurs for all coalitions including the player \\(i\\)","title":"8. Shapley"},{"location":"UofT/note/","text":"# CSC413 MATOME 1. Non-deep Learning AI models Linear reg Logistic reg Feature mapping Lack of complexity. Can not fit non-convex set. (Non-linearity) Dataset needs to be manifolding. 1-1. Optimization in High Dimensional space Convexity : Linear regression and logistic regressions are convex i.e. has exactly one minima Saddle Points : Minima with respect to some direction but not global minima Plateaux : Flat surface. Usually occurs due to saturated unit a.k.a dead unit Ravines : Mixture of high and small gradients. Must be dealt with Momentum def gradient_decent ( x , t ): b_ = 0 w_ = 0 y = forward ( x ) for i in range ( iterations ): b_ += - 2 / N * ( t - y ) * a w_ += - 2 / N * ( t - y ) * x * a return 2. MLP Basic neural network. Composition of linear transformations Can be used as both embedder and classifier The more layers, the higher risk of overfitting Not convext hence has multiple minima Unlimited complexity under the universal approximation theorems which asserts that MLP with infinite number of neurons can regress any linear function. Dropout: Enabled only on training 2-1. Language Modeling N-gram : Constract a table of all possible inputs and probabilities of all possible outputs GloVe : Embedding space that relies on Distributional Hypothesis and built co-occurrence matrix that indicates wether two words appears in the similar context or not. 2-2. Training Neural Network Genelarization : Increase test accuracy Data Augmentation : 3. CNN Convert 2d image into 1d vector along channel dimension High robustness with various size of images Classification is done by MLP Affine layers Still overfits without skip-connection and residual blocks BatchNorm : Channel wise normalization increasing robustness Loss function : Cross Entropy in classification, MSE in regression AlexNet, VGG, ResNet, U-net Major models used for fine-tuning and transfer learning. Transfer learning usually freezes Conv layers and only train FC layers while fine-tuning trains all parameters from scratch. 4. RNN Some major architectures are LSTM MLP GRU MLP VAE RCNN ConvLSTM LSTM layer has 4 logic gates while GRU has 3 LSTM GATES 4-1 Encoder-Decoder Model (Seq2Seq) Context Vector : Dimensional vector representing context of the sentece. The dimension is equivalent to the number of hidden state. 4-2. VAE VAE is the first generative image model Encoder learns a mapping from input image to gaussian parameters mean and variance Generator takes a sampling as an input from normal distribution with given gaussian parameters by the encoder. If regression, generator outputs new gaussian parameters otherwise output an image depending on the task Loss function The training of VAE is equivalent to MAP with respect to decoder's log likelihood \\[ \\begin{eqnarray} \\log p_\\theta(x) &=& \\log \\int p_\\theta(x, z) dz \\\\ &=& \\log \\int q_\\varphi(z|x)\\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &\\geq& \\int q_\\varphi(z|x) \\log \\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &=& L(x; \\varphi, \\theta) \\end{eqnarray} \\] The difference between LHS and RHS is KL-divergence \\[ \\begin{eqnarray} \\log p_\\theta(x) \u2013 L(x; \\varphi, \\theta) &=& \\log p_\\theta(x) \u2013 \\int q_\\varphi(z|x) \\log \\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &=& \\log p_{\\theta}(x) \\int q_{\\varphi} (z|x) dz \u2013 \\int q_{\\varphi} (z|x) \\log \\frac{p_{\\theta} (z|x)p(x)}{q_{\\varphi}(z|x)} dz \\\\ &=& \\int q_\\varphi (z|x) \\{ \\log p_{\\theta}(x) \u2013 \\log p_\\theta(z|x) \u2013 \\log p_{\\theta}(x) + \\log q_\\varphi (z|x) \\} dz\\\\ &=& \\int q_\\varphi (z|x) \\{ \\log q_\\varphi (z|x) \u2013 \\log p_\\theta(z|x) \\} dz\\\\ &=& KL[q_\\varphi (z|x) \\| p_\\theta (z|x)] \\end{eqnarray} \\] This is equivalent to maximizing the evidence lower bound. $$ \\begin{eqnarray} E_{q_\\varphi (z|x)}[\\log p_\\theta (x|z)] &=& E_{q_\\varphi (z|x)}[\\log \\prod_l^{L} f(z_l)^x (1 \u2013 f(z_l))^{(1 \u2013 x)}] \\ &=& \\frac{1}{L} \\sum_{l=1}^L { x \\log f(z_l) + (1 \u2013 x) \\log (1 \u2013 f(z_l)) } \\end{eqnarray} $$ Translation from first line to second line is called monte carlo estimation . Minimizing the difference stricts the encoder's divergence. 5. GAN Composition of generator and discriminator MLPs. It is represented as a two-player minimax game. Modified loss \\[minEz [\u2212logD\u03b8(G\u03c6(z))]\\] 5-1. Cycle GAN GAN with extra MLP after","title":"Note"},{"location":"UofT/note/#1-non-deep-learning-ai-models","text":"Linear reg Logistic reg Feature mapping Lack of complexity. Can not fit non-convex set. (Non-linearity) Dataset needs to be manifolding.","title":"1. Non-deep Learning AI models"},{"location":"UofT/note/#1-1-optimization-in-high-dimensional-space","text":"Convexity : Linear regression and logistic regressions are convex i.e. has exactly one minima Saddle Points : Minima with respect to some direction but not global minima Plateaux : Flat surface. Usually occurs due to saturated unit a.k.a dead unit Ravines : Mixture of high and small gradients. Must be dealt with Momentum def gradient_decent ( x , t ): b_ = 0 w_ = 0 y = forward ( x ) for i in range ( iterations ): b_ += - 2 / N * ( t - y ) * a w_ += - 2 / N * ( t - y ) * x * a return","title":"1-1. Optimization in High Dimensional space"},{"location":"UofT/note/#2-mlp","text":"Basic neural network. Composition of linear transformations Can be used as both embedder and classifier The more layers, the higher risk of overfitting Not convext hence has multiple minima Unlimited complexity under the universal approximation theorems which asserts that MLP with infinite number of neurons can regress any linear function. Dropout: Enabled only on training","title":"2. MLP"},{"location":"UofT/note/#2-1-language-modeling","text":"N-gram : Constract a table of all possible inputs and probabilities of all possible outputs GloVe : Embedding space that relies on Distributional Hypothesis and built co-occurrence matrix that indicates wether two words appears in the similar context or not.","title":"2-1. Language Modeling"},{"location":"UofT/note/#2-2-training-neural-network","text":"Genelarization : Increase test accuracy Data Augmentation :","title":"2-2. Training Neural Network"},{"location":"UofT/note/#3-cnn","text":"Convert 2d image into 1d vector along channel dimension High robustness with various size of images Classification is done by MLP Affine layers Still overfits without skip-connection and residual blocks BatchNorm : Channel wise normalization increasing robustness Loss function : Cross Entropy in classification, MSE in regression","title":"3. CNN"},{"location":"UofT/note/#alexnet-vgg-resnet-u-net","text":"Major models used for fine-tuning and transfer learning. Transfer learning usually freezes Conv layers and only train FC layers while fine-tuning trains all parameters from scratch.","title":"AlexNet, VGG, ResNet, U-net"},{"location":"UofT/note/#4-rnn","text":"Some major architectures are LSTM MLP GRU MLP VAE RCNN ConvLSTM LSTM layer has 4 logic gates while GRU has 3 LSTM GATES","title":"4. RNN"},{"location":"UofT/note/#4-1-encoder-decoder-model-seq2seq","text":"Context Vector : Dimensional vector representing context of the sentece. The dimension is equivalent to the number of hidden state.","title":"4-1 Encoder-Decoder Model (Seq2Seq)"},{"location":"UofT/note/#4-2-vae","text":"VAE is the first generative image model Encoder learns a mapping from input image to gaussian parameters mean and variance Generator takes a sampling as an input from normal distribution with given gaussian parameters by the encoder. If regression, generator outputs new gaussian parameters otherwise output an image depending on the task","title":"4-2. VAE"},{"location":"UofT/note/#loss-function","text":"The training of VAE is equivalent to MAP with respect to decoder's log likelihood \\[ \\begin{eqnarray} \\log p_\\theta(x) &=& \\log \\int p_\\theta(x, z) dz \\\\ &=& \\log \\int q_\\varphi(z|x)\\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &\\geq& \\int q_\\varphi(z|x) \\log \\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &=& L(x; \\varphi, \\theta) \\end{eqnarray} \\] The difference between LHS and RHS is KL-divergence \\[ \\begin{eqnarray} \\log p_\\theta(x) \u2013 L(x; \\varphi, \\theta) &=& \\log p_\\theta(x) \u2013 \\int q_\\varphi(z|x) \\log \\frac{p_\\theta(x, z)}{q_\\varphi(z|x)} dz \\\\ &=& \\log p_{\\theta}(x) \\int q_{\\varphi} (z|x) dz \u2013 \\int q_{\\varphi} (z|x) \\log \\frac{p_{\\theta} (z|x)p(x)}{q_{\\varphi}(z|x)} dz \\\\ &=& \\int q_\\varphi (z|x) \\{ \\log p_{\\theta}(x) \u2013 \\log p_\\theta(z|x) \u2013 \\log p_{\\theta}(x) + \\log q_\\varphi (z|x) \\} dz\\\\ &=& \\int q_\\varphi (z|x) \\{ \\log q_\\varphi (z|x) \u2013 \\log p_\\theta(z|x) \\} dz\\\\ &=& KL[q_\\varphi (z|x) \\| p_\\theta (z|x)] \\end{eqnarray} \\] This is equivalent to maximizing the evidence lower bound. $$ \\begin{eqnarray} E_{q_\\varphi (z|x)}[\\log p_\\theta (x|z)] &=& E_{q_\\varphi (z|x)}[\\log \\prod_l^{L} f(z_l)^x (1 \u2013 f(z_l))^{(1 \u2013 x)}] \\ &=& \\frac{1}{L} \\sum_{l=1}^L { x \\log f(z_l) + (1 \u2013 x) \\log (1 \u2013 f(z_l)) } \\end{eqnarray} $$ Translation from first line to second line is called monte carlo estimation . Minimizing the difference stricts the encoder's divergence.","title":"Loss function"},{"location":"UofT/note/#5-gan","text":"Composition of generator and discriminator MLPs. It is represented as a two-player minimax game. Modified loss \\[minEz [\u2212logD\u03b8(G\u03c6(z))]\\]","title":"5. GAN"},{"location":"UofT/note/#5-1-cycle-gan","text":"GAN with extra MLP after","title":"5-1. Cycle GAN"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/","text":"Python\u306e\u304a\u52c9\u5f37\u306b\u3064\u3044\u3066 \u6a5f\u68b0\u5b66\u7fd2\u3092\u52c9\u5f37\u3057\u305f\u3044\u3051\u3069\u305d\u3082\u305d\u3082Python\u89e6\u3063\u305f\u3053\u3068\u306a\u3044\u4eba Django\u3068\u304bFlask\u3068\u304b\u3067Python\u304c\u5fc5\u8981\u306b\u306a\u3063\u305f\u4eba \u305d\u3093\u306a\u7247\u65b9\u3080\u3051\u306b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u3057\u3066\u306ePython\u306e\u52c9\u5f37\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8a18\u8ff0\u3057\u307e\u3059\u3002 python\u74b0\u5883\u306e\u8a2d\u5b9a python\u306e\u74b0\u5883\u3092\u6574\u3048\u308b\u306a\u3089\u4ee5\u4e0b\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u304a\u3059\u3059\u3081 mini-conda pyenv anaconda\u306f\u4e0d\u8981\u306a\u7269\u306e\u4ed8\u968f\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b\u306e\u3067\u304a\u52e7\u3081\u3057\u307e\u305b\u3093\u3002 Mac\u306e\u5834\u5408 \u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\u304c2.x\u7cfb\u306a\u306e\u3067brew\u3067\u76f4\u63a5python3.x\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304b\u3001pyenv\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4eba\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002 Linux\u306e\u5834\u5408 Ubuntu\u7528\u306edocker\u30b3\u30f3\u30c6\u30ca\u306b\u79fb\u3063\u3066\u304b\u3089\u3001Ubuntu\u4e0a\u3067miniconda\u306epython3.x\u74b0\u5883\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u81ea\u5206\u3067Ubuntu\u306epython\u74b0\u5883\u69cb\u7bc9\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\u304c2.x\u7cfb\u306a\u306e\u3067sudo apt-get install\u3067\u76f4\u63a5python3.x\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304b\u3001pyenv\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4eba\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002 python\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0 AI\u30a8\u30f3\u30b8\u30cb\u30a2\u304c\u6c17\u3092\u3064\u3051\u305f\u3044Python\u5b9f\u88c5\u306e\u30ce\u30a6\u30cf\u30a6\u30fb\u30b3\u30c4\u307e\u3068\u3081 \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u81ea\u8d70\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u300d \u8aad\u307f\u3084\u3059\u3044\u30b3\u30fc\u30c9\u3068\u306f \u4e00\u5e74\u5f8c\u306e\u81ea\u5206\u306b\u3068\u3063\u3066\u3082\u3001\u5206\u304b\u308a\u3084\u3059\u3044\u30fb\u8aad\u307f\u3084\u3059\u3044\u30fb\u7c21\u6f54\u306a\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3088\u3046\u306b\u5fc3\u304c\u3051\u307e\u3057\u3087\u3046\u3002\u4eba\u306e\u30b3\u30fc\u30c9\u3092\u898b\u305f\u308a\u3001\u898b\u3066\u8cb0\u3063\u305f\u308a\u3059\u308b\u306e\u306f\u975e\u5e38\u306b\u8cb4\u91cd\u306a\u7d4c\u9a13\u306b\u306a\u308a\u307e\u3059\u3002 \u95a2\u6570\u3084\u30d5\u30a1\u30a4\u30eb\u306f\u9069\u5ea6\u306a\u884c\u6570\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u304c\u591a\u3044\u5834\u5408\u306f\u95a2\u6570\u540d\u3084\u5909\u6570\u540d\u3067\u5206\u304b\u308b\u3088\u3046\u306b\u51fa\u6765\u306a\u3044\u304b\u691c\u8a0e\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u3067\u4fee\u6b63\u3059\u308b\u5834\u5408\u306f\u30c6\u30b9\u30c8\u306f\u5fc5\u9808\u3067\u3059\u3002 [\u53c2\u8003]\u6bb5\u968e\u3092\u8e0f\u307f\u3064\u3064\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u3057\u3066\u307f\u308b: https://qiita.com/suzuki-hoge/items/220e36eb34b160ada527 \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u30ea\u30fc\u30c0\u30d6\u30eb\u30b3\u30fc\u30c9\u300d \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u3068\u306f \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u3068\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u52d5\u304d\u30fb\u8a2d\u8a08\u610f\u56f3\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u6700\u521d\u306e\u3046\u3061\u306fprint\u30c7\u30d0\u30c3\u30b0\u306a\u3069\u5b9f\u969b\u306b\u52d5\u304b\u3057\u3066\u6319\u52d5\u3092\u628a\u63e1\u3059\u308b\u306e\u3082\u5927\u4e8b\u3068\u601d\u3044\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u305f\u3081\u306e\u6280\u8853(\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8) \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u3068\u306f(\u7121\u7406\u3084\u308a\u4e00\u8a00\u3067\u8a00\u3046\u3068)\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6574\u7406\u306e\u4ed5\u65b9\u3067\u3059\u3002\u76ee\u7684\u3054\u3068\u306e\u5358\u4f4d\u3067\u95a2\u6570\u3084\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u3001\u8a2d\u8a08\u3092\u7406\u89e3\u30fb\u6539\u5584\u3057\u3084\u3059\u304f\u3057\u307e\u3059\u3002python\u3067\u3082\u30af\u30e9\u30b9\u306a\u3069\u8a2d\u8a08\u3059\u308b\u3068\u304d\u306b\u306f\u610f\u8b58\u3057\u305f\u65b9\u304c\u826f\u3044\u3067\u3059\u3002 \u3010\u56f3\u89e3\u3011\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u3068\u306f\uff1f: https://26gram.com/what-is-object-oriented \u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3 \u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3\u3068\u306f(\u7121\u7406\u3084\u308a\u4e00\u8a00\u3067\u8a00\u3046\u3068)\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5f79\u5272\u3067\u5206\u3051\u308b\u65b9\u6cd5\u3067\u3059\u3002\u904e\u53bb\u306e\u826f\u304b\u3063\u305f\u5206\u3051\u65b9\u3092\u518d\u5229\u7528\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u540d\u524d\u304c\u3064\u3044\u3066\u3044\u307e\u3059\u3002\u30d1\u30bf\u30fc\u30f3\u3092\u899a\u3048\u308b\u3068\u4f7f\u3063\u3066\u307f\u305f\u304f\u306a\u308b\u75c5\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u6562\u3048\u3066\u4f7f\u308f\u306a\u3044\u3082\u3072\u3068\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u3068\u601d\u3044\u307e\u3059\u3002python\u3067\u3082\u3056\u3063\u3068\u6982\u8981\u306f\u77e5\u3063\u3066\u304a\u3044\u305f\u65b9\u304c\u5f79\u306b\u7acb\u3061\u307e\u3059\u3002 \u4e8b\u4f8b\u3067\u5b66\u3076\u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3: https://www.ogis-ri.co.jp/otc/hiroba/technical/DesignPatternsWithExample/chapter01.html \u30c6\u30b9\u30c8\u5927\u4e8b \u4ed5\u4e8b\u3067\u66f8\u304f\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u6642\u306b\u534a\u5206\u4ee5\u4e0a\u304c\u30c6\u30b9\u30c8\u30b3\u30fc\u30c9\u306b\u306a\u308b\u307b\u3069\u30c6\u30b9\u30c8\u306f\u5927\u4e8b\u3067\u3059\u3002\u6b63\u3057\u304f\u52d5\u304f\u304b\u306e\u78ba\u8a8d\u304b\u3089\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u6642\u306b\u30c7\u30b0\u30ec(\u30d0\u30b0\u304c\u6df7\u5165)\u3057\u306a\u3044\u304b\u3092\u78ba\u8a8d\u306a\u3069\u3001\u826f\u3044\u30b3\u30fc\u30c9\u306b\u306f\u5e38\u306b\u30c6\u30b9\u30c8\u304c\u4f34\u3044\u307e\u3059\u3002 TDD \u306a\u3093\u3066\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u6271\u3044 \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3 \u4ed5\u4e8b\u3067\u66f8\u304f\u30b3\u30fc\u30c9\u306f\u81ea\u5206\u6d41\u3067\u306f\u7121\u304f\u3001\u7686\u3093\u306a\u304c\u898b\u3084\u3059\u3044\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u66f8\u304d\u307e\u3059\u3002 \u57fa\u672c\u3001flake8\u306a\u3069lint\u30c4\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u958b\u767a\u3057\u307e\u3057\u3087\u3046\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u3088\u3063\u3066\u306fblack\u3082\u63a8\u5968\u306a\u306e\u3067\u8981\u78ba\u8a8d\u3002 \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u898f\u7d04\u306b\u3064\u3044\u3066\u306f\u5225\u8a18\u4e8b\u3067\u3057\u3063\u304b\u308a\u89e3\u8aac\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u66f4\u306a\u308b\u52c9\u5f37\u65b9\u6cd5 PoC\u306e\u6210\u679c\u3092\u88fd\u54c1\u5316\u3057\u305f\u308a\u3001\u793e\u5185\u3067\u4ed6\u90e8\u7f72\u306b\u30ea\u30ea\u30fc\u30b9\u3057\u305f\u308a\u3059\u308b\u306b\u306f\u3001\u3055\u3089\u306a\u308b\u7cbe\u9032\u304c\u5fc5\u8981\u3068\u601d\u3044\u307e\u3059\u3002\u66f8\u7c4d\u3082\u5f79\u7acb\u3061\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u4e0a\u9054\u306e\u57fa\u672c\u306f\u3001\u6ca2\u5c71\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3053\u3068\u30fb\u8aad\u3080\u3053\u3068\u3067\u3059\u3002 \u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u4e0a\u7d1a\u8005\u306b\u5411\u3051\u3066 \u304a\u52e7\u3081\u66f8\u7c4d \u300cEffective Python\u300d \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u9054\u4eba\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u300d","title":"Python\u306e\u304a\u52c9\u5f37\u306b\u3064\u3044\u3066"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#python","text":"\u6a5f\u68b0\u5b66\u7fd2\u3092\u52c9\u5f37\u3057\u305f\u3044\u3051\u3069\u305d\u3082\u305d\u3082Python\u89e6\u3063\u305f\u3053\u3068\u306a\u3044\u4eba Django\u3068\u304bFlask\u3068\u304b\u3067Python\u304c\u5fc5\u8981\u306b\u306a\u3063\u305f\u4eba \u305d\u3093\u306a\u7247\u65b9\u3080\u3051\u306b\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u8a00\u8a9e\u3068\u3057\u3066\u306ePython\u306e\u52c9\u5f37\u65b9\u6cd5\u306b\u3064\u3044\u3066\u8a18\u8ff0\u3057\u307e\u3059\u3002","title":"Python\u306e\u304a\u52c9\u5f37\u306b\u3064\u3044\u3066"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#python_1","text":"python\u306e\u74b0\u5883\u3092\u6574\u3048\u308b\u306a\u3089\u4ee5\u4e0b\u306e\u30d1\u30c3\u30b1\u30fc\u30b8\u304c\u304a\u3059\u3059\u3081 mini-conda pyenv anaconda\u306f\u4e0d\u8981\u306a\u7269\u306e\u4ed8\u968f\u3057\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3055\u308c\u308b\u306e\u3067\u304a\u52e7\u3081\u3057\u307e\u305b\u3093\u3002","title":"python\u74b0\u5883\u306e\u8a2d\u5b9a"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#mac","text":"\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\u304c2.x\u7cfb\u306a\u306e\u3067brew\u3067\u76f4\u63a5python3.x\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304b\u3001pyenv\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4eba\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002","title":"Mac\u306e\u5834\u5408"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#linux","text":"Ubuntu\u7528\u306edocker\u30b3\u30f3\u30c6\u30ca\u306b\u79fb\u3063\u3066\u304b\u3089\u3001Ubuntu\u4e0a\u3067miniconda\u306epython3.x\u74b0\u5883\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u81ea\u5206\u3067Ubuntu\u306epython\u74b0\u5883\u69cb\u7bc9\u3059\u308b\u5834\u5408\u306f\u3001\u30c7\u30d5\u30a9\u30eb\u30c8\u306epython\u304c2.x\u7cfb\u306a\u306e\u3067sudo apt-get install\u3067\u76f4\u63a5python3.x\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u304b\u3001pyenv\u7d4c\u7531\u3067\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b\u4eba\u304c\u591a\u3044\u3088\u3046\u3067\u3059\u3002","title":"Linux\u306e\u5834\u5408"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#python_2","text":"AI\u30a8\u30f3\u30b8\u30cb\u30a2\u304c\u6c17\u3092\u3064\u3051\u305f\u3044Python\u5b9f\u88c5\u306e\u30ce\u30a6\u30cf\u30a6\u30fb\u30b3\u30c4\u307e\u3068\u3081 \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u81ea\u8d70\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u300d","title":"python\u3067\u306e\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_1","text":"\u4e00\u5e74\u5f8c\u306e\u81ea\u5206\u306b\u3068\u3063\u3066\u3082\u3001\u5206\u304b\u308a\u3084\u3059\u3044\u30fb\u8aad\u307f\u3084\u3059\u3044\u30fb\u7c21\u6f54\u306a\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3088\u3046\u306b\u5fc3\u304c\u3051\u307e\u3057\u3087\u3046\u3002\u4eba\u306e\u30b3\u30fc\u30c9\u3092\u898b\u305f\u308a\u3001\u898b\u3066\u8cb0\u3063\u305f\u308a\u3059\u308b\u306e\u306f\u975e\u5e38\u306b\u8cb4\u91cd\u306a\u7d4c\u9a13\u306b\u306a\u308a\u307e\u3059\u3002 \u95a2\u6570\u3084\u30d5\u30a1\u30a4\u30eb\u306f\u9069\u5ea6\u306a\u884c\u6570\u3067\u3001\u30b3\u30e1\u30f3\u30c8\u304c\u591a\u3044\u5834\u5408\u306f\u95a2\u6570\u540d\u3084\u5909\u6570\u540d\u3067\u5206\u304b\u308b\u3088\u3046\u306b\u51fa\u6765\u306a\u3044\u304b\u691c\u8a0e\u3057\u3066\u307f\u307e\u3057\u3087\u3046\u3002\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u3067\u4fee\u6b63\u3059\u308b\u5834\u5408\u306f\u30c6\u30b9\u30c8\u306f\u5fc5\u9808\u3067\u3059\u3002 [\u53c2\u8003]\u6bb5\u968e\u3092\u8e0f\u307f\u3064\u3064\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u3057\u3066\u307f\u308b: https://qiita.com/suzuki-hoge/items/220e36eb34b160ada527 \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u30ea\u30fc\u30c0\u30d6\u30eb\u30b3\u30fc\u30c9\u300d","title":"\u8aad\u307f\u3084\u3059\u3044\u30b3\u30fc\u30c9\u3068\u306f"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_2","text":"\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u3068\u306f\u3001\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u52d5\u304d\u30fb\u8a2d\u8a08\u610f\u56f3\u3092\u7406\u89e3\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u6700\u521d\u306e\u3046\u3061\u306fprint\u30c7\u30d0\u30c3\u30b0\u306a\u3069\u5b9f\u969b\u306b\u52d5\u304b\u3057\u3066\u6319\u52d5\u3092\u628a\u63e1\u3059\u308b\u306e\u3082\u5927\u4e8b\u3068\u601d\u3044\u307e\u3059\u3002 \u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u305f\u3081\u306e\u6280\u8853(\u30c1\u30fc\u30c8\u30b7\u30fc\u30c8)","title":"\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u3092\u8aad\u3080\u3068\u306f"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_3","text":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u3068\u306f(\u7121\u7406\u3084\u308a\u4e00\u8a00\u3067\u8a00\u3046\u3068)\u30d7\u30ed\u30b0\u30e9\u30e0\u306e\u6574\u7406\u306e\u4ed5\u65b9\u3067\u3059\u3002\u76ee\u7684\u3054\u3068\u306e\u5358\u4f4d\u3067\u95a2\u6570\u3084\u30c7\u30fc\u30bf\u3092\u307e\u3068\u3081\u3066\u3001\u8a2d\u8a08\u3092\u7406\u89e3\u30fb\u6539\u5584\u3057\u3084\u3059\u304f\u3057\u307e\u3059\u3002python\u3067\u3082\u30af\u30e9\u30b9\u306a\u3069\u8a2d\u8a08\u3059\u308b\u3068\u304d\u306b\u306f\u610f\u8b58\u3057\u305f\u65b9\u304c\u826f\u3044\u3067\u3059\u3002 \u3010\u56f3\u89e3\u3011\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411\u3068\u306f\uff1f: https://26gram.com/what-is-object-oriented","title":"\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u6307\u5411"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_4","text":"\u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3\u3068\u306f(\u7121\u7406\u3084\u308a\u4e00\u8a00\u3067\u8a00\u3046\u3068)\u30d7\u30ed\u30b0\u30e9\u30e0\u3092\u5f79\u5272\u3067\u5206\u3051\u308b\u65b9\u6cd5\u3067\u3059\u3002\u904e\u53bb\u306e\u826f\u304b\u3063\u305f\u5206\u3051\u65b9\u3092\u518d\u5229\u7528\u3057\u3084\u3059\u3044\u3088\u3046\u306b\u540d\u524d\u304c\u3064\u3044\u3066\u3044\u307e\u3059\u3002\u30d1\u30bf\u30fc\u30f3\u3092\u899a\u3048\u308b\u3068\u4f7f\u3063\u3066\u307f\u305f\u304f\u306a\u308b\u75c5\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u6562\u3048\u3066\u4f7f\u308f\u306a\u3044\u3082\u3072\u3068\u3064\u306e\u30d1\u30bf\u30fc\u30f3\u3068\u601d\u3044\u307e\u3059\u3002python\u3067\u3082\u3056\u3063\u3068\u6982\u8981\u306f\u77e5\u3063\u3066\u304a\u3044\u305f\u65b9\u304c\u5f79\u306b\u7acb\u3061\u307e\u3059\u3002 \u4e8b\u4f8b\u3067\u5b66\u3076\u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3: https://www.ogis-ri.co.jp/otc/hiroba/technical/DesignPatternsWithExample/chapter01.html","title":"\u30c7\u30b6\u30a4\u30f3\u30d1\u30bf\u30fc\u30f3"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_5","text":"\u4ed5\u4e8b\u3067\u66f8\u304f\u30d7\u30ed\u30b0\u30e9\u30e0\u306f\u6642\u306b\u534a\u5206\u4ee5\u4e0a\u304c\u30c6\u30b9\u30c8\u30b3\u30fc\u30c9\u306b\u306a\u308b\u307b\u3069\u30c6\u30b9\u30c8\u306f\u5927\u4e8b\u3067\u3059\u3002\u6b63\u3057\u304f\u52d5\u304f\u304b\u306e\u78ba\u8a8d\u304b\u3089\u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u6642\u306b\u30c7\u30b0\u30ec(\u30d0\u30b0\u304c\u6df7\u5165)\u3057\u306a\u3044\u304b\u3092\u78ba\u8a8d\u306a\u3069\u3001\u826f\u3044\u30b3\u30fc\u30c9\u306b\u306f\u5e38\u306b\u30c6\u30b9\u30c8\u304c\u4f34\u3044\u307e\u3059\u3002 TDD \u306a\u3093\u3066\u3082\u306e\u3082\u3042\u308a\u307e\u3059\u3002","title":"\u30c6\u30b9\u30c8\u5927\u4e8b"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_6","text":"","title":"\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306e\u6271\u3044"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_7","text":"\u4ed5\u4e8b\u3067\u66f8\u304f\u30b3\u30fc\u30c9\u306f\u81ea\u5206\u6d41\u3067\u306f\u7121\u304f\u3001\u7686\u3093\u306a\u304c\u898b\u3084\u3059\u3044\u30eb\u30fc\u30eb\u306b\u5f93\u3063\u3066\u66f8\u304d\u307e\u3059\u3002 \u57fa\u672c\u3001flake8\u306a\u3069lint\u30c4\u30fc\u30eb\u3092\u4f7f\u7528\u3057\u3066\u958b\u767a\u3057\u307e\u3057\u3087\u3046\u3002\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u306b\u3088\u3063\u3066\u306fblack\u3082\u63a8\u5968\u306a\u306e\u3067\u8981\u78ba\u8a8d\u3002 \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u898f\u7d04\u306b\u3064\u3044\u3066\u306f\u5225\u8a18\u4e8b\u3067\u3057\u3063\u304b\u308a\u89e3\u8aac\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002","title":"\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u30ac\u30a4\u30c9\u30e9\u30a4\u30f3"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_8","text":"PoC\u306e\u6210\u679c\u3092\u88fd\u54c1\u5316\u3057\u305f\u308a\u3001\u793e\u5185\u3067\u4ed6\u90e8\u7f72\u306b\u30ea\u30ea\u30fc\u30b9\u3057\u305f\u308a\u3059\u308b\u306b\u306f\u3001\u3055\u3089\u306a\u308b\u7cbe\u9032\u304c\u5fc5\u8981\u3068\u601d\u3044\u307e\u3059\u3002\u66f8\u7c4d\u3082\u5f79\u7acb\u3061\u307e\u3059\u304c\u3001\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u4e0a\u9054\u306e\u57fa\u672c\u306f\u3001\u6ca2\u5c71\u306e\u30b3\u30fc\u30c9\u3092\u66f8\u304f\u3053\u3068\u30fb\u8aad\u3080\u3053\u3068\u3067\u3059\u3002","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u306e\u66f4\u306a\u308b\u52c9\u5f37\u65b9\u6cd5"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/python_study/#_9","text":"\u304a\u52e7\u3081\u66f8\u7c4d \u300cEffective Python\u300d \u304a\u52e7\u3081\u66f8\u7c4d \u300c\u9054\u4eba\u30d7\u30ed\u30b0\u30e9\u30de\u30fc\u300d","title":"\u30d7\u30ed\u30b0\u30e9\u30df\u30f3\u30b0\u4e0a\u7d1a\u8005\u306b\u5411\u3051\u3066"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC/","text":"\u30ef\u30fc\u30af\u30d5\u30ed\u30fc \u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3059\u308b\u524d\u306b \u8003\u3048\u308b - \u76ee\u7684\u306f\uff1f - \u305d\u306e\u30b3\u30fc\u30c9\u306f\u76ee\u7684\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u3001\u8ad6\u7406\u7684\u6839\u62e0\u304c\u3042\u308b\u304b - \u305d\u306e\u884c\u52d5\u3092\u7d30\u5206\u5316\u3057\u3066\u307f\u3066\u3001\u5b9f\u73fe\u53ef\u80fd\u304b - \u826f\u3044\u9762\u306e\u4ed6\u306b\u60aa\u3044\u9762\u306f\u4f55\u304b \u65e9\u3044\u884c\u52d5\u3088\u308a \u30df\u30b9\u306e\u5c11\u306a\u3044 \u884c\u52d5\u3092\u3002 \u8a8d\u8b58\u304c\u66d6\u6627\u306a\u6642\u306f\u7d50\u8ad6\u3092\u51fa\u3059\u524d\u306b\u805e\u304f \u5de5\u6570 \u898b\u7a4d\u3082\u308a\u3092\u3059\u308b\u3001\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u7684\u306b\u5b9f\u73fe\u53ef\u80fd\u304b\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 \u5de5\u6570\u898b\u7a4d\u3082\u3063\u305f\u3060\u3051\u3067\u4eca\u306e\u30ea\u30bd\u30fc\u30b9\u3067\u5b9f\u73fe\u53ef\u80fd\u304b\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3057\u306a\u3044\u3068\u3069\u3053\u304b\u3067\u9813\u632b\u3059\u308b\u3002 \u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0 \u306f\u7d76\u5bfe \u30c6\u30b9\u30c8 \u30b9\u30af\u30ea\u30d7\u30c8\u5fc5\u8981\u3002 \u5c11\u3057\u305a\u3064 \u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u21c6\u30c6\u30b9\u30c8\u3092\u884c\u3046 \u30b3\u30fc\u30c9\u3092 \u7406\u89e3\u305b\u305a\u66f8\u304d\u63db\u3048 \u308b\u3068\u3001 \u30d0\u30b0 \u306e\u5143\u3068\u306a\u308b \u5b9f\u9a13 \u306fmlflow\u306a\u3069\u3067 \u81ea\u52d5\u7ba1\u7406 \u3057\u30de\u30cb\u30e5\u30a2\u30eb\u64cd\u4f5c\u3092\u6e1b\u3089\u3059\u3068\u7d50\u679c\u7684\u306b\u304c\u4eba\u7684\u30df\u30b9\u304c\u6e1b\u308b \u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u9060\u9694\u306b\u9032\u3081\u308b\u305f\u3081\u306b \u30c7\u30fc\u30bf \u4f5c\u6210\u3068 \u56f3 \u306e\u4f5c\u6210\u30b9\u30af\u30ea\u30d7\u30c8\u306f\u5225\u3005\u306b\u4f5c\u308a\u3001\u56f3\u306e\u4f5c\u6210\u306f\u30b3\u30de\u30f3\u30c9\u4e00\u767a\u3067\u4f5c\u308b \uff08Excel\u3060\u3068\u624b\u9806\u5fd8\u308c\u308b\uff09 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e \u8aac\u660e \u306f\u3057\u3063\u304b\u308a\u6b8b\u3059\u3002README\u3084wiki\u4f7f\u7528\u3057\u3066\u3001\u5171\u540c\u4f5c\u696d\u3084\u81ea\u5206\u306e\u601d\u3044\u51fa\u3057\u306b\u6d3b\u7528\u3059\u308b \u76f8\u95a2 \u3068 \u56e0\u679c \u306f\u5206\u3051\u3066\u8003\u3048\u308b\u3002\u540c\u3058\u8981\u56e0\u3001\u307e\u305f\u306f\u4ecb\u5728\u5909\u6570\u306b\u3088\u3063\u3066\u52d5\u3044\u3066\u3044\u308b\u4e8b\u8c61\u306f\u76f8\u95a2\u3092\u6301\u3064\uff08\u64ec\u4f3c\u76f8\u95a2\uff09\u304c\u3001\u539f\u56e0\u3068\u7d50\u679c\u306e\u95a2\u4fc2\uff08\u56e0\u679c\uff09\u306b\u306a\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u3002","title":"\u30ef\u30fc\u30af\u30d5\u30ed\u30fc"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC/#_1","text":"","title":"\u30ef\u30fc\u30af\u30d5\u30ed\u30fc"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC/#_2","text":"- \u76ee\u7684\u306f\uff1f - \u305d\u306e\u30b3\u30fc\u30c9\u306f\u76ee\u7684\u3092\u9054\u6210\u3059\u308b\u305f\u3081\u306b\u3001\u8ad6\u7406\u7684\u6839\u62e0\u304c\u3042\u308b\u304b - \u305d\u306e\u884c\u52d5\u3092\u7d30\u5206\u5316\u3057\u3066\u307f\u3066\u3001\u5b9f\u73fe\u53ef\u80fd\u304b - \u826f\u3044\u9762\u306e\u4ed6\u306b\u60aa\u3044\u9762\u306f\u4f55\u304b","title":"\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3059\u308b\u524d\u306b\u8003\u3048\u308b"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC/#_3","text":"\u8a8d\u8b58\u304c\u66d6\u6627\u306a\u6642\u306f\u7d50\u8ad6\u3092\u51fa\u3059\u524d\u306b\u805e\u304f \u5de5\u6570 \u898b\u7a4d\u3082\u308a\u3092\u3059\u308b\u3001\u30b9\u30b1\u30b8\u30e5\u30fc\u30eb\u7684\u306b\u5b9f\u73fe\u53ef\u80fd\u304b\u30c1\u30a7\u30c3\u30af\u3059\u308b\u3002 \u5de5\u6570\u898b\u7a4d\u3082\u3063\u305f\u3060\u3051\u3067\u4eca\u306e\u30ea\u30bd\u30fc\u30b9\u3067\u5b9f\u73fe\u53ef\u80fd\u304b\u30b7\u30df\u30e5\u30ec\u30fc\u30b7\u30e7\u30f3\u3057\u306a\u3044\u3068\u3069\u3053\u304b\u3067\u9813\u632b\u3059\u308b\u3002 \u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0 \u306f\u7d76\u5bfe \u30c6\u30b9\u30c8 \u30b9\u30af\u30ea\u30d7\u30c8\u5fc5\u8981\u3002 \u5c11\u3057\u305a\u3064 \u30ea\u30d5\u30a1\u30af\u30bf\u30ea\u30f3\u30b0\u21c6\u30c6\u30b9\u30c8\u3092\u884c\u3046 \u30b3\u30fc\u30c9\u3092 \u7406\u89e3\u305b\u305a\u66f8\u304d\u63db\u3048 \u308b\u3068\u3001 \u30d0\u30b0 \u306e\u5143\u3068\u306a\u308b \u5b9f\u9a13 \u306fmlflow\u306a\u3069\u3067 \u81ea\u52d5\u7ba1\u7406 \u3057\u30de\u30cb\u30e5\u30a2\u30eb\u64cd\u4f5c\u3092\u6e1b\u3089\u3059\u3068\u7d50\u679c\u7684\u306b\u304c\u4eba\u7684\u30df\u30b9\u304c\u6e1b\u308b","title":"\u65e9\u3044\u884c\u52d5\u3088\u308a\u30df\u30b9\u306e\u5c11\u306a\u3044\u884c\u52d5\u3092\u3002"},{"location":"%E3%81%9D%E3%81%AE%E4%BB%96/%E3%83%AF%E3%83%BC%E3%82%AF%E3%83%95%E3%83%AD%E3%83%BC/#_4","text":"\u30c7\u30fc\u30bf \u4f5c\u6210\u3068 \u56f3 \u306e\u4f5c\u6210\u30b9\u30af\u30ea\u30d7\u30c8\u306f\u5225\u3005\u306b\u4f5c\u308a\u3001\u56f3\u306e\u4f5c\u6210\u306f\u30b3\u30de\u30f3\u30c9\u4e00\u767a\u3067\u4f5c\u308b \uff08Excel\u3060\u3068\u624b\u9806\u5fd8\u308c\u308b\uff09 \u30d7\u30ed\u30b0\u30e9\u30e0\u306e \u8aac\u660e \u306f\u3057\u3063\u304b\u308a\u6b8b\u3059\u3002README\u3084wiki\u4f7f\u7528\u3057\u3066\u3001\u5171\u540c\u4f5c\u696d\u3084\u81ea\u5206\u306e\u601d\u3044\u51fa\u3057\u306b\u6d3b\u7528\u3059\u308b \u76f8\u95a2 \u3068 \u56e0\u679c \u306f\u5206\u3051\u3066\u8003\u3048\u308b\u3002\u540c\u3058\u8981\u56e0\u3001\u307e\u305f\u306f\u4ecb\u5728\u5909\u6570\u306b\u3088\u3063\u3066\u52d5\u3044\u3066\u3044\u308b\u4e8b\u8c61\u306f\u76f8\u95a2\u3092\u6301\u3064\uff08\u64ec\u4f3c\u76f8\u95a2\uff09\u304c\u3001\u539f\u56e0\u3068\u7d50\u679c\u306e\u95a2\u4fc2\uff08\u56e0\u679c\uff09\u306b\u306a\u3063\u3066\u3044\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u3002","title":"\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u9060\u9694\u306b\u9032\u3081\u308b\u305f\u3081\u306b"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/","text":"Streamlit\u30a2\u30d7\u30ea\u3092Heroku\u3067\u30c6\u30a3\u30d7\u30ed\u30a4\u3059\u308b 1. 3\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210 Procfile main.py \u306e\u90e8\u5206\u306fstreamlit\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u30dd\u30a4\u30f3\u30c8 web: sh setup.sh && streamlit run main.py requirements.txt streamlit --find-links https://download.pytorch.org/whl/torch_stable.html torch==1.7.1+cpu torchvision setup.sh mkdir -p ~/.streamlit echo \"[server] headless = true port = $PORT enableCORS = false \" > ~/.streamlit/config.toml 2. Heroku\u8a2d\u5b9a tips $ heroku plugins:install heroku-accounts $ heroku accounts:add account1 $ heroku accounts:set account2 $ heroku accounts:remove account2 $ heroku accounts 3. Deploy heroku create name git push heroku master # with valid commit","title":"Streamlit\u30a2\u30d7\u30ea\u3092Heroku\u3067\u30c6\u30a3\u30d7\u30ed\u30a4\u3059\u308b"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#streamlitheroku","text":"","title":"Streamlit\u30a2\u30d7\u30ea\u3092Heroku\u3067\u30c6\u30a3\u30d7\u30ed\u30a4\u3059\u308b"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#1-3","text":"","title":"1. 3\u3064\u306e\u30d5\u30a1\u30a4\u30eb\u3092\u4f5c\u6210"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#procfile","text":"main.py \u306e\u90e8\u5206\u306fstreamlit\u306e\u30a8\u30f3\u30c8\u30ea\u30fc\u30dd\u30a4\u30f3\u30c8 web: sh setup.sh && streamlit run main.py","title":"Procfile"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#requirementstxt","text":"streamlit --find-links https://download.pytorch.org/whl/torch_stable.html torch==1.7.1+cpu torchvision","title":"requirements.txt"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#setupsh","text":"mkdir -p ~/.streamlit echo \"[server] headless = true port = $PORT enableCORS = false \" > ~/.streamlit/config.toml","title":"setup.sh"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#2-heroku","text":"tips $ heroku plugins:install heroku-accounts $ heroku accounts:add account1 $ heroku accounts:set account2 $ heroku accounts:remove account2 $ heroku accounts","title":"2. Heroku\u8a2d\u5b9a"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/heroku_st/#3-deploy","text":"heroku create name git push heroku master # with valid commit","title":"3. Deploy"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/","text":"PlantUML\u306e\u30b9\u30b9\u30e1 PlantUML\u3068\u306fUML\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u3092\u30c6\u30ad\u30b9\u30c8\u3067\u4f5c\u6210\u3067\u304d\u308b\u8a00\u8a9e \u56f3\u3092\u66f8\u304f\u3053\u3068\u304c\u3042\u308b\u306a\u3089PlantUML\u3092\u4f7f\u304a\u3046\uff01\u3063\u3068\u8a00\u3046\u3050\u3089\u3044\u306b\u56f3\u3092\u66f8\u304f\u306a\u3089\u3053\u308c\u3063\u3068\u3044\u3046\u30c4\u30fc\u30eb Vscode\u7b49\u4e00\u90e8\u74b0\u5883\u3067\u306fMarkDown\u5185\u306b\u666e\u901a\u306b\u66f8\u304f\u3053\u3068\u3067\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u3055\u308c\u308b \u305f\u307e\u306b\u672a\u5bfe\u5fdc\u306e\u66f8\u304d\u65b9\u304c\u3042\u308a\u3001\u30d7\u30ec\u30d3\u30e5\u30fc\u304c\u30a8\u30e9\u30fc\u3059\u308b\u306e\u3067\u6ce8\u610f\u3002 \u203b < \u306f\u5927\u4f53\u3046\u307e\u304f\u51fa\u529b\u3055\u308c\u306a\u3044 VS Code\u3092\u958b\u767a\u74b0\u5883\u306b\u3059\u308b\u3068\u3001\u3068\u3066\u3082\u6357\u308b( \u4e0b\u8a18\u53c2\u7167 ) PlantUML\u3067\u3067\u304d\u308b\u3053\u3068 \u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u56f3\u3092\u30c6\u30ad\u30b9\u30c8\u3067\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b \u30af\u30e9\u30b9\u56f3 \u30d1\u30c3\u30b1\u30fc\u30b8\u56f3 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u56f3 \u30a2\u30af\u30c6\u30a3\u30d3\u30c6\u30a3\u56f3 \u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u56f3 \u30b9\u30c6\u30fc\u30c8\u30c1\u30e3\u30fc\u30c8\u56f3 \u30b7\u30fc\u30b1\u30f3\u30b9\u56f3 \u30b3\u30e9\u30dc\u30ec\u30fc\u30b7\u30e7\u30f3\u56f3 ER\u56f3 etc... \u30b7\u30fc\u30b1\u30f3\u30b9\u56f3\u3084ER\u56f3\u306a\u3069\u306b\u7528\u3044\u308b\u3053\u3068\u304c\u591a\u3044 \u53c2\u8003 UML\u3068\u306f\uff1f\uff5c\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u306e\u7a2e\u985e\u3084\u7528\u9014\u306b\u3064\u3044\u3066\u89e3\u8aac | PINTO! \u5404\u56f3\u306e\u753b\u50cf\u304c\u3042\u308b\u306e\u3067\u30a4\u30e1\u30fc\u30b8\u304c\u4ed8\u304f\u304b\u898b\u3084\u3059\u3044\u3068\u601d\u3046 \u30e1\u30ea\u30c3\u30c8\u30fb\u30c7\u30e1\u30ea\u30c3\u30c8 \u30c6\u30ad\u30b9\u30c8\u3067\u7ba1\u7406\u3059\u308b\u305f\u3081Git\u306a\u3069\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u304c\u3067\u304d\u308b \u56f3\u306e\u914d\u7f6e\u306a\u3069\u3092\u7d30\u304b\u304f\u60a9\u3080\u5fc5\u8981\u304c\u306a\u3044 \u56f3\u3068\u95a2\u4fc2\u306e\u7dda\u3092\u5b9a\u7fa9\u3059\u308c\u3070\u3001\u3088\u3057\u306a\u306b\u5f15\u304b\u308c\u308b\u306e\u3067\u3001\u30ba\u30ec\u3066\u5fae\u5999\u306b\u306a\u308b\u60a9\u307f\u304b\u3089\u958b\u653e\u3055\u308c\u308b \u9006\u306b\u8abf\u6574\u3057\u305f\u304f\u3066\u3082\u3067\u304d\u306a\u3044\u3068\u3044\u3046\u70b9\u306f\u3042\u308b\u304c\u3001\u3067\u304d\u306a\u3044\u3082\u306e\u306f\u3057\u3087\u3046\u304c\u306a\u3044\u3068\u5272\u308a\u5207\u308c\u308b \u66f8\u304d\u65b9\u3092\u899a\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b GUI\u30c4\u30fc\u30eb\u306a\u3069\u306f\u306a\u3044 \u57fa\u672c\u516c\u5f0f\u306e\u66f8\u304d\u65b9\u3092\u30b3\u30d4\u30da\u3057\u3066\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\u5341\u5206 \u30b5\u30f3\u30d7\u30eb \u3088\u304f\u4f7f\u3046\u56f3\u306e\u30b5\u30f3\u30d7\u30eb ER\u56f3 \u516c\u5f0f\u30b5\u30f3\u30d7\u30eb \u57fa\u672c\u7684\u306b\u62bc\u3055\u3048\u3066\u304a\u3051\u3070\u3044\u3044\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b3\u30ec \u8ad6\u7406\u540d\u3001\u7269\u7406\u540d\u306e\u66f8\u304d\u65b9\u306f\u500b\u4eba\u5dee\u304c\u3067\u3066\u3057\u307e\u3046\u304c\u3001DB\u304c\u8907\u96d1\u306b\u306a\u3063\u3066\u3082\u7dda\u306e\u5f15\u304d\u65b9\u3092\u8ff7\u3063\u305f\u308a\u305b\u305a\u3001\u30c6\u30fc\u30d6\u30eb\u3092\u4f4d\u7f6e\u306a\u3069\u3082\u6c17\u306b\u3057\u306a\u304f\u3066\u826f\u3044\u306e\u304c\u5f37\u3044 \u30b7\u30fc\u30b1\u30f3\u30b9\u56f3 \u516c\u5f0f\u30b5\u30f3\u30d7\u30eb \u4f8b \u3053\u306e\u56f3\u304b\u3089\u5fc5\u8981\u306a\u3082\u306e\u3092\u629c\u7c8b\u3057\u3066\u629c\u304f\u3068\u591a\u5206\u308f\u304b\u308a\u3084\u3059\u3044 \u5927\u4f53\u4e3b\u8981\u306a\u30b7\u30fc\u30b1\u30f3\u30b9\u56f3\u306e\u30d1\u30fc\u30c4\u306f\u4f7f\u3063\u3066\u308b mkdocs \u3067\u8868\u793a\u3067\u304d\u306a\u3044\u30d0\u30b0\u304c\u3042\u308b\u70ba\u30b3\u30fc\u30c9\u3082\u8f09\u305b\u3066\u304a\u304d\u307e\u3059\u3002 @startuml participant \"API\u30b5\u30fc\u30d0\u30fc\" as A box \"Apache Kafka\" #LightBlue participant \"\u63a8\u8ad6Topic\" as B participant \"\u505c\u6b62Topic\" as C end Box participant \"\u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\" as D [-> A: API\u53d7\u4fe1 A ->> B: \u63a8\u8ad6\u306e\u30ad\u30e5\u30fc\u30a4\u30f3\u30b0 activate B hnote over B #LightPink: Waiting... note right: \u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\u304c\u7a7a\u304d\u6b21\u7b2c\u51e6\u7406 B ->> D: \u63a8\u8ad6\u306e\u5b9f\u884c activate D #IndianRed note right \u81ea\u52d5\u7d42\u4e86\u306e\u6761\u4ef6 - \u56de\u6570 - \u6642\u9593\u5236\u9650 - \u51e6\u7406\u306e\u5b8c\u4e86 end note D ->> B: \u30ad\u30e5\u30fc\u306e\u53d7\u53d6\u901a\u77e5 deactivate B D -->> A: \u51e6\u7406\u6bb5\u968e\u3067\u968f\u6642\u30b9\u30c6\u30fc\u30bf\u30b9\u9001\u4fe1 alt \u624b\u52d5\u3067\u7d42\u4e86\u3055\u305b\u308b\u5834\u5408 A ->> C: \u505c\u6b62\u4fe1\u53f7\u3092\u30ad\u30e5\u30fc\u30a4\u30f3\u30b0 activate C hnote over C #LightPink: Waiting... C ->> D: \u5225\u30d7\u30ed\u30bb\u30b9\u3067\u53d7\u4fe1 note right \u8907\u6570\u53f0\u306e\u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\u304c\u3042\u308b\u5834\u5408\u3001 \u3059\u3079\u3066\u306e\u30b5\u30fc\u30d0\u30fc\u304c\u53d7\u4fe1\u3059\u308b end note D ->> D: \u63a8\u8ad6\u51e6\u7406\u306e\u505c\u6b62 note right \u8a72\u5f53\u306eID\u304c\u51e6\u7406\u4e2d\u306e\u5834\u5408\u306b\u505c\u6b62\u3055\u305b\u308b > \u63a8\u8ad6\u4e2d\u306e\u5225\u30d7\u30ed\u30bb\u30b9\u3078\u901a\u4fe1 \u8a72\u5f53\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u4f55\u3082\u3057\u306a\u3044 end note D ->> C: \u30ad\u30e5\u30fc\u306e\u53d7\u53d6\u901a\u77e5 deactivate C end D ->> A: \u7d42\u4e86\u901a\u77e5 deactivate D @enduml Git\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u7f6e\u304d\u65b9 \u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30eb\u30fc\u30c8\u306b plantuml.pu \u3063\u3068\u3044\u3046\u540d\u524d\u3067\u7f6e\u304f\u306e\u304c\u30d9\u30bf \u4ed6\u306b\u3082\u62e1\u5f35\u5b50\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u306e\u3067\u304a\u597d\u307f\u306b VS Code\u3067\u66f8\u304f \u57fa\u672c\u30d7\u30e9\u30b0\u30a4\u30f3\u306eREADME\u901a\u308a \u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u5fc5\u8981\u306a\u30c4\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb $ brew cask install java $ brew install graphviz VS Code\u306e\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u30ea\u30f3\u30af \u4f7f\u3044\u65b9 PlantUML\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304d\u3001 cmd+shift+p \u3067\u30b3\u30de\u30f3\u30c9\u30d1\u30ec\u30c3\u30c8\u3092\u958b\u304d \u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u306e\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u3092\u30d7\u30ec\u30d3\u30e5\u30fc \u3092\u9078\u629e \u53f3\u5074\u306b\u5206\u5272\u30a6\u30a4\u30f3\u30c9\u30a6\u3067\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u306b\u30d7\u30ec\u30d3\u30e5\u30fc\u304c\u8868\u793a\u3055\u308c\u308b \u30d7\u30ec\u30d3\u30e5\u30fc\u8868\u793a\u6642\u306b\u3001PATH\u304c\u901a\u3063\u3066\u3044\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u3067\u308b\u5834\u5408 \u8a2d\u5b9a\u304b\u3089Plantuml\uff1aJava\u306e\u8a2d\u5b9a\u9805\u76ee\u3092\u63a2\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305fJava\u306e\u30d5\u30eb\u30d1\u30b9\u3092\u767b\u9332\u3057\u307e\u3059\u3002 \u53c2\u7167\uff1a \u3010VScode\u3011PlantUML\u306e\u30d3\u30e5\u30fc\u304cjava path\u30a8\u30e9\u30fc\u3067\u8868\u793a\u3055\u308c\u306a\u3044","title":"PlantUML\u306e\u30b9\u30b9\u30e1"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#plantuml","text":"PlantUML\u3068\u306fUML\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u3092\u30c6\u30ad\u30b9\u30c8\u3067\u4f5c\u6210\u3067\u304d\u308b\u8a00\u8a9e \u56f3\u3092\u66f8\u304f\u3053\u3068\u304c\u3042\u308b\u306a\u3089PlantUML\u3092\u4f7f\u304a\u3046\uff01\u3063\u3068\u8a00\u3046\u3050\u3089\u3044\u306b\u56f3\u3092\u66f8\u304f\u306a\u3089\u3053\u308c\u3063\u3068\u3044\u3046\u30c4\u30fc\u30eb Vscode\u7b49\u4e00\u90e8\u74b0\u5883\u3067\u306fMarkDown\u5185\u306b\u666e\u901a\u306b\u66f8\u304f\u3053\u3068\u3067\u30ec\u30f3\u30c0\u30ea\u30f3\u30b0\u3055\u308c\u308b \u305f\u307e\u306b\u672a\u5bfe\u5fdc\u306e\u66f8\u304d\u65b9\u304c\u3042\u308a\u3001\u30d7\u30ec\u30d3\u30e5\u30fc\u304c\u30a8\u30e9\u30fc\u3059\u308b\u306e\u3067\u6ce8\u610f\u3002 \u203b < \u306f\u5927\u4f53\u3046\u307e\u304f\u51fa\u529b\u3055\u308c\u306a\u3044 VS Code\u3092\u958b\u767a\u74b0\u5883\u306b\u3059\u308b\u3068\u3001\u3068\u3066\u3082\u6357\u308b( \u4e0b\u8a18\u53c2\u7167 )","title":"PlantUML\u306e\u30b9\u30b9\u30e1"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#plantuml_1","text":"\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u56f3\u3092\u30c6\u30ad\u30b9\u30c8\u3067\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u308b \u30af\u30e9\u30b9\u56f3 \u30d1\u30c3\u30b1\u30fc\u30b8\u56f3 \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u56f3 \u30a2\u30af\u30c6\u30a3\u30d3\u30c6\u30a3\u56f3 \u30e6\u30fc\u30b9\u30b1\u30fc\u30b9\u56f3 \u30b9\u30c6\u30fc\u30c8\u30c1\u30e3\u30fc\u30c8\u56f3 \u30b7\u30fc\u30b1\u30f3\u30b9\u56f3 \u30b3\u30e9\u30dc\u30ec\u30fc\u30b7\u30e7\u30f3\u56f3 ER\u56f3 etc... \u30b7\u30fc\u30b1\u30f3\u30b9\u56f3\u3084ER\u56f3\u306a\u3069\u306b\u7528\u3044\u308b\u3053\u3068\u304c\u591a\u3044 \u53c2\u8003 UML\u3068\u306f\uff1f\uff5c\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u306e\u7a2e\u985e\u3084\u7528\u9014\u306b\u3064\u3044\u3066\u89e3\u8aac | PINTO! \u5404\u56f3\u306e\u753b\u50cf\u304c\u3042\u308b\u306e\u3067\u30a4\u30e1\u30fc\u30b8\u304c\u4ed8\u304f\u304b\u898b\u3084\u3059\u3044\u3068\u601d\u3046","title":"PlantUML\u3067\u3067\u304d\u308b\u3053\u3068"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_1","text":"\u30c6\u30ad\u30b9\u30c8\u3067\u7ba1\u7406\u3059\u308b\u305f\u3081Git\u306a\u3069\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u7ba1\u7406\u304c\u3067\u304d\u308b \u56f3\u306e\u914d\u7f6e\u306a\u3069\u3092\u7d30\u304b\u304f\u60a9\u3080\u5fc5\u8981\u304c\u306a\u3044 \u56f3\u3068\u95a2\u4fc2\u306e\u7dda\u3092\u5b9a\u7fa9\u3059\u308c\u3070\u3001\u3088\u3057\u306a\u306b\u5f15\u304b\u308c\u308b\u306e\u3067\u3001\u30ba\u30ec\u3066\u5fae\u5999\u306b\u306a\u308b\u60a9\u307f\u304b\u3089\u958b\u653e\u3055\u308c\u308b \u9006\u306b\u8abf\u6574\u3057\u305f\u304f\u3066\u3082\u3067\u304d\u306a\u3044\u3068\u3044\u3046\u70b9\u306f\u3042\u308b\u304c\u3001\u3067\u304d\u306a\u3044\u3082\u306e\u306f\u3057\u3087\u3046\u304c\u306a\u3044\u3068\u5272\u308a\u5207\u308c\u308b \u66f8\u304d\u65b9\u3092\u899a\u3048\u308b\u5fc5\u8981\u304c\u3042\u308b GUI\u30c4\u30fc\u30eb\u306a\u3069\u306f\u306a\u3044 \u57fa\u672c\u516c\u5f0f\u306e\u66f8\u304d\u65b9\u3092\u30b3\u30d4\u30da\u3057\u3066\u7d44\u307f\u5408\u308f\u305b\u308c\u3070\u5341\u5206","title":"\u30e1\u30ea\u30c3\u30c8\u30fb\u30c7\u30e1\u30ea\u30c3\u30c8"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_2","text":"\u3088\u304f\u4f7f\u3046\u56f3\u306e\u30b5\u30f3\u30d7\u30eb","title":"\u30b5\u30f3\u30d7\u30eb"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#er","text":"\u516c\u5f0f\u30b5\u30f3\u30d7\u30eb \u57fa\u672c\u7684\u306b\u62bc\u3055\u3048\u3066\u304a\u3051\u3070\u3044\u3044\u30ec\u30a4\u30a2\u30a6\u30c8\u306f\u30b3\u30ec \u8ad6\u7406\u540d\u3001\u7269\u7406\u540d\u306e\u66f8\u304d\u65b9\u306f\u500b\u4eba\u5dee\u304c\u3067\u3066\u3057\u307e\u3046\u304c\u3001DB\u304c\u8907\u96d1\u306b\u306a\u3063\u3066\u3082\u7dda\u306e\u5f15\u304d\u65b9\u3092\u8ff7\u3063\u305f\u308a\u305b\u305a\u3001\u30c6\u30fc\u30d6\u30eb\u3092\u4f4d\u7f6e\u306a\u3069\u3082\u6c17\u306b\u3057\u306a\u304f\u3066\u826f\u3044\u306e\u304c\u5f37\u3044","title":"ER\u56f3"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_3","text":"\u516c\u5f0f\u30b5\u30f3\u30d7\u30eb","title":"\u30b7\u30fc\u30b1\u30f3\u30b9\u56f3"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_4","text":"\u3053\u306e\u56f3\u304b\u3089\u5fc5\u8981\u306a\u3082\u306e\u3092\u629c\u7c8b\u3057\u3066\u629c\u304f\u3068\u591a\u5206\u308f\u304b\u308a\u3084\u3059\u3044 \u5927\u4f53\u4e3b\u8981\u306a\u30b7\u30fc\u30b1\u30f3\u30b9\u56f3\u306e\u30d1\u30fc\u30c4\u306f\u4f7f\u3063\u3066\u308b mkdocs \u3067\u8868\u793a\u3067\u304d\u306a\u3044\u30d0\u30b0\u304c\u3042\u308b\u70ba\u30b3\u30fc\u30c9\u3082\u8f09\u305b\u3066\u304a\u304d\u307e\u3059\u3002 @startuml participant \"API\u30b5\u30fc\u30d0\u30fc\" as A box \"Apache Kafka\" #LightBlue participant \"\u63a8\u8ad6Topic\" as B participant \"\u505c\u6b62Topic\" as C end Box participant \"\u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\" as D [-> A: API\u53d7\u4fe1 A ->> B: \u63a8\u8ad6\u306e\u30ad\u30e5\u30fc\u30a4\u30f3\u30b0 activate B hnote over B #LightPink: Waiting... note right: \u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\u304c\u7a7a\u304d\u6b21\u7b2c\u51e6\u7406 B ->> D: \u63a8\u8ad6\u306e\u5b9f\u884c activate D #IndianRed note right \u81ea\u52d5\u7d42\u4e86\u306e\u6761\u4ef6 - \u56de\u6570 - \u6642\u9593\u5236\u9650 - \u51e6\u7406\u306e\u5b8c\u4e86 end note D ->> B: \u30ad\u30e5\u30fc\u306e\u53d7\u53d6\u901a\u77e5 deactivate B D -->> A: \u51e6\u7406\u6bb5\u968e\u3067\u968f\u6642\u30b9\u30c6\u30fc\u30bf\u30b9\u9001\u4fe1 alt \u624b\u52d5\u3067\u7d42\u4e86\u3055\u305b\u308b\u5834\u5408 A ->> C: \u505c\u6b62\u4fe1\u53f7\u3092\u30ad\u30e5\u30fc\u30a4\u30f3\u30b0 activate C hnote over C #LightPink: Waiting... C ->> D: \u5225\u30d7\u30ed\u30bb\u30b9\u3067\u53d7\u4fe1 note right \u8907\u6570\u53f0\u306e\u63a8\u8ad6\u30b5\u30fc\u30d0\u30fc\u304c\u3042\u308b\u5834\u5408\u3001 \u3059\u3079\u3066\u306e\u30b5\u30fc\u30d0\u30fc\u304c\u53d7\u4fe1\u3059\u308b end note D ->> D: \u63a8\u8ad6\u51e6\u7406\u306e\u505c\u6b62 note right \u8a72\u5f53\u306eID\u304c\u51e6\u7406\u4e2d\u306e\u5834\u5408\u306b\u505c\u6b62\u3055\u305b\u308b > \u63a8\u8ad6\u4e2d\u306e\u5225\u30d7\u30ed\u30bb\u30b9\u3078\u901a\u4fe1 \u8a72\u5f53\u3057\u3066\u3044\u306a\u3044\u5834\u5408\u306f\u4f55\u3082\u3057\u306a\u3044 end note D ->> C: \u30ad\u30e5\u30fc\u306e\u53d7\u53d6\u901a\u77e5 deactivate C end D ->> A: \u7d42\u4e86\u901a\u77e5 deactivate D @enduml","title":"\u4f8b"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#git","text":"\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u30eb\u30fc\u30c8\u306b plantuml.pu \u3063\u3068\u3044\u3046\u540d\u524d\u3067\u7f6e\u304f\u306e\u304c\u30d9\u30bf \u4ed6\u306b\u3082\u62e1\u5f35\u5b50\u304c\u3044\u304f\u3064\u304b\u3042\u308b\u306e\u3067\u304a\u597d\u307f\u306b","title":"Git\u306e\u30ea\u30dd\u30b8\u30c8\u30ea\u306e\u7f6e\u304d\u65b9"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#vs-code","text":"\u57fa\u672c\u30d7\u30e9\u30b0\u30a4\u30f3\u306eREADME\u901a\u308a","title":"VS Code\u3067\u66f8\u304f"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_5","text":"\u5fc5\u8981\u306a\u30c4\u30fc\u30eb\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb $ brew cask install java $ brew install graphviz VS Code\u306e\u30d7\u30e9\u30b0\u30a4\u30f3\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb \u30ea\u30f3\u30af","title":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#_6","text":"PlantUML\u30d5\u30a1\u30a4\u30eb\u3092\u958b\u304d\u3001 cmd+shift+p \u3067\u30b3\u30de\u30f3\u30c9\u30d1\u30ec\u30c3\u30c8\u3092\u958b\u304d \u30ab\u30fc\u30bd\u30eb\u4f4d\u7f6e\u306e\u30c0\u30a4\u30a2\u30b0\u30e9\u30e0\u3092\u30d7\u30ec\u30d3\u30e5\u30fc \u3092\u9078\u629e \u53f3\u5074\u306b\u5206\u5272\u30a6\u30a4\u30f3\u30c9\u30a6\u3067\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u306b\u30d7\u30ec\u30d3\u30e5\u30fc\u304c\u8868\u793a\u3055\u308c\u308b","title":"\u4f7f\u3044\u65b9"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/plantuml/#path","text":"\u8a2d\u5b9a\u304b\u3089Plantuml\uff1aJava\u306e\u8a2d\u5b9a\u9805\u76ee\u3092\u63a2\u3057\u3001\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305fJava\u306e\u30d5\u30eb\u30d1\u30b9\u3092\u767b\u9332\u3057\u307e\u3059\u3002 \u53c2\u7167\uff1a \u3010VScode\u3011PlantUML\u306e\u30d3\u30e5\u30fc\u304cjava path\u30a8\u30e9\u30fc\u3067\u8868\u793a\u3055\u308c\u306a\u3044","title":"\u30d7\u30ec\u30d3\u30e5\u30fc\u8868\u793a\u6642\u306b\u3001PATH\u304c\u901a\u3063\u3066\u3044\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u3067\u308b\u5834\u5408"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/","text":"tmux\u306f\u3048\u3048\u305e \u305d\u3082\u305d\u3082tmux\uff08\u30c6\u30a3\u30fc\u30de\u30c3\u30af\u30b9\uff09\u3068\u306f tmux\u306f\u7aef\u672b\u591a\u91cd\u5316\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u4e00\u3064\u3002\u4e00\u3064\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u4e0a\u3067\u8907\u6570\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3092\u7acb\u3061\u4e0a\u3052\u3066\u4e26\u884c\u3057\u3066\u4f5c\u696d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3082\u306e\u3002 \u30b5\u30fc\u30d0\u30fc\u5074\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u3051\u3070SSH\u63a5\u7d9a\u3067\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5074\u3067\u3082\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u3055\u3089\u306btmux\u3092\u4f7f\u3046\u3068 screen \u306e\u69d8\u306b\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u5207\u308a\u96e2\u3057\u3092\u884c\u3048\u308b\u306e\u3067\u5b66\u7fd2\u3092\u6b62\u3081\u305a\u306bssh\u3092\u7d42\u4e86\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4ed6\u306b\u3082 tensorboard \u3092\u898b\u306a\u304c\u3089\u5225\u306e\u4f5c\u696d\u3092\u3057\u305f\u3044\u6642\u306b\u8907\u6570\u56dessh\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u3064\u304f\u3089\u305a\u306b\u4e00\u3064\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304b\u3089\u5206\u5c90\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 tmux\u306e\u69cb\u9020 \u30bb\u30c3\u30b7\u30e7\u30f3\uff1a 1\u3064\u4ee5\u4e0a\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u30bf\u30fc\u30df\u30ca\u30eb\u5168\u4f53 \u30a6\u30a3\u30f3\u30c9\u30a6\uff1a \u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u306b\u958b\u304b\u308c\u3066\u3044\u308b\u30011\u3064\u4ee5\u4e0a\u306e\u30da\u30a4\u30f3\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u30bf\u30fc\u30df\u30ca\u30eb\u5168\u4f53 \u30da\u30a4\u30f3\uff1a \u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u3067\u5206\u5272\u3055\u308c\u3066\u3044\u308b1\u3064\u306e\u30bf\u30fc\u30df\u30ca\u30eb \u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8 \u83ab\u5927\u306a\u6570\u306e\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\u304c\u3046\u307e\u304f\u307e\u3068\u307e\u3063\u305f\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u30c1\u30e9\u30c1\u30e9\u307f\u306a\u304c\u3089\u4f7f\u3044\u307e\u3057\u3087\u3046 \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8 \u30ab\u30b9\u30bf\u30de\u30a4\u30ba ~/.tmux.conf \u3067\u7ba1\u7406\u3055\u308c\u3066\u3044\u308b\u3002\u306a\u306e\u3067 ~/.tmux.conf \u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30ad\u30fc\u30d0\u30a4\u30f3\u30c9\u306a\u3069\u5909\u66f4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u4ee5\u4e0bQiita\u304b\u3089\u304a\u3059\u3059\u3081\u8a18\u4e8b\u4e8c\u9078 \u300ctmux\u304c\u5feb\u9069\u3001\u304b\u3064 .tmux.conf \u306e\u8a2d\u5b9a\u300d \u300ctmux\u3092\u5fc5\u8981\u6700\u4f4e\u9650\u3067\u5165\u9580\u3057\u3066\u4f7f\u3046\u300d \u4f7f\u3044\u65b9 \u57fa\u672c\u306f \uff1c\u8d77\u52d5\uff1e tmux <\u7d42\u4e86> tmux kill-session \uff1c\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3057\u305f\u307e\u307e\u4e00\u65e6\u96e2\u8131\uff08\u30c7\u30bf\u30c3\u30c1\uff09\uff1e prefix d \uff1c\u5143\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u306b\u63a5\u7d9a\uff08\u30a2\u30bf\u30c3\u30c1\uff09\uff1e $ tmux a \u3067\u304ak\u3002 \u3061\u306a\u307f\u306b\u3001\u591a\u6bb5ssh\u3067\u63a5\u7d9a\u3059\u308b\u6642\u306f \u4f5c\u696d\u3059\u308b\u4e00\u3064\u524d\u306e\u30b5\u30fc\u30d0\u30fc\u304b\u3089tmux\u3092\u8d77\u52d5\u3057\u3066\u304f\u3060\u3055\u3044 \u3002 \u81ea\u5206\u306f\u8e0f\u307f\u53f0\u30b5\u30fc\u30d0\u30fc\u7d4c\u7531\u3067gpu\u3092\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u8e0f\u307f\u53f0\u30b5\u30fc\u30d0\u30fc\u304b\u3089tmux\u3067\u5165\u3063\u3066\u3044\u307e\u3059\u3002","title":"tmux\u306f\u3048\u3048\u305e"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#tmux","text":"","title":"tmux\u306f\u3048\u3048\u305e"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#tmux_1","text":"tmux\u306f\u7aef\u672b\u591a\u91cd\u5316\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u306e\u4e00\u3064\u3002\u4e00\u3064\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u4e0a\u3067\u8907\u6570\u306e\u30bf\u30fc\u30df\u30ca\u30eb\u3092\u7acb\u3061\u4e0a\u3052\u3066\u4e26\u884c\u3057\u3066\u4f5c\u696d\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3082\u306e\u3002 \u30b5\u30fc\u30d0\u30fc\u5074\u306b\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u3066\u304a\u3051\u3070SSH\u63a5\u7d9a\u3067\u30af\u30e9\u30a4\u30a2\u30f3\u30c8\u5074\u3067\u3082\u4f7f\u7528\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3059\u3002 \u3055\u3089\u306btmux\u3092\u4f7f\u3046\u3068 screen \u306e\u69d8\u306b\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u5207\u308a\u96e2\u3057\u3092\u884c\u3048\u308b\u306e\u3067\u5b66\u7fd2\u3092\u6b62\u3081\u305a\u306bssh\u3092\u7d42\u4e86\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u4ed6\u306b\u3082 tensorboard \u3092\u898b\u306a\u304c\u3089\u5225\u306e\u4f5c\u696d\u3092\u3057\u305f\u3044\u6642\u306b\u8907\u6570\u56dessh\u30bb\u30c3\u30b7\u30e7\u30f3\u3092\u3064\u304f\u3089\u305a\u306b\u4e00\u3064\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u304b\u3089\u5206\u5c90\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002","title":"\u305d\u3082\u305d\u3082tmux\uff08\u30c6\u30a3\u30fc\u30de\u30c3\u30af\u30b9\uff09\u3068\u306f"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#tmux_2","text":"\u30bb\u30c3\u30b7\u30e7\u30f3\uff1a 1\u3064\u4ee5\u4e0a\u306e\u30a6\u30a3\u30f3\u30c9\u30a6\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u30bf\u30fc\u30df\u30ca\u30eb\u5168\u4f53 \u30a6\u30a3\u30f3\u30c9\u30a6\uff1a \u30bb\u30c3\u30b7\u30e7\u30f3\u5185\u306b\u958b\u304b\u308c\u3066\u3044\u308b\u30011\u3064\u4ee5\u4e0a\u306e\u30da\u30a4\u30f3\u3092\u7ba1\u7406\u3057\u3066\u3044\u308b\u30bf\u30fc\u30df\u30ca\u30eb\u5168\u4f53 \u30da\u30a4\u30f3\uff1a \u30a6\u30a3\u30f3\u30c9\u30a6\u5185\u3067\u5206\u5272\u3055\u308c\u3066\u3044\u308b1\u3064\u306e\u30bf\u30fc\u30df\u30ca\u30eb","title":"tmux\u306e\u69cb\u9020"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#_1","text":"\u83ab\u5927\u306a\u6570\u306e\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8\u304c\u3042\u308a\u307e\u3059\u304c\u3046\u307e\u304f\u307e\u3068\u307e\u3063\u305f\u8a18\u4e8b\u304c\u3042\u3063\u305f\u306e\u3067\u30c1\u30e9\u30c1\u30e9\u307f\u306a\u304c\u3089\u4f7f\u3044\u307e\u3057\u3087\u3046 \u30c1\u30fc\u30c8\u30b7\u30fc\u30c8","title":"\u30b7\u30e7\u30fc\u30c8\u30ab\u30c3\u30c8"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#_2","text":"~/.tmux.conf \u3067\u7ba1\u7406\u3055\u308c\u3066\u3044\u308b\u3002\u306a\u306e\u3067 ~/.tmux.conf \u3092\u5909\u66f4\u3059\u308b\u3053\u3068\u3067\u30ad\u30fc\u30d0\u30a4\u30f3\u30c9\u306a\u3069\u5909\u66f4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002 \u4ee5\u4e0bQiita\u304b\u3089\u304a\u3059\u3059\u3081\u8a18\u4e8b\u4e8c\u9078 \u300ctmux\u304c\u5feb\u9069\u3001\u304b\u3064 .tmux.conf \u306e\u8a2d\u5b9a\u300d \u300ctmux\u3092\u5fc5\u8981\u6700\u4f4e\u9650\u3067\u5165\u9580\u3057\u3066\u4f7f\u3046\u300d","title":"\u30ab\u30b9\u30bf\u30de\u30a4\u30ba"},{"location":"%E3%83%84%E3%83%BC%E3%83%AB/tmux/#_3","text":"\u57fa\u672c\u306f \uff1c\u8d77\u52d5\uff1e tmux <\u7d42\u4e86> tmux kill-session \uff1c\u30b3\u30fc\u30c9\u3092\u52d5\u304b\u3057\u305f\u307e\u307e\u4e00\u65e6\u96e2\u8131\uff08\u30c7\u30bf\u30c3\u30c1\uff09\uff1e prefix d \uff1c\u5143\u306e\u30bb\u30c3\u30b7\u30e7\u30f3\u306b\u63a5\u7d9a\uff08\u30a2\u30bf\u30c3\u30c1\uff09\uff1e $ tmux a \u3067\u304ak\u3002 \u3061\u306a\u307f\u306b\u3001\u591a\u6bb5ssh\u3067\u63a5\u7d9a\u3059\u308b\u6642\u306f \u4f5c\u696d\u3059\u308b\u4e00\u3064\u524d\u306e\u30b5\u30fc\u30d0\u30fc\u304b\u3089tmux\u3092\u8d77\u52d5\u3057\u3066\u304f\u3060\u3055\u3044 \u3002 \u81ea\u5206\u306f\u8e0f\u307f\u53f0\u30b5\u30fc\u30d0\u30fc\u7d4c\u7531\u3067gpu\u3092\u4f7f\u3063\u3066\u3044\u308b\u306e\u3067\u8e0f\u307f\u53f0\u30b5\u30fc\u30d0\u30fc\u304b\u3089tmux\u3067\u5165\u3063\u3066\u3044\u307e\u3059\u3002","title":"\u4f7f\u3044\u65b9"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/","text":"Graph Attention Network \u89e3\u8aac \u3069\u3093\u306a\u30e2\u30c7\u30eb\uff1f \u30ce\u30fc\u30c9\u306e\u7279\u5fb4\u8868\u73fe\u3092\u901a\u5e38\u306e\u7573\u307f\u8fbc\u307f\u3067\u306f\u306a\u304f\uff0c\u5468\u8fba\u30ce\u30fc\u30c9\u306e\u91cd\u307f\u4ed8\u304d\u548c\u3067\u8868\u73fe \u3059\u308b ---> \u8981\u3059\u308b\u306b\uff0c \u30a8\u30c3\u30b8\u8868\u73fe\u3092\u30b7\u30f3\u30d7\u30eb\u306battention weight \u3067\u8868\u73fe \u7570\u306a\u308b\u30b5\u30a4\u30ba\u306e\u8fd1\u508d\u3092\u6271\u3044\u306a\u304c\u3089\u3001\u8fd1\u508d\u5185\u306e\u7570\u306a\u308b\u30ce\u30fc\u30c9\u306b\u7570\u306a\u308b\u91cd\u8981\u5ea6\u3092\uff08\u6697\u9ed9\u7684\u306b\uff09\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u308b \u30b0\u30e9\u30d5\u69cb\u9020\u5168\u4f53\u3092\u4e8b\u524d\u306b\u77e5\u308b\u3053\u3068\u306b\u4f9d\u5b58\u3057\u306a\u3044\u306a\u3069\u3001 \u5f93\u6765\u306e\u30b9\u30da\u30af\u30c8\u30eb\u30d9\u30fc\u30b9 \u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u7406\u8ad6\u7684\u306a\u554f\u984c\u306e\u591a\u304f\u3092\u89e3\u6c7a \u30a8\u30c3\u30b8\u3092\u5358\u7d14\u306b\u8868\u73fe\u3067\u304d\u308b\u305f\u3081\u8a08\u7b97\u901f\u5ea6\u3082\u305d\u3053\u305d\u3053\u6539\u5584 \u76ee\u7684 GNN \u3067\u8fd1\u5e74\u983b\u7e41\u306b\u4f7f\u308f\u308c\u3066\u3044\u308b GATs(Gragh Attention Networks) \u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b \u524d\u63d0 \u8ad6\u6587\u306f\u3053\u3061\u3089 ICLR 2018 \u80cc\u666f graph\u306f\u30ce\u30fc\u30c9\u306b\u52a0\u3048\u3066\u30a8\u30c3\u30b8\u306e\u60c5\u5831\u3082\u91cd\u8981 \u3057\u304b\u3057\u306a\u304c\u3089\uff0c\u30a8\u30c3\u30b8\u306e\u6f5c\u5728\u8868\u73fe\u3092\u4f5c\u3063\u3066\u3044\u3066\u306f\u8a08\u7b97\u901f\u5ea6\u304c\u9045\u3044 -> \u30a8\u30c3\u30b8\u3092\u5358\u7d14\u306battention\u306e\u91cd\u307f\u3067\u8868\u73fe\u3059\u308b \u624b\u6cd5 GATs \u306e\u6982\u8981\u56f3\u3092\u4ee5\u4e0b\u306b\u793a\u3059 \u30ce\u30fc\u30c9\u306e\u66f4\u65b0\u5f0f\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u73fe \u5b9f\u969b\u306b\u306f Multi-Head Attention (head\u6570=K) \u3092\u8a08\u7b97\u3059\u308b (|| \u306f concat \u3092\u610f\u5473\u3059\u308b) \u3082\u3046\u5c11\u3057\u565b\u307f\u7815\u3044\u3066 \u5404\u3005\u306e\u5f0f\u306e\u89e3\u91c8 (1) node i \u306e\u7279\u5fb4\u91cf\u3092 linear\u5c64\u3067\u5909\u63db (2) \u96a3\u63a5\u3059\u308bnode i \u3068 j \u3092 concatenate (\u5f0f\u306e || \u306f concatenate) \u3057\uff0clinear + LeReLU \u3067\u30a8\u30cd\u30eb\u30ae\u30fc\u95a2\u6570\u3092\u8a08\u7b97 dot-product attention\u3067\u306f\u306a\u304f additive attention(\u52a0\u6cd5\u6ce8\u610f) \u3068\u547c\u3070\u308c\u308b\u3082\u306e (3) \u91cd\u307f\u306b\u5909\u63db (4) node i \u306e Layer (l+1) \u306e\u7279\u5fb4\u91cf\u306f attention\u306b\u3088\u308a\u8a08\u7b97\u3057\u305f(3) \u3068 (1) \u306e\u91cd\u307f\u4ed8\u304d\u548c\u3067\u8868\u73fe \u5b9f\u9a13 Transductive Learning \u5b66\u7fd2\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u3068\u30c6\u30b9\u30c8\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u304c\u540c\u3058 (\u3059\u3067\u306b\u5b58\u5728\u3057\u3066\u3044\u308b\u304c\uff0c\u30e9\u30d9\u30eb\u304c\u672a\u77e5\u306e\u30ce\u30fc\u30c9\u3092\u4e88\u6e2c\u3059\u308b\u306a\u3069) - 3\u3064\u306ecitation network benchmark dataset\u3092\u4f7f\u7528 Inductive Learning \u5b66\u7fd2\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u3068\u30c6\u30b9\u30c8\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u304c\u7570\u306a\u308b\u5834\u5408\u304c\u3042\u308b (\u65b0\u3057\u3044\u30a8\u30c3\u30b8\u3084\u30ce\u30fc\u30c9\u306e\u4e88\u6e2c\uff0c\u5225\u306e\u30b0\u30e9\u30d5\u3067\u306e\u8a55\u4fa1\u306a\u3069) - \u30bf\u30f3\u30d1\u30af\u8cea\u306e\u76f8\u4e92\u4f5c\u7528\u3092\u8868\u3059dataset (PPI dataset) \u4e00\u822c\u7684\u306bInductive\u306a\u8a2d\u5b9a\u306e\u65b9\u304c\u6c4e\u5316\u6027\u80fd\u304c\u6c42\u3081\u3089\u308c\u308b \u53ef\u8996\u5316\u4f8b node\u9593\u306e\u5f37\u3044\u7d50\u3073\u4ed8\u304d\u306f\u5927\u304d\u3044\u91cd\u307f\u3067\u8868\u73fe\u3059\u308b\u3068\u3044\u3046\u30b7\u30f3\u30d7\u30eb\u306a\u65b9\u6cd5\u304c\u8868\u73fe\u3067\u304d\u3066\u3044\u308b epochs\u6bce\u306e node classification \u306e\u53ef\u8996\u5316\u4f8b(\u4f59\u8ac7) \u53c2\u8003\u8cc7\u6599 https://www.slideshare.net/takahirokubo7792/graph-attention-network","title":"Graph Attention Network \u89e3\u8aac"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#graph-attention-network","text":"","title":"Graph Attention Network \u89e3\u8aac"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_1","text":"\u30ce\u30fc\u30c9\u306e\u7279\u5fb4\u8868\u73fe\u3092\u901a\u5e38\u306e\u7573\u307f\u8fbc\u307f\u3067\u306f\u306a\u304f\uff0c\u5468\u8fba\u30ce\u30fc\u30c9\u306e\u91cd\u307f\u4ed8\u304d\u548c\u3067\u8868\u73fe \u3059\u308b ---> \u8981\u3059\u308b\u306b\uff0c \u30a8\u30c3\u30b8\u8868\u73fe\u3092\u30b7\u30f3\u30d7\u30eb\u306battention weight \u3067\u8868\u73fe \u7570\u306a\u308b\u30b5\u30a4\u30ba\u306e\u8fd1\u508d\u3092\u6271\u3044\u306a\u304c\u3089\u3001\u8fd1\u508d\u5185\u306e\u7570\u306a\u308b\u30ce\u30fc\u30c9\u306b\u7570\u306a\u308b\u91cd\u8981\u5ea6\u3092\uff08\u6697\u9ed9\u7684\u306b\uff09\u5272\u308a\u5f53\u3066\u308b\u3053\u3068\u304c\u3067\u304d\u308b \u30b0\u30e9\u30d5\u69cb\u9020\u5168\u4f53\u3092\u4e8b\u524d\u306b\u77e5\u308b\u3053\u3068\u306b\u4f9d\u5b58\u3057\u306a\u3044\u306a\u3069\u3001 \u5f93\u6765\u306e\u30b9\u30da\u30af\u30c8\u30eb\u30d9\u30fc\u30b9 \u306e\u30a2\u30d7\u30ed\u30fc\u30c1\u306e\u7406\u8ad6\u7684\u306a\u554f\u984c\u306e\u591a\u304f\u3092\u89e3\u6c7a \u30a8\u30c3\u30b8\u3092\u5358\u7d14\u306b\u8868\u73fe\u3067\u304d\u308b\u305f\u3081\u8a08\u7b97\u901f\u5ea6\u3082\u305d\u3053\u305d\u3053\u6539\u5584","title":"\u3069\u3093\u306a\u30e2\u30c7\u30eb\uff1f"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_2","text":"GNN \u3067\u8fd1\u5e74\u983b\u7e41\u306b\u4f7f\u308f\u308c\u3066\u3044\u308b GATs(Gragh Attention Networks) \u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b","title":"\u76ee\u7684"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_3","text":"\u8ad6\u6587\u306f\u3053\u3061\u3089 ICLR 2018","title":"\u524d\u63d0"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_4","text":"graph\u306f\u30ce\u30fc\u30c9\u306b\u52a0\u3048\u3066\u30a8\u30c3\u30b8\u306e\u60c5\u5831\u3082\u91cd\u8981 \u3057\u304b\u3057\u306a\u304c\u3089\uff0c\u30a8\u30c3\u30b8\u306e\u6f5c\u5728\u8868\u73fe\u3092\u4f5c\u3063\u3066\u3044\u3066\u306f\u8a08\u7b97\u901f\u5ea6\u304c\u9045\u3044 -> \u30a8\u30c3\u30b8\u3092\u5358\u7d14\u306battention\u306e\u91cd\u307f\u3067\u8868\u73fe\u3059\u308b","title":"\u80cc\u666f"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_5","text":"GATs \u306e\u6982\u8981\u56f3\u3092\u4ee5\u4e0b\u306b\u793a\u3059 \u30ce\u30fc\u30c9\u306e\u66f4\u65b0\u5f0f\u306f\u4ee5\u4e0b\u306e\u5f0f\u3067\u8868\u73fe \u5b9f\u969b\u306b\u306f Multi-Head Attention (head\u6570=K) \u3092\u8a08\u7b97\u3059\u308b (|| \u306f concat \u3092\u610f\u5473\u3059\u308b)","title":"\u624b\u6cd5"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_6","text":"\u5404\u3005\u306e\u5f0f\u306e\u89e3\u91c8 (1) node i \u306e\u7279\u5fb4\u91cf\u3092 linear\u5c64\u3067\u5909\u63db (2) \u96a3\u63a5\u3059\u308bnode i \u3068 j \u3092 concatenate (\u5f0f\u306e || \u306f concatenate) \u3057\uff0clinear + LeReLU \u3067\u30a8\u30cd\u30eb\u30ae\u30fc\u95a2\u6570\u3092\u8a08\u7b97 dot-product attention\u3067\u306f\u306a\u304f additive attention(\u52a0\u6cd5\u6ce8\u610f) \u3068\u547c\u3070\u308c\u308b\u3082\u306e (3) \u91cd\u307f\u306b\u5909\u63db (4) node i \u306e Layer (l+1) \u306e\u7279\u5fb4\u91cf\u306f attention\u306b\u3088\u308a\u8a08\u7b97\u3057\u305f(3) \u3068 (1) \u306e\u91cd\u307f\u4ed8\u304d\u548c\u3067\u8868\u73fe","title":"\u3082\u3046\u5c11\u3057\u565b\u307f\u7815\u3044\u3066"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_7","text":"","title":"\u5b9f\u9a13"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#transductive-learning","text":"\u5b66\u7fd2\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u3068\u30c6\u30b9\u30c8\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u304c\u540c\u3058 (\u3059\u3067\u306b\u5b58\u5728\u3057\u3066\u3044\u308b\u304c\uff0c\u30e9\u30d9\u30eb\u304c\u672a\u77e5\u306e\u30ce\u30fc\u30c9\u3092\u4e88\u6e2c\u3059\u308b\u306a\u3069) - 3\u3064\u306ecitation network benchmark dataset\u3092\u4f7f\u7528","title":"Transductive Learning"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#inductive-learning","text":"\u5b66\u7fd2\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u3068\u30c6\u30b9\u30c8\u306b\u4f7f\u3046\u30b0\u30e9\u30d5\u304c\u7570\u306a\u308b\u5834\u5408\u304c\u3042\u308b (\u65b0\u3057\u3044\u30a8\u30c3\u30b8\u3084\u30ce\u30fc\u30c9\u306e\u4e88\u6e2c\uff0c\u5225\u306e\u30b0\u30e9\u30d5\u3067\u306e\u8a55\u4fa1\u306a\u3069) - \u30bf\u30f3\u30d1\u30af\u8cea\u306e\u76f8\u4e92\u4f5c\u7528\u3092\u8868\u3059dataset (PPI dataset) \u4e00\u822c\u7684\u306bInductive\u306a\u8a2d\u5b9a\u306e\u65b9\u304c\u6c4e\u5316\u6027\u80fd\u304c\u6c42\u3081\u3089\u308c\u308b \u53ef\u8996\u5316\u4f8b node\u9593\u306e\u5f37\u3044\u7d50\u3073\u4ed8\u304d\u306f\u5927\u304d\u3044\u91cd\u307f\u3067\u8868\u73fe\u3059\u308b\u3068\u3044\u3046\u30b7\u30f3\u30d7\u30eb\u306a\u65b9\u6cd5\u304c\u8868\u73fe\u3067\u304d\u3066\u3044\u308b epochs\u6bce\u306e node classification \u306e\u53ef\u8996\u5316\u4f8b(\u4f59\u8ac7)","title":"Inductive Learning"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/GNN/#_8","text":"https://www.slideshare.net/takahirokubo7792/graph-attention-network","title":"\u53c2\u8003\u8cc7\u6599"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/","text":"Mesh RCNN\u306e\u304a\u6c17\u6301\u3061\u3092\u7406\u89e3\u3057\u305f\u3044 \u76ee\u7684 Mesh R CNN \u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b Lv1. \u304a\u6c17\u6301\u3061\u306e\u7406\u89e3 Lv2. \u3069\u3093\u306aNetwork\u69cb\u9020 Lv3. loss \u306a\u3069\u8a73\u7d30\u306a\u8a2d\u8a08 \u524d\u63d0 ICCV 2019 \u5143\u8ad6\u6587\u306f\u3053\u3061\u3089\u304b\u3089 - github \u306f\u3053\u3061\u3089 https://github.com/facebookresearch/meshrcnn \u6982\u8981 2D image \u304b\u3089 mesh(3D \u5f62\u72b6) \u3092 \u7372\u5f97\u3059\u308b\u624b\u6cd5(\u4e0b\u56f3) \u5165\u529b\u304b\u3089\u51fa\u529b\u307e\u3067End-to-End\u5b66\u7fd2 \u80cc\u666f(\u95a2\u9023\u7814\u7a76) 3D\u95a2\u9023\u306e\u7814\u7a76\u304c\u8fd1\u5e74\u76db\u3093 \u69d8\u3005\u306a\u6280\u8853\u304c\u5b58\u5728\u3059\u308b\u304b\u95a2\u9023\u3059\u308b\u4ee3\u8868\u7684\u306a\u3082\u306e\u3092\u7c21\u5358\u306b\u7d39\u4ecb\u3059\u308b 2D Object Recognition \u5358\u4e00\u753b\u50cf\u3092\u5165\u529b\u3057\u3066\u7279\u5b9a\u306e\u7269\u4f53\u3092\u691c\u51fa\u3059\u308b\uff0eBbox\u306b\u3088\u308b\u77e9\u5f62\u306e\u62bd\u51fa\u3068\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3059\u308b\u3082\u306e\u304c\u4e00\u822c\u7684 mask R-CNN \u3067\u306f\u3053\u308c\u306b\u52a0\u3048\u3066\u7269\u4f53\u306e\u9818\u57df\u3092\u8868\u3059\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u3092\u62bd\u51fa\u3059\u308b Single-View Shape Prediction \u5358\u4e00\u753b\u50cf\u30923\u6b21\u5143\u518d\u69cb\u6210\u3059\u308b\uff0e3D\u306epose \u3084\u65e2\u77e5\u306e\u5f62\u72b6\u306e\u5411\u304d\u3092\u63a8\u5b9a\u3059\u308b\u3082\u306e\u304c\u4e00\u822c\u7684 Multi-View Shape Prediction \u8907\u6570\u30ab\u30e1\u30e9\u306b\u3088\u308b\u753b\u50cf\u304b\u3089\uff13\u6b21\u5143\u518d\u69cb\u6210\u3092\u884c\u3046\uff0emesh R-CNN \u3067\u306f\u6271\u308f\u306a\u3044\u304c\uff0c\u3053\u306e\u5206\u91ce\u306e\u7814\u7a76\u306f\u8fd1\u5e74\u76db\u3093 3D input \uff13\u6b21\u5143\u753b\u50cf\u3092\u5165\u529b\u3057\u3066semantic label \u306a\u3069\u3092\u51fa\u529b\u3059\u308b\u30bf\u30b9\u30af RGB-D images\u3084point cloud \u306a\u3069\u3092\u5165\u529b\u3059\u308b\u306e\u304c\u4e00\u822c\u7684 3D dataset imagenet \u3084 COCO dataset\u306b\u3088\u308a 2D\u5206\u91ce\u306f\u5927\u5e45\u306a\u767a\u5c55\u3092\u9042\u3052\u305f 3D dataset \u306f 2D \u306b\u6bd4\u3079\u3066\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u304c\u56f0\u96e3\u3067\u3042\u308b\u554f\u984c\u304b\u3089\u3044\u307e\u3060\u767a\u5c55\u9014\u4e0a\u306e\u9818\u57df\u3067\u3042\u308b ShapeNet, Pix3D, Pascal3D \u306a\u3069\u69d8\u3005\u306a\u3082\u306e\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u304c\u5408\u6210\u30c7\u30fc\u30bf\u3067\u3042\u308b\u3082\u306e\u3084\u77e9\u5f62\u306e\u307f\u3067\u5f62\u72b6\u306e\u6ce8\u91c8\u304c\u306a\u3044\u306a\u3069\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u7279\u6709\u306e\u554f\u984c\u306a\u3069\u3082\u591a\u3044 \u624b\u6cd5 Lv1.\u304a\u6c17\u6301\u3061\u306e\u7406\u89e3 \u5927\u307e\u304b\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u4ee5\u4e0b\u56f3\u306b\u793a\u3059\u3088\u3046\u306b\uff0c \u7269\u4f53\u306e\u691c\u51fa \u2192 \u7269\u4f53\u306eboxel\u5316 \u2192 mesh\u306b\u5909\u63db \u2192 mesh\u3092\u6539\u826f(refinement) \u306estep \u3067\u69cb\u6210\u3055\u308c\u308b\u3068\u601d\u3063\u3066\u826f\u3044\uff0e Lv2 Network\u306a\u3069\u3082\u542b\u3081\u3066\u89e3\u8aac \u4e0b\u56f3\u306b\u63d0\u6848\u30b7\u30b9\u30c6\u30e0\u306e\u8a73\u7d30\u306a\u6d41\u308c\u3092\u793a\u3059\uff0e \u5927\u307e\u304b\u306a\u6d41\u308c\u306fLv1 \u3067\u8aac\u660e\u3057\u305f\u304c\u3082\u3046\u5c11\u3057\u8a73\u7d30\u306b\u8ff0\u3079\u308b\uff0e \u307e\u305a\uff0c\u7269\u4f53\u691c\u51fa\u90e8\u5206\u3067\u306f\u5f93\u6765\u306eSoTA model \u3067\u3042\u308b mask R-CNN \u3092\u7528\u3044\u3066\u3044\u308b \u2192 \u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb, bbox, segmentation mask \u3092\u51fa\u529b backbone \u306f resnet50(pre-trained by imagenet) 3D \u5f62\u72b6\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306b\uff0c Voxel Branch \u3068 Mesh Refinement Branch \u3092\u7528\u3044\u308b \u540d\u524d\u306e\u901a\u308a\u3067\u306f\u3042\u308b\u304c\uff0c - Boxel Branch .. object \u306e\u5927\u307e\u304b\u306a 3\u6b21\u5143\u30dc\u30af\u30bb\u30eb\u5316\u3092\u63a8\u5b9a\u3057\uff0c\u521d\u671f\u4e09\u89d2mesh \u306b\u5909\u63db - Mesh Refinement Branch .. \u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f\u5c64\u3092\u4f7f\u7528\u3057\u3066\u3001\u3053\u306e\u521d\u671f\u30e1\u30c3\u30b7\u30e5\u306e\u9802\u70b9\u4f4d\u7f6e\u3092\u8abf\u6574(mesh\u3092\u6574\u3048\u308b) \u3068\u8a00\u3063\u305f\u5f79\u5272\u304c\u3042\u308b\uff0e Voxel Branch \u5404object \u306e\u5f62\u72b6\u3092\u30dc\u30c8\u30e0\u30a2\u30c3\u30d7\u3067\u4e88\u6e2c \u6c17\u6301\u3061\u3068\u3057\u3066\u306f\uff0cmask R-CNN \u306e mask branch \u3068\u4f3c\u305f\u3088\u3046\u306a\u6a5f\u80fd 2D \u5e73\u9762\u306e M * M \u2192 3D \u306a G * G * G \u306e\u5f62\u72b6\u3092\u4e88\u6e2c Loss \u306b\u3064\u3044\u3066\u306f\u5f8c\u8ff0(Lv3 \u306b\u3066) (\u88dc\u8db3) \u753b\u50cf\u3068\u4e88\u6e2c\u5024\u3068\u306e\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e\u5bfe\u5fdc\u95a2\u4fc2\u3092\u7dad\u6301\u3059\u308b\u3053\u3068\u306f\u30013D\u3067\u306f\u30ab\u30e1\u30e9\u304b\u3089\u96e2\u308c\u308b\u306b\u3064\u308c\u3066\u7269\u4f53\u304c\u5c0f\u3055\u304f\u306a\u308b\u305f\u3081\u8907\u96d1\u3067\u3042\u308b\uff0e\u305d\u308c\u3092\u56de\u907f\u3059\u308b\u305f\u3081\u306b\u30ab\u30e1\u30e9\u7279\u6709\u306e\u56fa\u6709\u884c\u5217\u3092\u5229\u7528\u3057\u3066\u7406\u60f3\u7684\u306a\u5f62\u72b6\u306b\u3059\u308b\u5de5\u592b\u3092\u884c\u306a\u3063\u3066\u3044\u308b Cubify(\u3053\u3053\u304c\u809d\u3063\u307d\u3044) Voxel to Mesh \u76ee\u7684\u306f\uff0c \u30dc\u30af\u30bb\u30eb\u4e88\u6e2c\u5024\u3092\u4e09\u89d2\u5f62\u306e\u30e1\u30c3\u30b7\u30e5\u306b\u5909\u63db\u3059\u308b \u30dc\u30af\u30bb\u30eb\u5360\u6709\u78ba\u7387\u3068\u3001\u305d\u308c\u3092\u4e8c\u5024\u5316\u3059\u308b\u305f\u3081\u306e\u95be\u5024\u3092\u5165\u529b Mesh Refinement Branch \u76ee\u7684\u306f\uff0c \u7c97\u3044\u5f62\u72b6(voxel\u5316)\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3053\u3068 \u51e6\u7406\u306e\u6d41\u308c\u306f\u5927\u304d\u304f3\u3064\u3067\u69cb\u6210(vertex alignment, graph convlution, vertex refinement) \u3053\u306e3\u3064\u3092\u7e70\u308a\u8fd4\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u9802\u70b9\u304c\u66f4\u65b0\u3055\u308c\u3066\u3044\u304f \u306a\u304a\uff0c\u5404\u30ec\u30a4\u30e4\u30fc\u3067\u306f \u5404\u30e1\u30c3\u30b7\u30e5\u9802\u70b9\u306e3D\u4f4d\u7f6evi\u3068\u7279\u5fb4\u30d9\u30af\u30c8\u30ebfi \u3092\u6301\u3064 vertex alignment \u5404\u30e1\u30c3\u30b7\u30e5\u9802\u70b9\u306b\u5bfe\u3057\u3066\u753b\u50cf\u6574\u5217\u3055\u308c\u305f\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 \u30ab\u30e1\u30e9\u306e\u56fa\u6709\u884c\u5217\u3092\u4f7f\u7528\u3057\u3066\uff0c\u5404\u9802\u70b9\u3092\u753b\u50cf\u5e73\u9762\u306b\u6295\u5f71 \u7279\u5fb4\u30de\u30c3\u30d7\u304c\u4e0e\u3048\u3089\u308c\u308b\u3068\u3001\u6295\u5f71\u3055\u308c\u305f\u9802\u70b9\u306e\u4f4d\u7f6e\u3092\u793a\u3059\u53cc\u7dda\u5f62\u88dc\u9593\u3055\u308c\u305f\u753b\u50cf\u7279\u5fb4\u91cf\u3092\u8a08\u7b97 graph convlution \u30e1\u30c3\u30b7\u30e5\u306e\u8fba\u306b\u6cbf\u3063\u3066\u60c5\u5831\u3092\u4f1d\u64ad GCN\u306b\u3088\u308a \uff0c\u5c40\u6240\u7684\u306a\u30e1\u30c3\u30b7\u30e5\u9818\u57df\u306e\u60c5\u5831\u3092\u96c6\u7d04 vertex refinement \u30e1\u30c3\u30b7\u30e5\u5f62\u72b6\u304c\u66f4\u65b0\u3055\u308c\u3001\u30c8\u30dd\u30ed\u30b8\u30fc\u304c\u56fa\u5b9a Loss \u306b\u3064\u3044\u3066\u306f\u5f8c\u8ff0(Lv3\u306b\u3066) Lv3 Loss \u306a\u3069\u306e\u8a2d\u8a08 \u63d0\u6848\u30b7\u30b9\u30c6\u30e0\u306f\u3069\u306e\u3088\u3046\u306b\u6700\u9069\u5316\u3055\u308c\u308b\u306e\u304b\u3092\u8aac\u660e\u3059\u308b Voxel Loss Voxel Branch\u306f\u3001\u4e88\u6e2c\u3055\u308c\u305f\u30dc\u30af\u30bb\u30eb\u5360\u6709\u78ba\u7387\u3068\u771f\u306e\u30dc\u30af\u30bb\u30eb\u5360\u6709\u7387\u3068\u306e\u9593\u306e2\u5024\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u6700\u5c0f\u5316\u3059\u308b\u3088\u3046\u306b\u8a13\u7df4 Mesh Loss \u6709\u9650\u306e\u70b9\u96c6\u5408\u4e0a\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u640d\u5931\u95a2\u6570\u3092\u4f7f\u7528 point cloud \u306b\u3088\u308b\u30e1\u30c3\u30b7\u30e5\u3092\u3001\u305d\u306e\u8868\u9762\u3092\u5bc6\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u8868\u73fe \u2192 \u70b9\u7fa4\u306e\u640d\u5931\u3092\u5f62\u72b6\u306e\u640d\u5931\u306b\u8fd1\u4f3c \u5927\u304d\u304f\uff13\u7a2e\u985e\u306eLoss\u3092\u5b9a\u7fa9 Chamfer Loss point cloud P, Q \u304c\u4e0e\u3048\u3089\u308c\u305f\u6642\uff0c\u4ee5\u4e0b\u306e\u5f0f\u3067\u5b9a\u7fa9 \u4e00\u756a\u8fd1\u3044\u9802\u70b9\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97 Normal Loss \u6cd5\u7ddaup, uq \u306e\u5185\u7a4d\u3092\u8a08\u7b97 \u9802\u70b9q \u306e\u63a5\u5e73\u9762\u72b6\u306b \u9802\u70b9p \u3068\u96a3\u63a5\u9802\u70b9\u304c\u5b58\u5728\u3059\u308b\u3068\u5c0f\u3055\u304f\u306a\u308b Edgde Loss \u3042\u308b\u9802\u70b9\u3068\u96a3\u63a5\u9802\u70b9\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3057\u3066\u6b63\u898f\u5316\u3059\u308b (\u88dc\u8db3) Edgde Loss \u306e \u6709\u7121\u306b\u3088\u308b\u5f62\u72b6\u306e\u6bd4\u8f03\u3092\u4ee5\u4e0b\u306b\u793a\u3059\uff0e\u306a\u3057\u306e\u65b9\u304c\u304d\u308c\u3044\u306b\u751f\u6210\u3067\u304d\u3066\u3044\u308b\u304c\uff0c\u4e0d\u898f\u5247\u306a\u30b5\u30a4\u30ba\u306e\u9762\u3068\u591a\u304f\u306e\u81ea\u5df1\u4ea4\u5dee\u3092\u6301\u3064\u306a\u3069\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u3068\u3063\u3066\u5b9f\u7528\u7684\u3067\u306a\u3044\u3053\u3068\u304c\u61f8\u5ff5\u3055\u308c\u308b mesh refinement branch \u3067\u306f\u4e0a\u8a18\u306e3\u3064\u306e\u640d\u5931\u306e\u52a0\u91cd\u548c\u3092\u6700\u9069\u5316\u3059\u308b \u5b9f\u9a13 \u5b66\u7fd2\u306f\u753b\u50cf\u3068mesh \u306e\u30da\u30a2\u3092\u7528\u3044\u305f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2 baseline\u624b\u6cd5 N3MR ... 3D \u306e\u6559\u5e2b\u306a\u3057\u3067\u5fae\u5206\u53ef\u80fd\u306a\u30ec\u30f3\u30c0\u30e9\u30fc\u3092\u4ecb\u3057\u3066mesh\u3092\u5b66\u7fd2\u3059\u308b\u5f31\u6559\u5e2b\u3042\u308a\u5b66\u7fd2 3D-R2N2 ... \u30dc\u30af\u30bb\u30eb\u3092\u63a8\u5b9a MVD ... \u30dc\u30af\u30bb\u30eb\u3092\u63a8\u5b9a PSG ... point cloud \u3092\u63a8\u5b9a Pixel2Mesh ... \u521d\u671f\u6955\u5186\u4f53\u3092\u5909\u5f62\u30fb\u7d30\u5206\u5316\u3057\u3066\u30e1\u30c3\u30b7\u30e5\u3092\u63a8\u5b9a GEOMetrics\u3000... Pixel2Mesh \u3092 \u9069\u5fdc\u7684\u306a\u9762\u7d30\u5206\u5316\u3067\u62e1\u5f35\u3057\u305f\u30e2\u30c7\u30eb \u306a\u304a\uff0cboxel_only \u306a\u624b\u6cd5\u306f \u63d0\u6848\u624b\u6cd5\u306b\u304a\u3051\u308bMesh Refinement Branch \u3092\u9664\u3044\u305f\u3082\u306e mesh_only \u306a\u624b\u6cd5\u306f Voxel Branch \u3092\u9664\u3044\u305f\u3082\u306e\u3068\u89e3\u91c8\u3057\u3066\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b dataset(2 patterns) ShapeNet ... mesh\u4e88\u6e2c\u306b\u304a\u3051\u308b\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306edataset Pix3D dataset ... \u5b9f\u753b\u50cf\u3082\u542b\u3093\u3060\u3088\u308a\u96e3\u3057\u3044dataset \u8a55\u4fa1\u624b\u6cd5 \u4e88\u6e2c\u30e1\u30c3\u30b7\u30e5\u3068\u6b63\u89e3\u30e1\u30c3\u30b7\u30e5\u306e\u8868\u9762\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u3067\u4e00\u69d8\u306b\uff11\u4e07\u70b9\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b \u2192 \u305d\u308c\u3089\u306e\u70b9\u7fa4\u306b\u304a\u3051\u308b chamfer distance(Lower is better), normal consistency, \u304a\u3088\u3073\u69d8\u3005\u306a\u8ddd\u96e2\u95be\u5024\u3067\u306eF1-score \u3092\u7b97\u51fa (precision .. GT\u5185\u306e\u4e88\u6e2c\u70b9\u306e\u5272\u5408, recall .. \u4e88\u6e2c\u5185\u306eGT\u306e\u5272\u5408) \u305f\u3060\u3057\uff0c\u3053\u308c\u3089\u306f\u7269\u4f53\u306e\u7d76\u5927\u7684\u306a\u5927\u304d\u3055\u306b\u4f9d\u5b58\u3057\u3066\u3057\u307e\u3046\u305f\u3081\uff0crescale\u3057\u305f\u4e0a\u3067\u8a55\u4fa1\u3057\u3066\u3044\u308b shapenet \u306e\u8a55\u4fa1\u7d50\u679c pix3D dataset \u306e\u8a55\u4fa1\u7d50\u679c","title":"Mesh RCNN\u306e\u304a\u6c17\u6301\u3061\u3092\u7406\u89e3\u3057\u305f\u3044"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#mesh-rcnn","text":"","title":"Mesh RCNN\u306e\u304a\u6c17\u6301\u3061\u3092\u7406\u89e3\u3057\u305f\u3044"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_1","text":"Mesh R CNN \u306b\u3064\u3044\u3066\u7406\u89e3\u3059\u308b Lv1. \u304a\u6c17\u6301\u3061\u306e\u7406\u89e3 Lv2. \u3069\u3093\u306aNetwork\u69cb\u9020 Lv3. loss \u306a\u3069\u8a73\u7d30\u306a\u8a2d\u8a08","title":"\u76ee\u7684"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_2","text":"ICCV 2019 \u5143\u8ad6\u6587\u306f\u3053\u3061\u3089\u304b\u3089 - github \u306f\u3053\u3061\u3089 https://github.com/facebookresearch/meshrcnn","title":"\u524d\u63d0"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_3","text":"2D image \u304b\u3089 mesh(3D \u5f62\u72b6) \u3092 \u7372\u5f97\u3059\u308b\u624b\u6cd5(\u4e0b\u56f3) \u5165\u529b\u304b\u3089\u51fa\u529b\u307e\u3067End-to-End\u5b66\u7fd2","title":"\u6982\u8981"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_4","text":"3D\u95a2\u9023\u306e\u7814\u7a76\u304c\u8fd1\u5e74\u76db\u3093 \u69d8\u3005\u306a\u6280\u8853\u304c\u5b58\u5728\u3059\u308b\u304b\u95a2\u9023\u3059\u308b\u4ee3\u8868\u7684\u306a\u3082\u306e\u3092\u7c21\u5358\u306b\u7d39\u4ecb\u3059\u308b","title":"\u80cc\u666f(\u95a2\u9023\u7814\u7a76)"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#2d-object-recognition","text":"\u5358\u4e00\u753b\u50cf\u3092\u5165\u529b\u3057\u3066\u7279\u5b9a\u306e\u7269\u4f53\u3092\u691c\u51fa\u3059\u308b\uff0eBbox\u306b\u3088\u308b\u77e9\u5f62\u306e\u62bd\u51fa\u3068\u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb\u3092\u51fa\u529b\u3059\u308b\u3082\u306e\u304c\u4e00\u822c\u7684 mask R-CNN \u3067\u306f\u3053\u308c\u306b\u52a0\u3048\u3066\u7269\u4f53\u306e\u9818\u57df\u3092\u8868\u3059\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u30de\u30b9\u30af\u3092\u62bd\u51fa\u3059\u308b","title":"2D  Object  Recognition"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#single-view-shape-prediction","text":"\u5358\u4e00\u753b\u50cf\u30923\u6b21\u5143\u518d\u69cb\u6210\u3059\u308b\uff0e3D\u306epose \u3084\u65e2\u77e5\u306e\u5f62\u72b6\u306e\u5411\u304d\u3092\u63a8\u5b9a\u3059\u308b\u3082\u306e\u304c\u4e00\u822c\u7684","title":"Single-View  Shape  Prediction"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#multi-view-shape-prediction","text":"\u8907\u6570\u30ab\u30e1\u30e9\u306b\u3088\u308b\u753b\u50cf\u304b\u3089\uff13\u6b21\u5143\u518d\u69cb\u6210\u3092\u884c\u3046\uff0emesh R-CNN \u3067\u306f\u6271\u308f\u306a\u3044\u304c\uff0c\u3053\u306e\u5206\u91ce\u306e\u7814\u7a76\u306f\u8fd1\u5e74\u76db\u3093","title":"Multi-View  Shape  Prediction"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#3d-input","text":"\uff13\u6b21\u5143\u753b\u50cf\u3092\u5165\u529b\u3057\u3066semantic label \u306a\u3069\u3092\u51fa\u529b\u3059\u308b\u30bf\u30b9\u30af RGB-D images\u3084point cloud \u306a\u3069\u3092\u5165\u529b\u3059\u308b\u306e\u304c\u4e00\u822c\u7684","title":"3D input"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#3d-dataset","text":"imagenet \u3084 COCO dataset\u306b\u3088\u308a 2D\u5206\u91ce\u306f\u5927\u5e45\u306a\u767a\u5c55\u3092\u9042\u3052\u305f 3D dataset \u306f 2D \u306b\u6bd4\u3079\u3066\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u304c\u56f0\u96e3\u3067\u3042\u308b\u554f\u984c\u304b\u3089\u3044\u307e\u3060\u767a\u5c55\u9014\u4e0a\u306e\u9818\u57df\u3067\u3042\u308b ShapeNet, Pix3D, Pascal3D \u306a\u3069\u69d8\u3005\u306a\u3082\u306e\u304c\u767b\u5834\u3057\u3066\u3044\u308b\u304c\u5408\u6210\u30c7\u30fc\u30bf\u3067\u3042\u308b\u3082\u306e\u3084\u77e9\u5f62\u306e\u307f\u3067\u5f62\u72b6\u306e\u6ce8\u91c8\u304c\u306a\u3044\u306a\u3069\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u7279\u6709\u306e\u554f\u984c\u306a\u3069\u3082\u591a\u3044","title":"3D dataset"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_5","text":"","title":"\u624b\u6cd5"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#lv1","text":"\u5927\u307e\u304b\u306a\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u306f\u4ee5\u4e0b\u56f3\u306b\u793a\u3059\u3088\u3046\u306b\uff0c \u7269\u4f53\u306e\u691c\u51fa \u2192 \u7269\u4f53\u306eboxel\u5316 \u2192 mesh\u306b\u5909\u63db \u2192 mesh\u3092\u6539\u826f(refinement) \u306estep \u3067\u69cb\u6210\u3055\u308c\u308b\u3068\u601d\u3063\u3066\u826f\u3044\uff0e","title":"Lv1.\u304a\u6c17\u6301\u3061\u306e\u7406\u89e3"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#lv2-network","text":"\u4e0b\u56f3\u306b\u63d0\u6848\u30b7\u30b9\u30c6\u30e0\u306e\u8a73\u7d30\u306a\u6d41\u308c\u3092\u793a\u3059\uff0e \u5927\u307e\u304b\u306a\u6d41\u308c\u306fLv1 \u3067\u8aac\u660e\u3057\u305f\u304c\u3082\u3046\u5c11\u3057\u8a73\u7d30\u306b\u8ff0\u3079\u308b\uff0e \u307e\u305a\uff0c\u7269\u4f53\u691c\u51fa\u90e8\u5206\u3067\u306f\u5f93\u6765\u306eSoTA model \u3067\u3042\u308b mask R-CNN \u3092\u7528\u3044\u3066\u3044\u308b \u2192 \u30ab\u30c6\u30b4\u30ea\u30e9\u30d9\u30eb, bbox, segmentation mask \u3092\u51fa\u529b backbone \u306f resnet50(pre-trained by imagenet) 3D \u5f62\u72b6\u3092\u4e88\u6e2c\u3059\u308b\u305f\u3081\u306b\uff0c Voxel Branch \u3068 Mesh Refinement Branch \u3092\u7528\u3044\u308b \u540d\u524d\u306e\u901a\u308a\u3067\u306f\u3042\u308b\u304c\uff0c - Boxel Branch .. object \u306e\u5927\u307e\u304b\u306a 3\u6b21\u5143\u30dc\u30af\u30bb\u30eb\u5316\u3092\u63a8\u5b9a\u3057\uff0c\u521d\u671f\u4e09\u89d2mesh \u306b\u5909\u63db - Mesh Refinement Branch .. \u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f\u5c64\u3092\u4f7f\u7528\u3057\u3066\u3001\u3053\u306e\u521d\u671f\u30e1\u30c3\u30b7\u30e5\u306e\u9802\u70b9\u4f4d\u7f6e\u3092\u8abf\u6574(mesh\u3092\u6574\u3048\u308b) \u3068\u8a00\u3063\u305f\u5f79\u5272\u304c\u3042\u308b\uff0e Voxel Branch \u5404object \u306e\u5f62\u72b6\u3092\u30dc\u30c8\u30e0\u30a2\u30c3\u30d7\u3067\u4e88\u6e2c \u6c17\u6301\u3061\u3068\u3057\u3066\u306f\uff0cmask R-CNN \u306e mask branch \u3068\u4f3c\u305f\u3088\u3046\u306a\u6a5f\u80fd 2D \u5e73\u9762\u306e M * M \u2192 3D \u306a G * G * G \u306e\u5f62\u72b6\u3092\u4e88\u6e2c Loss \u306b\u3064\u3044\u3066\u306f\u5f8c\u8ff0(Lv3 \u306b\u3066) (\u88dc\u8db3) \u753b\u50cf\u3068\u4e88\u6e2c\u5024\u3068\u306e\u30d4\u30af\u30bb\u30eb\u5358\u4f4d\u306e\u5bfe\u5fdc\u95a2\u4fc2\u3092\u7dad\u6301\u3059\u308b\u3053\u3068\u306f\u30013D\u3067\u306f\u30ab\u30e1\u30e9\u304b\u3089\u96e2\u308c\u308b\u306b\u3064\u308c\u3066\u7269\u4f53\u304c\u5c0f\u3055\u304f\u306a\u308b\u305f\u3081\u8907\u96d1\u3067\u3042\u308b\uff0e\u305d\u308c\u3092\u56de\u907f\u3059\u308b\u305f\u3081\u306b\u30ab\u30e1\u30e9\u7279\u6709\u306e\u56fa\u6709\u884c\u5217\u3092\u5229\u7528\u3057\u3066\u7406\u60f3\u7684\u306a\u5f62\u72b6\u306b\u3059\u308b\u5de5\u592b\u3092\u884c\u306a\u3063\u3066\u3044\u308b Cubify(\u3053\u3053\u304c\u809d\u3063\u307d\u3044) Voxel to Mesh \u76ee\u7684\u306f\uff0c \u30dc\u30af\u30bb\u30eb\u4e88\u6e2c\u5024\u3092\u4e09\u89d2\u5f62\u306e\u30e1\u30c3\u30b7\u30e5\u306b\u5909\u63db\u3059\u308b","title":"Lv2 Network\u306a\u3069\u3082\u542b\u3081\u3066\u89e3\u8aac"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_6","text":"Mesh Refinement Branch \u76ee\u7684\u306f\uff0c \u7c97\u3044\u5f62\u72b6(voxel\u5316)\u3092\u6ed1\u3089\u304b\u306b\u3059\u308b\u3053\u3068 \u51e6\u7406\u306e\u6d41\u308c\u306f\u5927\u304d\u304f3\u3064\u3067\u69cb\u6210(vertex alignment, graph convlution, vertex refinement) \u3053\u306e3\u3064\u3092\u7e70\u308a\u8fd4\u3057\u3066\u3044\u304f\u3053\u3068\u3067\u9802\u70b9\u304c\u66f4\u65b0\u3055\u308c\u3066\u3044\u304f \u306a\u304a\uff0c\u5404\u30ec\u30a4\u30e4\u30fc\u3067\u306f \u5404\u30e1\u30c3\u30b7\u30e5\u9802\u70b9\u306e3D\u4f4d\u7f6evi\u3068\u7279\u5fb4\u30d9\u30af\u30c8\u30ebfi \u3092\u6301\u3064 vertex alignment \u5404\u30e1\u30c3\u30b7\u30e5\u9802\u70b9\u306b\u5bfe\u3057\u3066\u753b\u50cf\u6574\u5217\u3055\u308c\u305f\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3092\u751f\u6210 \u30ab\u30e1\u30e9\u306e\u56fa\u6709\u884c\u5217\u3092\u4f7f\u7528\u3057\u3066\uff0c\u5404\u9802\u70b9\u3092\u753b\u50cf\u5e73\u9762\u306b\u6295\u5f71 \u7279\u5fb4\u30de\u30c3\u30d7\u304c\u4e0e\u3048\u3089\u308c\u308b\u3068\u3001\u6295\u5f71\u3055\u308c\u305f\u9802\u70b9\u306e\u4f4d\u7f6e\u3092\u793a\u3059\u53cc\u7dda\u5f62\u88dc\u9593\u3055\u308c\u305f\u753b\u50cf\u7279\u5fb4\u91cf\u3092\u8a08\u7b97 graph convlution \u30e1\u30c3\u30b7\u30e5\u306e\u8fba\u306b\u6cbf\u3063\u3066\u60c5\u5831\u3092\u4f1d\u64ad GCN\u306b\u3088\u308a \uff0c\u5c40\u6240\u7684\u306a\u30e1\u30c3\u30b7\u30e5\u9818\u57df\u306e\u60c5\u5831\u3092\u96c6\u7d04 vertex refinement \u30e1\u30c3\u30b7\u30e5\u5f62\u72b6\u304c\u66f4\u65b0\u3055\u308c\u3001\u30c8\u30dd\u30ed\u30b8\u30fc\u304c\u56fa\u5b9a Loss \u306b\u3064\u3044\u3066\u306f\u5f8c\u8ff0(Lv3\u306b\u3066)","title":"\u30dc\u30af\u30bb\u30eb\u5360\u6709\u78ba\u7387\u3068\u3001\u305d\u308c\u3092\u4e8c\u5024\u5316\u3059\u308b\u305f\u3081\u306e\u95be\u5024\u3092\u5165\u529b"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#lv3-loss","text":"\u63d0\u6848\u30b7\u30b9\u30c6\u30e0\u306f\u3069\u306e\u3088\u3046\u306b\u6700\u9069\u5316\u3055\u308c\u308b\u306e\u304b\u3092\u8aac\u660e\u3059\u308b Voxel Loss Voxel Branch\u306f\u3001\u4e88\u6e2c\u3055\u308c\u305f\u30dc\u30af\u30bb\u30eb\u5360\u6709\u78ba\u7387\u3068\u771f\u306e\u30dc\u30af\u30bb\u30eb\u5360\u6709\u7387\u3068\u306e\u9593\u306e2\u5024\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3092\u6700\u5c0f\u5316\u3059\u308b\u3088\u3046\u306b\u8a13\u7df4 Mesh Loss \u6709\u9650\u306e\u70b9\u96c6\u5408\u4e0a\u3067\u5b9a\u7fa9\u3055\u308c\u305f\u640d\u5931\u95a2\u6570\u3092\u4f7f\u7528 point cloud \u306b\u3088\u308b\u30e1\u30c3\u30b7\u30e5\u3092\u3001\u305d\u306e\u8868\u9762\u3092\u5bc6\u306b\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b\u3053\u3068\u3067\u8868\u73fe \u2192 \u70b9\u7fa4\u306e\u640d\u5931\u3092\u5f62\u72b6\u306e\u640d\u5931\u306b\u8fd1\u4f3c \u5927\u304d\u304f\uff13\u7a2e\u985e\u306eLoss\u3092\u5b9a\u7fa9 Chamfer Loss point cloud P, Q \u304c\u4e0e\u3048\u3089\u308c\u305f\u6642\uff0c\u4ee5\u4e0b\u306e\u5f0f\u3067\u5b9a\u7fa9 \u4e00\u756a\u8fd1\u3044\u9802\u70b9\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97 Normal Loss \u6cd5\u7ddaup, uq \u306e\u5185\u7a4d\u3092\u8a08\u7b97 \u9802\u70b9q \u306e\u63a5\u5e73\u9762\u72b6\u306b \u9802\u70b9p \u3068\u96a3\u63a5\u9802\u70b9\u304c\u5b58\u5728\u3059\u308b\u3068\u5c0f\u3055\u304f\u306a\u308b Edgde Loss \u3042\u308b\u9802\u70b9\u3068\u96a3\u63a5\u9802\u70b9\u3068\u306e\u8ddd\u96e2\u3092\u8a08\u7b97\u3057\u3066\u6b63\u898f\u5316\u3059\u308b (\u88dc\u8db3) Edgde Loss \u306e \u6709\u7121\u306b\u3088\u308b\u5f62\u72b6\u306e\u6bd4\u8f03\u3092\u4ee5\u4e0b\u306b\u793a\u3059\uff0e\u306a\u3057\u306e\u65b9\u304c\u304d\u308c\u3044\u306b\u751f\u6210\u3067\u304d\u3066\u3044\u308b\u304c\uff0c\u4e0d\u898f\u5247\u306a\u30b5\u30a4\u30ba\u306e\u9762\u3068\u591a\u304f\u306e\u81ea\u5df1\u4ea4\u5dee\u3092\u6301\u3064\u306a\u3069\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u306b\u3068\u3063\u3066\u5b9f\u7528\u7684\u3067\u306a\u3044\u3053\u3068\u304c\u61f8\u5ff5\u3055\u308c\u308b mesh refinement branch \u3067\u306f\u4e0a\u8a18\u306e3\u3064\u306e\u640d\u5931\u306e\u52a0\u91cd\u548c\u3092\u6700\u9069\u5316\u3059\u308b","title":"Lv3 Loss \u306a\u3069\u306e\u8a2d\u8a08"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_7","text":"\u5b66\u7fd2\u306f\u753b\u50cf\u3068mesh \u306e\u30da\u30a2\u3092\u7528\u3044\u305f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2","title":"\u5b9f\u9a13"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#baseline","text":"N3MR ... 3D \u306e\u6559\u5e2b\u306a\u3057\u3067\u5fae\u5206\u53ef\u80fd\u306a\u30ec\u30f3\u30c0\u30e9\u30fc\u3092\u4ecb\u3057\u3066mesh\u3092\u5b66\u7fd2\u3059\u308b\u5f31\u6559\u5e2b\u3042\u308a\u5b66\u7fd2 3D-R2N2 ... \u30dc\u30af\u30bb\u30eb\u3092\u63a8\u5b9a MVD ... \u30dc\u30af\u30bb\u30eb\u3092\u63a8\u5b9a PSG ... point cloud \u3092\u63a8\u5b9a Pixel2Mesh ... \u521d\u671f\u6955\u5186\u4f53\u3092\u5909\u5f62\u30fb\u7d30\u5206\u5316\u3057\u3066\u30e1\u30c3\u30b7\u30e5\u3092\u63a8\u5b9a GEOMetrics\u3000... Pixel2Mesh \u3092 \u9069\u5fdc\u7684\u306a\u9762\u7d30\u5206\u5316\u3067\u62e1\u5f35\u3057\u305f\u30e2\u30c7\u30eb \u306a\u304a\uff0cboxel_only \u306a\u624b\u6cd5\u306f \u63d0\u6848\u624b\u6cd5\u306b\u304a\u3051\u308bMesh Refinement Branch \u3092\u9664\u3044\u305f\u3082\u306e mesh_only \u306a\u624b\u6cd5\u306f Voxel Branch \u3092\u9664\u3044\u305f\u3082\u306e\u3068\u89e3\u91c8\u3057\u3066\u6bd4\u8f03\u3059\u308b\u3053\u3068\u304c\u53ef\u80fd\u3067\u3042\u308b","title":"baseline\u624b\u6cd5"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#dataset2-patterns","text":"ShapeNet ... mesh\u4e88\u6e2c\u306b\u304a\u3051\u308b\u30d9\u30f3\u30c1\u30de\u30fc\u30af\u306edataset Pix3D dataset ... \u5b9f\u753b\u50cf\u3082\u542b\u3093\u3060\u3088\u308a\u96e3\u3057\u3044dataset","title":"dataset(2 patterns)"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#_8","text":"\u4e88\u6e2c\u30e1\u30c3\u30b7\u30e5\u3068\u6b63\u89e3\u30e1\u30c3\u30b7\u30e5\u306e\u8868\u9762\u304b\u3089\u30e9\u30f3\u30c0\u30e0\u3067\u4e00\u69d8\u306b\uff11\u4e07\u70b9\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3059\u308b \u2192 \u305d\u308c\u3089\u306e\u70b9\u7fa4\u306b\u304a\u3051\u308b chamfer distance(Lower is better), normal consistency, \u304a\u3088\u3073\u69d8\u3005\u306a\u8ddd\u96e2\u95be\u5024\u3067\u306eF1-score \u3092\u7b97\u51fa (precision .. GT\u5185\u306e\u4e88\u6e2c\u70b9\u306e\u5272\u5408, recall .. \u4e88\u6e2c\u5185\u306eGT\u306e\u5272\u5408) \u305f\u3060\u3057\uff0c\u3053\u308c\u3089\u306f\u7269\u4f53\u306e\u7d76\u5927\u7684\u306a\u5927\u304d\u3055\u306b\u4f9d\u5b58\u3057\u3066\u3057\u307e\u3046\u305f\u3081\uff0crescale\u3057\u305f\u4e0a\u3067\u8a55\u4fa1\u3057\u3066\u3044\u308b","title":"\u8a55\u4fa1\u624b\u6cd5"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#shapenet","text":"","title":"shapenet \u306e\u8a55\u4fa1\u7d50\u679c"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/mesh_rcnn/#pix3d-dataset","text":"","title":"pix3D dataset \u306e\u8a55\u4fa1\u7d50\u679c"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/","text":"\u53c2\u8003\u6587\u732e https://arxiv.org/abs/1803.08225 https://arxiv.org/pdf/1803.08225.pdf https://arxiv.org/abs/1701.01779 (\u5099\u5fd8\u9332)PoseNet\u3092\u4f7f\u3063\u305f\u59ff\u52e2\u63a8\u5b9a in Tensorflow \u6982\u8981 Posenet : https://github.com/tensorflow/tfjs-models/tree/master/posenet \u3092\u6271\u3046 \u3053\u306e\u3088\u3046\u306a\u59ff\u52e2\u63a8\u5b9a(\u9aa8\u683c\u63a8\u5b9a)\u306f\u3001input\u3055\u308c\u305f\u753b\u50cf\u306e \u3069\u3053\u5ea7\u6a19\u304c\u3069\u306e\u70b9(\u8098\u3068\u304b\u624b\u9996\u3068\u304b)\u306b\u3042\u305f\u308b\u304b\u3092\u63a8\u5b9a\u3057\u3066\u3044\u308b\u3002 Posenet\u3067\u306f\u4ee5\u4e0b\u306e17\u70b9\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u3053\u308c\u3089\u306e\u70b9\u3092Keypoint\u3068\u8a00\u3046\u3002 0 nose 1 leftEye 2 rightEye 3 leftEar 4 rightEar 5 leftShoulder 6 rightShoulder 7 leftElbow 8 rightElbow 9 leftWrist 10 rightWrist 11 leftHip 12 rightHip 13 leftKnee 14 rightKnee 15 leftAnkle 16 rightAnkle Keypoint \u3092\u898b\u3064\u3051\u51fa\u3059\u4ed5\u7d44\u307f \u53c2\u8003(MobileNet\u7248Posenet\u306b\u3064\u3044\u3066) https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 \u5b66\u7fd2\u6e08\u307f\u306eMobileNet\u306f\u3001\u753b\u50cf\u3092\u5165\u529b\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u753b\u50cf\u306e\u3088\u3046\u306a keypoint heatmap \u3084 offset vectors \u7b49\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3059\u308b\u3002 \u753b\u50cf\u4e2d keypoint heatmap , offset vectors \u304c\u683c\u5b50\u72b6\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f\u3001\u4e00\u9023\u306e\u51e6\u7406\u3092\u9ad8\u901f\u5316\u3055\u305b\u308b\u305f\u3081\u306e outputStride (\u30e1\u30c3\u30b7\u30e5\u5e45\u307f\u305f\u3044\u306a\u3082\u306e)\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u304b\u3089\u3002 outputStride \u306e\u6570\u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u30e1\u30c3\u30b7\u30e5\u304c\u5927\u304d\u304f\u5207\u3089\u308c\u3001\u51e6\u7406\u901f\u5ea6\u306f\u5411\u4e0a\u3059\u308b\u304c\u7cbe\u5ea6\u306f\u843d\u3061\u308b\u3002 \u305d\u3057\u3066 keypoint heatmap \u3092\u30b9\u30b3\u30a2\u5316\u3059\u308b\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u6d3b\u6027\u5316\u3057\u305f\u3082\u306e\u2192 heatmapPositions \u3092\u7528\u610f\u3059\u308b\u3002 Keypoint\u306f\u4ee5\u4e0a\u306e\u4e09\u8981\u7d20 ( heatmapPositions , offset vectors , outputStride ) \u3067\u8a08\u7b97\u3055\u308c\u3066\u3044\u308b\u3002 keypointPositions = heatmapPositions * outputStride + offsetVectors offset vectors \u306b\u3064\u3044\u3066 \u3053\u308c\u304cPosenet\u306e\u767a\u660e\u54c1\u306e\u3088\u3046\u3060 Long-range offsets , Short-range offsets , Mid-range offsets \u306e3\u7a2e\u985e\u3042\u308b\u304c\u3001\u3053\u306e\u4e2d\u3067\u3044\u3061\u3070\u3093\u91cd\u8981\u306a\u3082\u306e\u306fShort-range offsets\u3067\u3042\u308b\u3002 offset vectors\u306f\u57fa\u672c\u7684\u306bShort-range offsets\u3057\u305f\u30d9\u30af\u30c8\u30eb\u306e\u30c7\u30fc\u30bf\u306e\u96c6\u307e\u308a\u306e\u3053\u3068\u3092\u8a00\u3063\u3066\u3044\u308b\u3002 Long-range offsets\u306f\u59ff\u52e2\u63a8\u5b9a\u3067\u306f\u4f7f\u308f\u306a\u3044\u3002 Long-range offsets \u4ee5\u4e0b\u306f\u4f8b\u3068\u3057\u3066\u9f3b\u306b\u304b\u3051\u3066\u306eLong-range offsets (vector)\u3092\u8868\u3057\u305f\u3082\u306e\u3060\u3002 https://arxiv.org/pdf/1803.08225.pdf \u3088\u308a \u3061\u306a\u307f\u306bLong-range offsets\u306f\u59ff\u52e2\u63a8\u5b9a\u3067\u306f\u4f7f\u3063\u3066\u304a\u3089\u305a(\u2190\u30bb\u30b0\u30e1\u30f3\u30c8\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b)\u3001\u59ff\u52e2\u63a8\u5b9a\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u306f\u4ee5\u4e0b\u306e\u5c40\u6240\u7684\u306a\u9818\u57df\u5185\u3067\u306e\u30d9\u30af\u30c8\u30eb\u3092\u793a\u3057\u305fShort-range offsets\u3068Mid-range offsets\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002 Short-range offsets \u306b\u3064\u3044\u3066 Short-range offsets\u306f\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u306e\u5ea7\u6a19\u3092\u4e00\u610f\u7684\u306b\u6c7a\u3081\u308b\u5404\u683c\u5b50\u70b9\u3092\u59cb\u70b9\u3068\u3059\u308b\u30d9\u30af\u30c8\u30eb\u60c5\u5831\u3067\u3042\u308b\u3002(outputStride\u306b\u3088\u3063\u3066\u5b9a\u3081\u3089\u308c\u305f\u683c\u5b50\u70b9\u304c10x20\u3042\u308b\u5834\u5408\u30011\u500b\u306eKeypoint\u306b\u5bfe\u3059\u308bShort-range offsets\u306e\u30b5\u30a4\u30ba\u306f\u3001\u5404\u70b9\u3054\u3068\u306ex,y\u65b9\u5411\u306e\u30d9\u30af\u30c8\u30eb\u305d\u308c\u305e\u308c\u306e\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u306e\u3067\u300120x10x2\u3068\u306a\u308b) offset vectors \u306f\u516817\u500b\u306eKeypoint\u5206\u306eShort-range offsets\u3092\u3072\u3068\u307e\u3068\u3081\u306b\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002(\u3064\u307e\u308a\u683c\u5b50\u70b9\u304c10x20\u3042\u308b\u5834\u5408\u3001\u30b5\u30a4\u30ba\u306f20x10x34\u3068\u306a\u308b) \u4e0b\u306e\u56f3\u3067\u306f\u3001\u9f3b\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u4ed8\u8fd1\u306eShort-range offsets\u3092\u53c2\u7167\u3057\u9f3b\u306eKeypoint\u3092\u691c\u51fa\u3059\u308b\u69d8\u5b50\u3067\u3042\u308b(\u5de6\u8098\u3082\u540c\u69d8) Mid-range offsets \u306b\u3064\u3044\u3066 Mid-range offsets \u306fKeypoint\u540c\u58eb\u306e\u4f4d\u7f6e, \u63a5\u7d9a\u95a2\u4fc2\u3092\u683c\u7d0d\u3057\u3066\u3044\u308b\u3002 \u30e2\u30c7\u30eb\u69cb\u9020(MobileNet\u5229\u7528) \u73fe\u5728\u3001\u7cbe\u5ea6\u304c\u9ad8\u304f\u901f\u5ea6\u306f\u9045\u3044ResNet\u7248\u3082\u3042\u308b\u304c\u3001MobileNet\u7248\u306b\u7d5e\u308b \u2193\u306f\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066 https://qiita.com/otmb/items/561a62d3413295cc744e \u672c\u30da\u30fc\u30b8\u3067\u89e6\u308c\u3066\u3044\u306a\u3044\u304c\u3001outputStride\u309216\u3068\u3057\u3001\u5165\u529b\u753b\u50cf\u30b5\u30a4\u30ba\u304c513\u306a\u306e\u3067\u3001 ((513 - 1)// 16) + 1 = 33 \u3088\u308a\u51fa\u529b\u3055\u308c\u308b\u30c6\u30f3\u30bd\u30eb\u306e\u30b5\u30a4\u30ba\u306f1\u8fba33\u3068\u306a\u308b\u3002 \u8a08\u7b97\u5f0f https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 Resolution = ((InputImageSize - 1) / OutputStride) + 1 \u901a\u5e38\u306e\u7573\u8fbc\u307f\u30921\u56de\u3001\u5f8c depthwise_conv2d relu6 conv2d relu6 \u309213\u56de\u7e70\u308a\u8fd4\u3057\u30014\u3064\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3057\u3001 \u66f4\u306b\u305d\u308c\u305e\u308c\u306b\u7573\u307f\u8fbc\u307f\u30921\u56de\u884c\u3063\u305f\u30014\u3064\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3059\u308b\u3002 \u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u306b\u3064\u3044\u3066 https://github.com/tensorflow/tfjs/issues/1137 https://github.com/tensorflow/tfjs/issues/1137#issuecomment-462450145 multi_person_mobilenet_v1_075_float.tflite \u306f\u5f8c\u8ff0\u3067\u6271\u3046 keypoint heatmap\u3060\u3051\u306f\u524d\u8ff0\u306e\u901a\u308a\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u6d3b\u6027\u5316\u3055\u305b\u308b\u304c\u3001\u4ed6\u306e\u30c6\u30f3\u30bd\u30eb\u306f\u305d\u306e\u307e\u307e\u5229\u7528\u3059\u308b\u3002 \u51fa\u529b\u3055\u308c\u308b\u30c6\u30f3\u30bd\u30eb\u306f keypoint, heatmap, offset vectors, horizontal, vertical directions \u3068\u56f3\u3067\u306f\u66f8\u3044\u3066\u3042\u308b\u304c\u3001 \u30d7\u30ed\u30b0\u30e9\u30e0\u4e2d\u3067\u306f heatmap, offset, displacementFwd, displacementBwd \u3053\u306e\u3088\u3046\u306a\u540d\u524d\u306e\u6271\u3044\u306b\u306a\u3063\u3066\u3044\u308b\u3002 \u3053\u306e(horizontal, vertical)\u3082\u3057\u304f\u306f(displacementFwd, displacementBwd)\u306e\u6b63\u4f53\u306fMid-range offsets\u3067\u3042\u308b\u3002 \u524d\u8ff0\u306e\u901a\u308a\u3001\u3053\u306e\u51fa\u529b\u3055\u308c\u305f4\u30c6\u30f3\u30bd\u30eb\u3092\u4f7f\u3044\u9aa8\u683c\u306e\u5ea7\u6a19\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3057\u304b\u3057\u3001\u6700\u65b0\u306etflite\u30e2\u30c7\u30eb\u306f\u76f4\u63a5Keypoint\u3092\u51fa\u529b\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u3044\u308b...\u3002 \u30e2\u30c7\u30eb\u306e\u89e3\u6790 \u69cb\u9020\u3084\u8a08\u7b97\u65b9\u5f0f\u304c\u30d0\u30fc\u30b8\u30e7\u30f3?\u306b\u3088\u3063\u3066\u5927\u304d\u304f\u5909\u308f\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\u3002 multiplier\u30920.75\u306b\u56fa\u5b9a\u3057\u8abf\u67fb\u3057\u305f\u3002(tflite\u306e\u30e2\u30c7\u30eb\u304c0.75\u306e\u3082\u306e\u3057\u304b\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f) \u30fb\u30aa\u30ea\u30b8\u30ca\u30eb:tensorflow js\u7248(checkpoint) https://github.com/tensorflow/tfjs-models/tree/master/posenet \u30fb\u975e\u516c\u5f0fpython\u5909\u63db\u7248(Protocol Buffer(.pb)) \u5165\u529b\u30b5\u30a4\u30ba\u81ea\u7531 \u51fa\u529b4\u3064(heatmap, offset, mid-offsets(displacement_fwd, displacement_bwd)) https://github.com/rwightman/posenet-python model-mobilenet_v1_075.pb : 5.1MB \u30fbtensorflow lite\u516c\u5f0f\u30da\u30fc\u30b8\u306epose_estimation \u5165\u529b\u30b5\u30a4\u30ba\u56fa\u5b9a257x353 \u51fa\u529b4\u3064(heatmaps, short_offsets, mid_offsets, segments) https://www.tensorflow.org/lite/models/pose_estimation/overview#get_started multi_person_mobilenet_v1_075_float.tflite : 5.0 MB \u30fbgoogle-coral \u7528\u306etensorflow lite\u30e2\u30c7\u30eb \u3053\u308c\u307e\u3067\u306e\u3082\u306e\u3068\u7570\u306a\u308a\u3001\u76f4\u63a5Keypoints\u3092\u51fa\u529b\u3059\u308b\u30b9\u30bf\u30a4\u30eb\u3002 \u5165\u529b\u30b5\u30a4\u30ba\u56fa\u5b9a(\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u8a18\u8f09) \u51fa\u529b4 poses(Keypoints), poses:1(keypoint_scores), poses:2(pose_scores), poses:3(\u7a7a) https://github.com/google-coral/project-posenet posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite : 1.5 MB posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite : 1.7 MB posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite : 2.5 MB \u975e\u516c\u5f0fpython\u5909\u63db\u7248(Protocol Buffer(.pb)) \u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba model-mobilenet_v1_075.pb : 5.1MB (model-mobilenet_v1_101.pb : 13.3MB ) \u51fa\u529b heatmap_2 offset_2 displacement_fwd_2 displacement_bwd_2 \u69cb\u9020 tensorbord\u3092\u4f7f\u3063\u3066\u89e3\u6790\u3092\u884c\u3044\u3001\u4e0a\u753b\u50cf\u4e2d MobilenetV1\u306e\u4e2d\u3092\u8868\u793a\u3057\u753b\u50cf\u5316\u3057\u3088\u3046\u3068\u8a66\u307f\u305f\u304c\u3001\u3042\u307e\u308a\u306b\u3082\u9577\u5927\u306a\u753b\u50cf\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u6dfb\u4ed8\u3057\u306a\u3044\u3002 \u70b9\u691c\u3057\u305f\u3068\u3053\u308d\u3001\u4e0a\u8a18 \u30e2\u30c7\u30eb\u69cb\u9020(MobileNet\u5229\u7528) \u3068\u540c\u3058\u3082\u306e\u306b\u306a\u3063\u3066\u3044\u305f\u3002 tensorflow lite\u516c\u5f0f\u30da\u30fc\u30b8\u306epose_estimation \u8a73\u3057\u3044\u3053\u3068\u304c\u3042\u307e\u308a\u8f09\u3063\u3066\u3044\u306a\u3044\u304c https://www.tensorflow.org/lite/models/pose_estimation/overview#get_started \u3053\u3053\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3002 Android, iOS\u306b\u4f7f\u3046\u98a8\u3060\u3002 \u4eca\u307e\u30672\u3064\u306b\u5206\u304b\u308c\u3066\u3044\u305fmid_offsets \u304c\u4e00\u3064\u306b\u307e\u3068\u307e\u3063\u3066\u51fa\u529b\u3055\u308c\u3001\u65b0\u3057\u304fsegments\u304c\u51fa\u529b\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002 K210\u7528\u306bkmodel\u3078\u5909\u63db Fatal: Layer DepthwiseConv2d is not supported \u3068\u30a8\u30e9\u30fc\u3092\u5f15\u304d\u8d77\u3053\u3057\u3066\u3057\u307e\u3063\u305f\u3002 https://github.com/kendryte/nncase/issues/14#issuecomment-489506085 Check your DepthwiseConv2d with 3x3 kernel and 2x2 stride, there is a hardware limitation that you must use tf.pad([[0,0],[1,1],[1,1],[0,0]]) to pad your input and the use valid padding in your DepthwiseConv2d. 2x2\u30b9\u30c8\u30e9\u30a4\u30c9\u306eDepthwiseConv2d \u306e\u524d\u306b tf.pad([[0,0],[1,1],[1,1],[0,0]])\u3092\u7f6e\u3044\u3066\u6570\u5408\u308f\u305b\u3057\u3066\u3042\u3052\u3001DepthwiseConv2d\u3067\u306epadding\u306fvalid\u3092\u4f7f\u308f\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3088\u3046\u3060s \u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba multi_person_mobilenet_v1_075_float.tflite : 5.0 MB \u51fa\u529b float_heatmaps float_mid_offsets float_segments float_short_offsets \u69cb\u9020 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py \u3092\u4f7f\u7528\u3057\u4e2d\u8eab\u3092\u62dd\u898b\u3057\u305f\u3002visualize.py\u5c0e\u5165\u307e\u3068\u3081\u306f\u5f8c\u8ff0\u3059\u308b\u3002 \u69cb\u9020\u81ea\u4f53\u306f\u4eca\u307e\u3067\u306e\u3082\u306e\u3068\u5927\u304d\u304f\u5909\u308f\u3089\u306a\u3044 multi_person_mobilenet_v1_075_float.tflite.html \u5165\u529b\u3001conv2d\u306e\u5f8c depthwise_conv2d relu6 conv2d relu6 \u309213\u56de\u7e70\u308a\u8fd4\u3057\u3001 displacement_fwd\u3068displacement_bwd\u3092conv2d\u3092\u3057\u305f\u5f8c\u304f\u3063\u3064\u3051\u3066\u308b\u3002(\u3053\u3053\u3060\u3051\u4eca\u307e\u3067\u3068\u7570\u306a\u308b) google-coral \u7528\u306etensorflow lite\u30e2\u30c7\u30eb \u3053\u308c\u307e\u3067\u306e\u3082\u306e\u3068\u7570\u306a\u308a\u3001\u76f4\u63a5Keypoints\u3092\u51fa\u529b\u3059\u308b\u30b9\u30bf\u30a4\u30eb\u3002 \u4e0a\u306epose_estimation\u3068\u6bd4\u3079\u3001\u3068\u3066\u3082\u30b7\u30f3\u30d7\u30eb\u306b\u306a\u3063\u3066\u3044\u308b\u304c\u3001\u4f55\u304c\u8d77\u304d\u3066\u3044\u308b\u306e\u304b\u5206\u304b\u3089\u306a\u3044\u3002 Edge-tpu \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63a2\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002 https://github.com/google-coral/project-posenet/blob/master/pose_engine.py \u51fa\u529b poses(Keypoints) poses:1(keypoint_scores) poses:2(pose_scores) poses:3(\u7a7a) \u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite : 1.5 MB posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite : 1.7 MB posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite : 2.5 MB \u69cb\u9020 \u5165\u529b\u30b5\u30a4\u30ba353x481\u306e posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite \u306b\u3064\u3044\u3066 posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite.html nputs/Outputs inputs 3 sub_2 UINT8 [1, 353, 481, 3] outputs [4, 5, 6, 7] 4 poses FLOAT32 [1, 10, 17, 2] 5 poses:1 FLOAT32 [1, 10, 17] 6 poses:2 FLOAT32 [1, 10] 7 poses:3 FLOAT32 [] Tensors index name type shape buffer quantization 0 MobilenetV1/heatmap_2/BiasAdd UINT8 [1, 23, 31, 17] 0 {'scale': [0.047059], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 1 MobilenetV1/offset_2/BiasAdd UINT8 [1, 23, 31, 34] 0 {'scale': [0.392157], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 2 concat UINT8 [1, 23, 31, 64] 0 {'scale': [1.387576], 'zero_point': [117], 'details_type': 'NONE', 'quantized_dimension': 0} 3 sub_2 UINT8 [1, 353, 481, 3] 0 {'scale': [0.007812], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 4 poses FLOAT32 [1, 10, 17, 2] 7 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 5 poses:1 FLOAT32 [1, 10, 17] 2 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 6 poses:2 FLOAT32 [1, 10] 6 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 7 poses:3 FLOAT32 [] 8 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} Ops index inputs outputs builtin_options opcode_index 0 [3] [0, 1, 2] None CUSTOM (0) 1 [0, 1, 2] [4, 5, 6, 7] None CUSTOM (1) CUSTOM \u306e\u6b63\u4f53\u304c\u8b0e","title":"Posenet"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#posenet-in-tensorflow","text":"","title":"(\u5099\u5fd8\u9332)PoseNet\u3092\u4f7f\u3063\u305f\u59ff\u52e2\u63a8\u5b9a in Tensorflow"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_1","text":"Posenet : https://github.com/tensorflow/tfjs-models/tree/master/posenet \u3092\u6271\u3046 \u3053\u306e\u3088\u3046\u306a\u59ff\u52e2\u63a8\u5b9a(\u9aa8\u683c\u63a8\u5b9a)\u306f\u3001input\u3055\u308c\u305f\u753b\u50cf\u306e \u3069\u3053\u5ea7\u6a19\u304c\u3069\u306e\u70b9(\u8098\u3068\u304b\u624b\u9996\u3068\u304b)\u306b\u3042\u305f\u308b\u304b\u3092\u63a8\u5b9a\u3057\u3066\u3044\u308b\u3002 Posenet\u3067\u306f\u4ee5\u4e0b\u306e17\u70b9\u3092\u63a8\u5b9a\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u3053\u308c\u3089\u306e\u70b9\u3092Keypoint\u3068\u8a00\u3046\u3002 0 nose 1 leftEye 2 rightEye 3 leftEar 4 rightEar 5 leftShoulder 6 rightShoulder 7 leftElbow 8 rightElbow 9 leftWrist 10 rightWrist 11 leftHip 12 rightHip 13 leftKnee 14 rightKnee 15 leftAnkle 16 rightAnkle","title":"\u6982\u8981"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#keypoint","text":"\u53c2\u8003(MobileNet\u7248Posenet\u306b\u3064\u3044\u3066) https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 \u5b66\u7fd2\u6e08\u307f\u306eMobileNet\u306f\u3001\u753b\u50cf\u3092\u5165\u529b\u3059\u308b\u3068\u4ee5\u4e0b\u306e\u753b\u50cf\u306e\u3088\u3046\u306a keypoint heatmap \u3084 offset vectors \u7b49\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3059\u308b\u3002 \u753b\u50cf\u4e2d keypoint heatmap , offset vectors \u304c\u683c\u5b50\u72b6\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u306f\u3001\u4e00\u9023\u306e\u51e6\u7406\u3092\u9ad8\u901f\u5316\u3055\u305b\u308b\u305f\u3081\u306e outputStride (\u30e1\u30c3\u30b7\u30e5\u5e45\u307f\u305f\u3044\u306a\u3082\u306e)\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u304b\u3089\u3002 outputStride \u306e\u6570\u5024\u3092\u5927\u304d\u304f\u3059\u308b\u3068\u30e1\u30c3\u30b7\u30e5\u304c\u5927\u304d\u304f\u5207\u3089\u308c\u3001\u51e6\u7406\u901f\u5ea6\u306f\u5411\u4e0a\u3059\u308b\u304c\u7cbe\u5ea6\u306f\u843d\u3061\u308b\u3002 \u305d\u3057\u3066 keypoint heatmap \u3092\u30b9\u30b3\u30a2\u5316\u3059\u308b\u305f\u3081\u30b7\u30b0\u30e2\u30a4\u30c9\u6d3b\u6027\u5316\u3057\u305f\u3082\u306e\u2192 heatmapPositions \u3092\u7528\u610f\u3059\u308b\u3002 Keypoint\u306f\u4ee5\u4e0a\u306e\u4e09\u8981\u7d20 ( heatmapPositions , offset vectors , outputStride ) \u3067\u8a08\u7b97\u3055\u308c\u3066\u3044\u308b\u3002 keypointPositions = heatmapPositions * outputStride + offsetVectors","title":"Keypoint \u3092\u898b\u3064\u3051\u51fa\u3059\u4ed5\u7d44\u307f"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#offset-vectors","text":"\u3053\u308c\u304cPosenet\u306e\u767a\u660e\u54c1\u306e\u3088\u3046\u3060 Long-range offsets , Short-range offsets , Mid-range offsets \u306e3\u7a2e\u985e\u3042\u308b\u304c\u3001\u3053\u306e\u4e2d\u3067\u3044\u3061\u3070\u3093\u91cd\u8981\u306a\u3082\u306e\u306fShort-range offsets\u3067\u3042\u308b\u3002 offset vectors\u306f\u57fa\u672c\u7684\u306bShort-range offsets\u3057\u305f\u30d9\u30af\u30c8\u30eb\u306e\u30c7\u30fc\u30bf\u306e\u96c6\u307e\u308a\u306e\u3053\u3068\u3092\u8a00\u3063\u3066\u3044\u308b\u3002 Long-range offsets\u306f\u59ff\u52e2\u63a8\u5b9a\u3067\u306f\u4f7f\u308f\u306a\u3044\u3002","title":"offset vectors \u306b\u3064\u3044\u3066"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#long-range-offsets","text":"\u4ee5\u4e0b\u306f\u4f8b\u3068\u3057\u3066\u9f3b\u306b\u304b\u3051\u3066\u306eLong-range offsets (vector)\u3092\u8868\u3057\u305f\u3082\u306e\u3060\u3002 https://arxiv.org/pdf/1803.08225.pdf \u3088\u308a \u3061\u306a\u307f\u306bLong-range offsets\u306f\u59ff\u52e2\u63a8\u5b9a\u3067\u306f\u4f7f\u3063\u3066\u304a\u3089\u305a(\u2190\u30bb\u30b0\u30e1\u30f3\u30c8\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u4f7f\u308f\u308c\u3066\u3044\u308b)\u3001\u59ff\u52e2\u63a8\u5b9a\u30e2\u30b8\u30e5\u30fc\u30eb\u3067\u306f\u4ee5\u4e0b\u306e\u5c40\u6240\u7684\u306a\u9818\u57df\u5185\u3067\u306e\u30d9\u30af\u30c8\u30eb\u3092\u793a\u3057\u305fShort-range offsets\u3068Mid-range offsets\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002","title":"Long-range offsets"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#short-range-offsets","text":"Short-range offsets\u306f\u30ad\u30fc\u30dd\u30a4\u30f3\u30c8\u306e\u5ea7\u6a19\u3092\u4e00\u610f\u7684\u306b\u6c7a\u3081\u308b\u5404\u683c\u5b50\u70b9\u3092\u59cb\u70b9\u3068\u3059\u308b\u30d9\u30af\u30c8\u30eb\u60c5\u5831\u3067\u3042\u308b\u3002(outputStride\u306b\u3088\u3063\u3066\u5b9a\u3081\u3089\u308c\u305f\u683c\u5b50\u70b9\u304c10x20\u3042\u308b\u5834\u5408\u30011\u500b\u306eKeypoint\u306b\u5bfe\u3059\u308bShort-range offsets\u306e\u30b5\u30a4\u30ba\u306f\u3001\u5404\u70b9\u3054\u3068\u306ex,y\u65b9\u5411\u306e\u30d9\u30af\u30c8\u30eb\u305d\u308c\u305e\u308c\u306e\u60c5\u5831\u3092\u683c\u7d0d\u3059\u308b\u306e\u3067\u300120x10x2\u3068\u306a\u308b) offset vectors \u306f\u516817\u500b\u306eKeypoint\u5206\u306eShort-range offsets\u3092\u3072\u3068\u307e\u3068\u3081\u306b\u3057\u305f\u3082\u306e\u3067\u3042\u308b\u3002(\u3064\u307e\u308a\u683c\u5b50\u70b9\u304c10x20\u3042\u308b\u5834\u5408\u3001\u30b5\u30a4\u30ba\u306f20x10x34\u3068\u306a\u308b) \u4e0b\u306e\u56f3\u3067\u306f\u3001\u9f3b\u306e\u30d2\u30fc\u30c8\u30de\u30c3\u30d7\u4ed8\u8fd1\u306eShort-range offsets\u3092\u53c2\u7167\u3057\u9f3b\u306eKeypoint\u3092\u691c\u51fa\u3059\u308b\u69d8\u5b50\u3067\u3042\u308b(\u5de6\u8098\u3082\u540c\u69d8)","title":"Short-range offsets \u306b\u3064\u3044\u3066"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#mid-range-offsets","text":"Mid-range offsets \u306fKeypoint\u540c\u58eb\u306e\u4f4d\u7f6e, \u63a5\u7d9a\u95a2\u4fc2\u3092\u683c\u7d0d\u3057\u3066\u3044\u308b\u3002","title":"Mid-range offsets \u306b\u3064\u3044\u3066"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_2","text":"","title":""},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#mobilenet","text":"\u73fe\u5728\u3001\u7cbe\u5ea6\u304c\u9ad8\u304f\u901f\u5ea6\u306f\u9045\u3044ResNet\u7248\u3082\u3042\u308b\u304c\u3001MobileNet\u7248\u306b\u7d5e\u308b \u2193\u306f\u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u30e2\u30c7\u30eb\u306b\u3064\u3044\u3066 https://qiita.com/otmb/items/561a62d3413295cc744e \u672c\u30da\u30fc\u30b8\u3067\u89e6\u308c\u3066\u3044\u306a\u3044\u304c\u3001outputStride\u309216\u3068\u3057\u3001\u5165\u529b\u753b\u50cf\u30b5\u30a4\u30ba\u304c513\u306a\u306e\u3067\u3001 ((513 - 1)// 16) + 1 = 33 \u3088\u308a\u51fa\u529b\u3055\u308c\u308b\u30c6\u30f3\u30bd\u30eb\u306e\u30b5\u30a4\u30ba\u306f1\u8fba33\u3068\u306a\u308b\u3002 \u8a08\u7b97\u5f0f https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5 Resolution = ((InputImageSize - 1) / OutputStride) + 1 \u901a\u5e38\u306e\u7573\u8fbc\u307f\u30921\u56de\u3001\u5f8c depthwise_conv2d relu6 conv2d relu6 \u309213\u56de\u7e70\u308a\u8fd4\u3057\u30014\u3064\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3057\u3001 \u66f4\u306b\u305d\u308c\u305e\u308c\u306b\u7573\u307f\u8fbc\u307f\u30921\u56de\u884c\u3063\u305f\u30014\u3064\u306e\u30c6\u30f3\u30bd\u30eb\u3092\u51fa\u529b\u3059\u308b\u3002 \u30ab\u30fc\u30cd\u30eb\u30b5\u30a4\u30ba\u306b\u3064\u3044\u3066 https://github.com/tensorflow/tfjs/issues/1137 https://github.com/tensorflow/tfjs/issues/1137#issuecomment-462450145 multi_person_mobilenet_v1_075_float.tflite \u306f\u5f8c\u8ff0\u3067\u6271\u3046 keypoint heatmap\u3060\u3051\u306f\u524d\u8ff0\u306e\u901a\u308a\u3001\u30b7\u30b0\u30e2\u30a4\u30c9\u6d3b\u6027\u5316\u3055\u305b\u308b\u304c\u3001\u4ed6\u306e\u30c6\u30f3\u30bd\u30eb\u306f\u305d\u306e\u307e\u307e\u5229\u7528\u3059\u308b\u3002 \u51fa\u529b\u3055\u308c\u308b\u30c6\u30f3\u30bd\u30eb\u306f keypoint, heatmap, offset vectors, horizontal, vertical directions \u3068\u56f3\u3067\u306f\u66f8\u3044\u3066\u3042\u308b\u304c\u3001 \u30d7\u30ed\u30b0\u30e9\u30e0\u4e2d\u3067\u306f heatmap, offset, displacementFwd, displacementBwd \u3053\u306e\u3088\u3046\u306a\u540d\u524d\u306e\u6271\u3044\u306b\u306a\u3063\u3066\u3044\u308b\u3002 \u3053\u306e(horizontal, vertical)\u3082\u3057\u304f\u306f(displacementFwd, displacementBwd)\u306e\u6b63\u4f53\u306fMid-range offsets\u3067\u3042\u308b\u3002 \u524d\u8ff0\u306e\u901a\u308a\u3001\u3053\u306e\u51fa\u529b\u3055\u308c\u305f4\u30c6\u30f3\u30bd\u30eb\u3092\u4f7f\u3044\u9aa8\u683c\u306e\u5ea7\u6a19\u3092\u8a08\u7b97\u3059\u308b\u3002 \u3057\u304b\u3057\u3001\u6700\u65b0\u306etflite\u30e2\u30c7\u30eb\u306f\u76f4\u63a5Keypoint\u3092\u51fa\u529b\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f7f\u3063\u3066\u3044\u308b...\u3002","title":"\u30e2\u30c7\u30eb\u69cb\u9020(MobileNet\u5229\u7528)"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_3","text":"\u69cb\u9020\u3084\u8a08\u7b97\u65b9\u5f0f\u304c\u30d0\u30fc\u30b8\u30e7\u30f3?\u306b\u3088\u3063\u3066\u5927\u304d\u304f\u5909\u308f\u308b\u3053\u3068\u304c\u5206\u304b\u3063\u305f\u3002 multiplier\u30920.75\u306b\u56fa\u5b9a\u3057\u8abf\u67fb\u3057\u305f\u3002(tflite\u306e\u30e2\u30c7\u30eb\u304c0.75\u306e\u3082\u306e\u3057\u304b\u898b\u3064\u304b\u3089\u306a\u304b\u3063\u305f) \u30fb\u30aa\u30ea\u30b8\u30ca\u30eb:tensorflow js\u7248(checkpoint) https://github.com/tensorflow/tfjs-models/tree/master/posenet \u30fb\u975e\u516c\u5f0fpython\u5909\u63db\u7248(Protocol Buffer(.pb)) \u5165\u529b\u30b5\u30a4\u30ba\u81ea\u7531 \u51fa\u529b4\u3064(heatmap, offset, mid-offsets(displacement_fwd, displacement_bwd)) https://github.com/rwightman/posenet-python model-mobilenet_v1_075.pb : 5.1MB \u30fbtensorflow lite\u516c\u5f0f\u30da\u30fc\u30b8\u306epose_estimation \u5165\u529b\u30b5\u30a4\u30ba\u56fa\u5b9a257x353 \u51fa\u529b4\u3064(heatmaps, short_offsets, mid_offsets, segments) https://www.tensorflow.org/lite/models/pose_estimation/overview#get_started multi_person_mobilenet_v1_075_float.tflite : 5.0 MB \u30fbgoogle-coral \u7528\u306etensorflow lite\u30e2\u30c7\u30eb \u3053\u308c\u307e\u3067\u306e\u3082\u306e\u3068\u7570\u306a\u308a\u3001\u76f4\u63a5Keypoints\u3092\u51fa\u529b\u3059\u308b\u30b9\u30bf\u30a4\u30eb\u3002 \u5165\u529b\u30b5\u30a4\u30ba\u56fa\u5b9a(\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u8a18\u8f09) \u51fa\u529b4 poses(Keypoints), poses:1(keypoint_scores), poses:2(pose_scores), poses:3(\u7a7a) https://github.com/google-coral/project-posenet posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite : 1.5 MB posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite : 1.7 MB posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite : 2.5 MB","title":"\u30e2\u30c7\u30eb\u306e\u89e3\u6790"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#pythonprotocol-bufferpb","text":"","title":"\u975e\u516c\u5f0fpython\u5909\u63db\u7248(Protocol Buffer(.pb))"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_4","text":"model-mobilenet_v1_075.pb : 5.1MB (model-mobilenet_v1_101.pb : 13.3MB )","title":"\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_5","text":"heatmap_2 offset_2 displacement_fwd_2 displacement_bwd_2","title":"\u51fa\u529b"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_6","text":"tensorbord\u3092\u4f7f\u3063\u3066\u89e3\u6790\u3092\u884c\u3044\u3001\u4e0a\u753b\u50cf\u4e2d MobilenetV1\u306e\u4e2d\u3092\u8868\u793a\u3057\u753b\u50cf\u5316\u3057\u3088\u3046\u3068\u8a66\u307f\u305f\u304c\u3001\u3042\u307e\u308a\u306b\u3082\u9577\u5927\u306a\u753b\u50cf\u306b\u306a\u3063\u3066\u3057\u307e\u3044\u6dfb\u4ed8\u3057\u306a\u3044\u3002 \u70b9\u691c\u3057\u305f\u3068\u3053\u308d\u3001\u4e0a\u8a18 \u30e2\u30c7\u30eb\u69cb\u9020(MobileNet\u5229\u7528) \u3068\u540c\u3058\u3082\u306e\u306b\u306a\u3063\u3066\u3044\u305f\u3002","title":"\u69cb\u9020"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#tensorflow-litepose_estimation","text":"\u8a73\u3057\u3044\u3053\u3068\u304c\u3042\u307e\u308a\u8f09\u3063\u3066\u3044\u306a\u3044\u304c https://www.tensorflow.org/lite/models/pose_estimation/overview#get_started \u3053\u3053\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u305f\u3082\u306e\u3002 Android, iOS\u306b\u4f7f\u3046\u98a8\u3060\u3002 \u4eca\u307e\u30672\u3064\u306b\u5206\u304b\u308c\u3066\u3044\u305fmid_offsets \u304c\u4e00\u3064\u306b\u307e\u3068\u307e\u3063\u3066\u51fa\u529b\u3055\u308c\u3001\u65b0\u3057\u304fsegments\u304c\u51fa\u529b\u3055\u308c\u308b\u3088\u3046\u306b\u306a\u3063\u305f\u3002","title":"tensorflow lite\u516c\u5f0f\u30da\u30fc\u30b8\u306epose_estimation"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#k210kmodel","text":"Fatal: Layer DepthwiseConv2d is not supported \u3068\u30a8\u30e9\u30fc\u3092\u5f15\u304d\u8d77\u3053\u3057\u3066\u3057\u307e\u3063\u305f\u3002 https://github.com/kendryte/nncase/issues/14#issuecomment-489506085 Check your DepthwiseConv2d with 3x3 kernel and 2x2 stride, there is a hardware limitation that you must use tf.pad([[0,0],[1,1],[1,1],[0,0]]) to pad your input and the use valid padding in your DepthwiseConv2d. 2x2\u30b9\u30c8\u30e9\u30a4\u30c9\u306eDepthwiseConv2d \u306e\u524d\u306b tf.pad([[0,0],[1,1],[1,1],[0,0]])\u3092\u7f6e\u3044\u3066\u6570\u5408\u308f\u305b\u3057\u3066\u3042\u3052\u3001DepthwiseConv2d\u3067\u306epadding\u306fvalid\u3092\u4f7f\u308f\u306a\u304f\u3066\u306f\u3044\u3051\u306a\u3044\u3088\u3046\u3060s","title":"K210\u7528\u306bkmodel\u3078\u5909\u63db"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_7","text":"multi_person_mobilenet_v1_075_float.tflite : 5.0 MB","title":"\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_8","text":"float_heatmaps float_mid_offsets float_segments float_short_offsets","title":"\u51fa\u529b"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_9","text":"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/visualize.py \u3092\u4f7f\u7528\u3057\u4e2d\u8eab\u3092\u62dd\u898b\u3057\u305f\u3002visualize.py\u5c0e\u5165\u307e\u3068\u3081\u306f\u5f8c\u8ff0\u3059\u308b\u3002 \u69cb\u9020\u81ea\u4f53\u306f\u4eca\u307e\u3067\u306e\u3082\u306e\u3068\u5927\u304d\u304f\u5909\u308f\u3089\u306a\u3044 multi_person_mobilenet_v1_075_float.tflite.html \u5165\u529b\u3001conv2d\u306e\u5f8c depthwise_conv2d relu6 conv2d relu6 \u309213\u56de\u7e70\u308a\u8fd4\u3057\u3001 displacement_fwd\u3068displacement_bwd\u3092conv2d\u3092\u3057\u305f\u5f8c\u304f\u3063\u3064\u3051\u3066\u308b\u3002(\u3053\u3053\u3060\u3051\u4eca\u307e\u3067\u3068\u7570\u306a\u308b)","title":"\u69cb\u9020"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#google-coral-tensorflow-lite","text":"\u3053\u308c\u307e\u3067\u306e\u3082\u306e\u3068\u7570\u306a\u308a\u3001\u76f4\u63a5Keypoints\u3092\u51fa\u529b\u3059\u308b\u30b9\u30bf\u30a4\u30eb\u3002 \u4e0a\u306epose_estimation\u3068\u6bd4\u3079\u3001\u3068\u3066\u3082\u30b7\u30f3\u30d7\u30eb\u306b\u306a\u3063\u3066\u3044\u308b\u304c\u3001\u4f55\u304c\u8d77\u304d\u3066\u3044\u308b\u306e\u304b\u5206\u304b\u3089\u306a\u3044\u3002 Edge-tpu \u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u63a2\u308b\u5fc5\u8981\u304c\u3042\u308b\u304b\u3082\u3057\u308c\u306a\u3044\u3002 https://github.com/google-coral/project-posenet/blob/master/pose_engine.py","title":"google-coral \u7528\u306etensorflow lite\u30e2\u30c7\u30eb"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_10","text":"poses(Keypoints) poses:1(keypoint_scores) poses:2(pose_scores) poses:3(\u7a7a)","title":"\u51fa\u529b"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_11","text":"posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite : 1.5 MB posenet_mobilenet_v1_075_481_641_quant_decoder_edgetpu.tflite : 1.7 MB posenet_mobilenet_v1_075_721_1281_quant_decoder_edgetpu.tflite : 2.5 MB","title":"\u30d5\u30a1\u30a4\u30eb\u30b5\u30a4\u30ba"},{"location":"%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E8%A7%A3%E8%AA%AC/posenet/#_12","text":"\u5165\u529b\u30b5\u30a4\u30ba353x481\u306e posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite \u306b\u3064\u3044\u3066 posenet_mobilenet_v1_075_353_481_quant_decoder_edgetpu.tflite.html nputs/Outputs inputs 3 sub_2 UINT8 [1, 353, 481, 3] outputs [4, 5, 6, 7] 4 poses FLOAT32 [1, 10, 17, 2] 5 poses:1 FLOAT32 [1, 10, 17] 6 poses:2 FLOAT32 [1, 10] 7 poses:3 FLOAT32 [] Tensors index name type shape buffer quantization 0 MobilenetV1/heatmap_2/BiasAdd UINT8 [1, 23, 31, 17] 0 {'scale': [0.047059], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 1 MobilenetV1/offset_2/BiasAdd UINT8 [1, 23, 31, 34] 0 {'scale': [0.392157], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 2 concat UINT8 [1, 23, 31, 64] 0 {'scale': [1.387576], 'zero_point': [117], 'details_type': 'NONE', 'quantized_dimension': 0} 3 sub_2 UINT8 [1, 353, 481, 3] 0 {'scale': [0.007812], 'zero_point': [128], 'details_type': 'NONE', 'quantized_dimension': 0} 4 poses FLOAT32 [1, 10, 17, 2] 7 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 5 poses:1 FLOAT32 [1, 10, 17] 2 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 6 poses:2 FLOAT32 [1, 10] 6 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} 7 poses:3 FLOAT32 [] 8 {'min': [-10.0], 'max': [10.0], 'details_type': 'NONE', 'quantized_dimension': 0} Ops index inputs outputs builtin_options opcode_index 0 [3] [0, 1, 2] None CUSTOM (0) 1 [0, 1, 2] [4, 5, 6, 7] None CUSTOM (1) CUSTOM \u306e\u6b63\u4f53\u304c\u8b0e","title":"\u69cb\u9020"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/","text":"3D vision Lecture video \u203b\u3053\u306e\u52d5\u753b\u3067\u306f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u307f\u6271\u3046\u3001 \u4e3b\u306a\u30bf\u30b9\u30af\u306f 1. \u7269\u4f53\u306e\u5f62\u3092\u63a8\u8ad6\u3059\u308b\uff08\u6df1\u5ea6\u63a8\u5b9a\uff09 2. \u7269\u4f53\u306e\u30af\u30e9\u30b9\u5206\u985e \u307e\u3068\u3081\uff1a\u6c17\u3092\u3064\u3051\u308b\u3053\u30684\u3064\uff01 \u30c7\u30fc\u30bf\u69cb\u9020 \u4e0b\u8a18\u306b\u307e\u3068\u3081\u308b\u304c\u3069\u3093\u306a\u30c7\u30fc\u30bf\u69cb\u9020\u30673D\u3092\u8868\u3059\u304b\u3067\u5206\u985e\u554f\u984c\u3067\u3082\u624b\u6cd5\u304c\u5168\u304f\u7570\u306a\u308b\u3001\u307f\u3093\u306a\u9055\u3063\u3066\u307f\u3093\u306a\u826f\u3044\u3002 \u8ddd\u96e2\u306e\u5b9a\u7fa9 3D\u30c7\u30fc\u30bf\u3092\u3069\u3046\u3084\u3063\u3066\u6bd4\u3079\u308b\u304b\u3002 Chamfer\u8ddd\u96e2\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u3001F1\u30ed\u30b9\u7b49\u3001\u4e0a\u624b\u304f\u9078\u307c\u3046\u3002IoU\u306f\u4f7f\u3063\u3061\u3083\u99c4\u76ee\u3067\u3059\u3002 \u30ab\u30e1\u30e9\u306e\u4ed5\u69d8 \u6b63\u6e96\u5ea7\u6a19 : \u3069\u306e\u5411\u304d\u3067\u64ae\u5f71\u3057\u305f\u304b\u3067\u306f\u306a\u304f\u5bfe\u8c61\u306e\u5411\u304d\u306b\u3088\u3063\u3066\u5ea7\u6a19\u8ef8\u3092\u5b9a\u7fa9\u3059\u308b\uff08\u4f8b\u3048\u3070\u3001\u753b\u50cf\u306e\u4eba\u9593\u306e\u5411\u3044\u3066\u308b\u65b9\u3092+Z\u306a\u3069\uff09\u3002\u4e0a\u624b\u304f\u5b66\u7fd2\u3067\u304d\u308c\u3070\u30ab\u30e1\u30e9\u30a2\u30f3\u30b0\u30eb\u7b49\u306e\u554f\u984c\u304c\u89e3\u6c7a\u3067\u304d\u308b\u304c\u904e\u5b66\u7fd2\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3002 \u8996\u70b9\u5ea7\u6a19 : \u30ab\u30e1\u30e9\u57fa\u6e96\u3067\u5ea7\u6a19\u8ef8\u3092\u6c7a\u3081\u308b\u3001\u30ad\u30cd\u30af\u30c8\u304c\u5411\u3044\u3066\u308b\u65b9\u304c+Z\u3002\u5b9f\u88c5\u304c\u697d\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 ShapeNet : \u5408\u6210\u3055\u308c\u305fCAD\u30c7\u30fc\u30bf\u3001\u52d5\u304b\u3057\u3066\u904a\u3079\u308b\u304c\u80cc\u666f\u304c\u306a\u3044\u306e\u3067\u73fe\u5b9f\u7684\u306a\u63a8\u8ad6\u306e\u8a13\u7df4\u306b\u306f\u5411\u304b\u306a\u3044\u3002\u4f55\u6545\u304b\u6905\u5b50\u3068\u8eca\u3068\u98db\u884c\u6a5f\u304c\u591a\u3044\u3002 Pix3D : \u73fe\u5b9f\u306eRGB\u3000\uff0b\u3000D\u306b\u306a\u3063\u3066\u304a\u308a\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8a13\u7df4\u306b\u3082\u4f7f\u3048\u308b\u3002\u30c7\u30fc\u30bf\u6570\u306e\u5c11\u306a\u3055\u3068\u4f4d\u7f6e\u753b\u50cf\u306b\u3064\u304d1\u7269\u4f53\u3057\u304b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3055\u308c\u3066\u306a\u3044\u3068\u3044\u3046\u5f31\u70b9\u3082\u3002 3D\u30c7\u30fc\u30bf\u69cb\u9020 3D\u30c7\u30fc\u30bf\u306e\u8868\u3057\u65b9\u306f 1. Depth Map (H x W) \u5404\u30d4\u30af\u30bb\u30eb\u3068\u30ab\u30e1\u30e9\u3068\u306e\u8ddd\u96e2\u306e\u884c\u5217\uff08RGB-D image often called 2.5D\uff09\u3002 \u96a0\u308c\u305f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u306f\u7121\u529b\u3002 Voxel Grid ( V x V x V) 3\u6b21\u5143\u884c\u5217\u3067\u5bfe\u5fdc\u3059\u308b\u4f4d\u7f6e\u306b\u7269\u4f53\u304c\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u793a\u3059\u30de\u30a4\u30f3\u30af\u30e9\u30d5\u30c8\u7684\u306a\u3082\u306e\u89e3\u50cf\u5ea6\u304c\u9ad8\u3044\u5fc5\u8981\u304c\u3042\u308a\u30e1\u30e2\u30ea\u30fc\u6d88\u8cbb\u3082\u6fc0\u3057\u3044\u305f\u3081 Oct-tree \u3084 Nested Shape Layers \u7b49\u306e\u6539\u826f\u7248\u304c\u3042\u308b Implicit surface (R^3 -> {0, 1}) \u5ea7\u6a19\u3092\u5909\u6570\u3068\u3057\u3066\u3001\u305d\u306e\u5ea7\u6a19\u306b\u7269\u4f53\u304c\u3042\u308b\u304b\u3092\u8fd4\u3059\u95a2\u6570\u3002\u63a8\u6e2c\u3059\u308b\u6642\u306f\u305d\u306e\u5ea7\u6a19\u306b\u7269\u4f53\u304c\u3042\u308b\u78ba\u7387\u3092\u8fd4\u3059\u8272\u3092\u63a8\u6e2c\u3059\u308b\u3002\u95a2\u6570\u306a\u306e\u3067\u3001\u4e00\u3064\u306e\uff13D\u30c7\u30fc\u30bf\u304b\u3089\u5ea7\u6a19\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u304c\u6b63\u3057\u3044\u304b\u5b66\u7fd2\u3059\u308b\u3068\u3044\u3046\u30b7\u30f3\u30d7\u30eb\u306a\u624b\u6cd5\u3067\u5b66\u7fd2\u3067\u304d\u308b PointCloud (P x 3) \u5ea7\u6a19\u306e\u96c6\u5408\u3002 \u30e1\u30e2\u30ea\u30fc\u6d88\u8cbb\u3092\u6291\u3048\u3064\u3064\u7269\u4f53\u306e\u5f62\u3092\u6349\u3048\u308b\u304c\u7269\u4f53\u306e\u8868\u9762\u304c\u30dc\u30b3\u30dc\u30b3\u3057\u3066\u308b\u3068\u3044\u3046\u5f31\u70b9\u304c\u3042\u308b\u3002 \u30b7\u30f3\u30d7\u30eb\u306b\u6271\u3048\u308b\u70ba\u5b66\u7fd2\u6642\u306f\u3053\u308c\u3092\u4f7f\u3046\u3053\u3068\u3082\u591a\u3044\u304c\u7269\u4f53\u3092\u8868\u73fe\u3059\u308b\u6642\u306f\u30dc\u30b3\u30dc\u30b3\u3092\u30ec\u30f3\u30c0\u30fc\u3059\u308b\u51e6\u7406\u304c\u5fc5\u8981 Mesh V\u500b\u306e\u70b9\u3092\u4e09\u89d2\u5f62\u3092\u69cb\u6210\u3059\u308b\u3088\u3046\u306b\u3064\u306a\u3044\u3060\u30b0\u30e9\u30d5\u3002 \u70b9\u306e\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u7269\u4f53\u3092\u73fe\u5b9f\u306b\u8fd1\u3044\u5f62\u3067\u8868\u73fe\u3067\u304d\u308b\uff08\u3069\u3093\u3069\u3093\u8868\u9762\u304c\u30b9\u30e0\u30fc\u30ba\u306b\u306a\u308b\uff09\u3002 \u4e09\u89d2\u5f62\u306e\u90e8\u5206\u306b\u30c6\u30af\u30b9\u30c1\u30e3\u8cbc\u3063\u305f\u308a\u3067\u304d\u308b\u3002 \u30bf\u30b9\u30af 1. Predicting Depth Maps RGB + Depth Map \u306e4\u30c1\u30e3\u30f3\u30cd\u30eb\u753b\u50cf\u3092\u5b66\u7fd2\u3057\u3001RGB\u30c1\u30e3\u30f3\u30cd\u30eb\u306e\u307f\u3067Depth Map\u3092\u63a8\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 \u6700\u521d\u306f\u5168\u7d50\u5408CNN\u3092\u4f7f\u3063\u3066\u3044\u305f\u304cRGB\u753b\u50cf\u5185\u306e \u5c0f\u3055\u3044\u7269\u4f53\u304c\u3001\u9060\u8fd1\u6cd5\u3067\u5c0f\u3055\u3044\u306e\u304b\u8fd1\u304f\u306b\u3042\u308b\u304c\u30de\u30b8\u3067\u5c0f\u3055\u3044\u306e\u304b\u308f\u304b\u3089\u306a\u3044 \u3068\u3044\u3046\u554f\u984c\u304c\u3042\u3063\u305f\u3002\u5bfe\u7b56\u3068\u3057\u3066\u8003\u3048\u3089\u308c\u305f\u306e\u304c \u30b9\u30b1\u30fc\u30eb\u4e0d\u5909\u6027\u3092\u6301\u3064RMS \u3092\u8aa4\u5dee\u95a2\u6570\u306b\u3059\u308b\u3053\u3068\u3060\u3063\u305f\u3002 \u3053\u306e\u30ed\u30b9\u3067\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u7269\u4f53\u306e\u8ddd\u96e2\u3092\u9593\u9055\u3048\u3066\u63a8\u5b9a\u3057\u305f\u3068\u3057\u3066\u3082\u3001 \u305d\u306e\u7269\u4f53\u306e\u7279\u5fb4\u304c\u4e00\u81f4\u3057\u3066\u3044\u308c\u3070\u8aa4\u5dee\u30920\u3068\u3057\u3066\u8a08\u7b97\u3059\u308b \u3002\u6b63\u78ba\u306b\u306f\u3001\u63a8\u6e2c\u3057\u305fdepthmap y \u3068grand truth\u3092 t \u3068\u3057\u3066 y * x = t where t is a scaler \u3092\u6e80\u305f\u3059\u30b9\u30ab\u30e9\u30fc x \u304c\u5b58\u5728\u3059\u308c\u3070\u6b63\u89e3\u3068\u3059\u308b\u3001\u591a\u5206\u753b\u50cf\u5185\u306e\u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u304c\u6b63\u3057\u3051\u308c\u3070\u304ak\u7684\u306a\u7406\u8ad6\u3063\u307d\u3044\u3002 1-2. Predicting Surface Normals \u30d4\u30af\u30bb\u30eb\u6bce\u306b\u30ce\u30fc\u30de\u30eb\u30d9\u30af\u30bf\u30fc\u3092\u63a8\u5b9a\u3059\u308b\u30bf\u30b9\u30af\u3002\u7269\u4f53\u306e\u5411\u304d\u3084\u306d\u3058\u308c\u3092\u6b63\u78ba\u306b\u6349\u3048\u308b 2. Classify Voxel Grid \u30dc\u30af\u30bb\u30eb\u30b0\u30ea\u30c3\u30c9\u304c\u3069\u3093\u306a\u7269\u4f53\u306a\u306e\u304b\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\u3002 \u30dc\u30af\u30bb\u30eb\u306e\u51e6\u7406\u306b\u306f\uff13DCNN\u3092\u4f7f\u3046\u30022D\u3067\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u30a2\u30d7\u30e9\u30a4\u3059\u308b\u3088\u3046\u306b\u3001\u4e09\u6b21\u5143\u7a7a\u9593\u306b\u9577\u65b9\u5f62\u306e\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u30a2\u30d7\u30e9\u30a4\u3057\u3066\u3044\u304f\u3002\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\u304c\u5897\u3048\u308b\u6bce\u306b\u30dc\u30af\u30bb\u30eb\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u304d\u3001\u6700\u5f8c\u306bFC\u5c64\u3067\u51e6\u7406\u3059\u308b\u3002 2-2. Generating Voxel RGB\u753b\u50cf\u304b\u3089Voxel grid\u3092\u63a8\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 \u5f53\u521d\u306f2DCNN\u3067\u753b\u50cf\u3092\u30d5\u30e9\u30c3\u30c8\u306b\u30573DCNN\u3067\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u3044\u305f\u3002 3D\u306e\u7279\u5fb4\u91cf\u30de\u30c3\u30d7\u306f\u8a08\u7b97\u91cf\u304c\u3084\u3070\u304b\u3063\u305f\u70ba\u3001 Voxel TUbe \u3068\u3044\u3046\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30922DCNN\u3067\u4ee3\u66ff\u3059\u308b\u3059\u308b\u624b\u6cd5\u304c\u3067\u304d\u305f\u3002 \u3053\u306e\u624b\u6cd5\u3067\u306fH x W\u306e\u753b\u50cf\u3092\u63a8\u6e2c\u3057\u3066\u304b\u3089\u5404\u30d4\u30af\u30bb\u30eb\u6bce\u306b\u5965\u884c\u304d\u3092\u4f5c\u3063\u3066\u3044\u304f\u3002 \u3053\u3063\u3061\u306e\u65b9\u304c\u65e9\u3044\u304c \u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u306f\u6b63\u3057\u3044\u304c\u7269\u4f53\u5168\u4f53\u304c\u30ba\u30ec\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (\u7269\u4f53\u306e\u5f62\u306f\u6349\u3048\u308b\u304c\u3001\u30bb\u30f3\u30b5\u30fc\u304b\u3089\u3069\u308c\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u9593\u9055\u3048\u308b)\u3002 3. Classify PointCloud Inputs PointCloud\u3067\u306f\u70b9\u306e\u9806\u756a\u306f\u95a2\u4fc2\u306a\u3044\u306e\u3067\u5404\u70b9\u3092MLP\u306b\u5165\u308c\u3066Affine\u3067\u5206\u985e\u3067\u304d\u308b\u3002 MLP\u306e\u51fa\u529b\u3092Maxpool\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u30af\u30e9\u30b9\u6570\u306e\u30d9\u30af\u30bf\u30fc\u306b\u843d\u3068\u3057\u8fbc\u3080\u3002 3-1. Predicting PointCloud Outputs PointCloud\u3092\u4e88\u6e2c\u3059\u308b\u70ba\u306b\u3001\u3075\u305f\u3064\u306ePointCloud\uff08\u51fa\u529b\u3068\u6559\u5e2b\u30c7\u30fc\u30bf\uff09\u3092\u6bd4\u3079\u308b\u30ed\u30b9\u95a2\u6570\u304c\u5fc5\u8981\u306b\u306a\u308b\u3002 Chamfer \u30ed\u30b9\u95a2\u6570\u3068\u3088\u3070\u308c\u3066\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u7684\u306b\u306f Y = predicted_point_cloud T = correct_point_cloud sum_A , sum_B = 0 , 0 for each_point in Y : y = the_point_in_Y x = the_closet_neighbour_of_y_in_T sum_A += square_of ( y - x ) # Do the same thing for each point in T # Add it to sum_B Loss = sum_A + sum_B Chamfer\u30ed\u30b9\u304c\u6700\u5c0f\u306b\u306a\u308b\u3088\u3046\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8abf\u6574\u3057\u307e\u3057\u3087\u3046\u3002 4. Predicting Meshes RGB\u753b\u50cf\u304b\u3089\u7269\u4f53\u306e\u30e1\u30c3\u30b7\u30e5\u3092\u4f5c\u6210\u3059\u308b\u30bf\u30b9\u30af\u3002 \u7c98\u571f\u307f\u305f\u3044\u306a\u30e1\u30c3\u30b7\u30e5\u3092\u3061\u3087\u3063\u3068\u6574\u5f62\u3059\u308b\u306e\u3092\u7e70\u308a\u8fd4\u3057\u3066\u76ee\u6a19\u306b\u8fd1\u3065\u3051\u308b. \u30e1\u30c3\u30b7\u30e5\u3092\u51e6\u7406\u3059\u308b\u30ec\u30a4\u30e4\u30fc\u3068\u3057\u3066 \u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f \u304c\u3042\u308b\u3002 \u5404\u70b9\u306b\u5bfe\u3057\u3066\u305d\u3053\u304b\u3089\u751f\u3048\u308b\u5168\u3066\u306e\u30a8\u30c3\u30b8\u306b\u91cd\u307f\u3092\u304b\u3051\u3066\u30a8\u30c3\u30b8\u3092\u5909\u5316\u3055\u305b\u308b\uff08\u4f34\u3063\u3066\u70b9\u3082\u52d5\u304f\uff09\u3002\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3059\u3068\u70b9\u304c\u52d5\u3044\u3066\u3044\u304f\u3002\u3053\u308c\u3092\u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u6570\u3060\u3051\u884c\u3044\u70b9\u3092\u5897\u3084\u3057\u3066\u304f\u3068\u3069\u3093\u3069\u3093\u7cbe\u5f69\u306a\uff13D\u30e2\u30c7\u30eb\u306b\u306a\u308b\u3002 \u51fa\u529b\u3055\u308c\u305f\u30e1\u30c3\u30b7\u30e5\u3068\u76ee\u6a19\u5024\u3092\u6bd4\u3079\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u65b0\u3057\u3044\u30ed\u30b9\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3002 \u5f53\u521d\u306f\u30e1\u30c3\u30b7\u30e5\u306e\u8868\u9762\u304b\u3089\u3044\u304f\u3064\u304b\u306e\u70b9\u3092\u30b5\u30f3\u30d7\u30eb\u3057\u3001Chamfer\u30ed\u30b9\u3092\u8a08\u7b97\u3057\u3066\u3044\u305f\u3002 \u554f\u984c\u3068\u3057\u3066Chamfer\u30ed\u30b9\u304c\u5916\u308c\u5024\u306b\u654f\u611f\u306a\u306e\u3067Chamfer\u30ed\u30b9\u3092\u305d\u306e\u307e\u307e\u8a08\u7b97\u3059\u308b\u3068\u30ed\u30b9\u304c\u30c7\u30ab\u3059\u304e\u308b\u3053\u3068\u304c\u3042\u3063\u305f\u3002\u305d\u3053\u3067\u70b9\u3069\u3046\u3057\u306e\u6bd4\u8f03\u3092F1\u30ed\u30b9\u3067\u8a08\u7b97\u3059\u308b\u624b\u6cd5\u304c\u64ae\u3089\u308c\u3066\u3044\u308b\u3002 \u5f37\u3044\u30e2\u30c7\u30eb Mesh R-CNN \u304c\u304a\u3059\u3059\u3081\u3089\u3057\u3044\u3002\u69cb\u9020\u3068\u3057\u3066\u306fMask R-CNN\u30672D\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u3057\u3001\u305d\u308c\u3092\u5143\u306b\u5404\u533a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30e1\u30c3\u30b7\u30e5\u3092Predicting Meshes\u306e\u9805\u306e\u624b\u6cd5\u3067\u4f5c\u3063\u3066\u3044\u304f\u3002 \u5b9f\u306f\u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f\u306b\u3088\u308b\u30e1\u30c3\u30b7\u30e5\u306e\u4f5c\u6210\u306f\u3001 \u30e1\u30c3\u30b7\u30e5\u306e\u521d\u671f\u72b6\u614b\u306b\u4f9d\u5b58\u3057\u3084\u3059\u3044 \u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u3001\u305d\u308c\u3092\u9632\u3050\u70ba\u306bGenerating Voxel\u306e\u9805\u306b\u66f8\u3044\u305f\u624b\u6cd5\u3067\u30e1\u30c3\u30b7\u30e5\u306e\u521d\u671f\u5316\u3092\u884c\u3046\u3002 [ Bounding_box , Category_label , Instance_segment ] = Mask_RCNN ( single_RGB_image ) initial_mesh = Voxel_prediction ( Bounding_box , Category_label , Instance_segment ) while not good : initial_mesh = Graph_Conv ( initial_mesh ) # \u6b63\u78ba\u306b\u306f\u4e0a\u8a18\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5404\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u6bce\u306b\u884c\u3046\u3002","title":"3D vision"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#3d-vision","text":"Lecture video \u203b\u3053\u306e\u52d5\u753b\u3067\u306f\u6559\u5e2b\u3042\u308a\u5b66\u7fd2\u306e\u307f\u6271\u3046\u3001 \u4e3b\u306a\u30bf\u30b9\u30af\u306f 1. \u7269\u4f53\u306e\u5f62\u3092\u63a8\u8ad6\u3059\u308b\uff08\u6df1\u5ea6\u63a8\u5b9a\uff09 2. \u7269\u4f53\u306e\u30af\u30e9\u30b9\u5206\u985e","title":"3D vision"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#4","text":"\u30c7\u30fc\u30bf\u69cb\u9020 \u4e0b\u8a18\u306b\u307e\u3068\u3081\u308b\u304c\u3069\u3093\u306a\u30c7\u30fc\u30bf\u69cb\u9020\u30673D\u3092\u8868\u3059\u304b\u3067\u5206\u985e\u554f\u984c\u3067\u3082\u624b\u6cd5\u304c\u5168\u304f\u7570\u306a\u308b\u3001\u307f\u3093\u306a\u9055\u3063\u3066\u307f\u3093\u306a\u826f\u3044\u3002 \u8ddd\u96e2\u306e\u5b9a\u7fa9 3D\u30c7\u30fc\u30bf\u3092\u3069\u3046\u3084\u3063\u3066\u6bd4\u3079\u308b\u304b\u3002 Chamfer\u8ddd\u96e2\u3001\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\u3001F1\u30ed\u30b9\u7b49\u3001\u4e0a\u624b\u304f\u9078\u307c\u3046\u3002IoU\u306f\u4f7f\u3063\u3061\u3083\u99c4\u76ee\u3067\u3059\u3002 \u30ab\u30e1\u30e9\u306e\u4ed5\u69d8 \u6b63\u6e96\u5ea7\u6a19 : \u3069\u306e\u5411\u304d\u3067\u64ae\u5f71\u3057\u305f\u304b\u3067\u306f\u306a\u304f\u5bfe\u8c61\u306e\u5411\u304d\u306b\u3088\u3063\u3066\u5ea7\u6a19\u8ef8\u3092\u5b9a\u7fa9\u3059\u308b\uff08\u4f8b\u3048\u3070\u3001\u753b\u50cf\u306e\u4eba\u9593\u306e\u5411\u3044\u3066\u308b\u65b9\u3092+Z\u306a\u3069\uff09\u3002\u4e0a\u624b\u304f\u5b66\u7fd2\u3067\u304d\u308c\u3070\u30ab\u30e1\u30e9\u30a2\u30f3\u30b0\u30eb\u7b49\u306e\u554f\u984c\u304c\u89e3\u6c7a\u3067\u304d\u308b\u304c\u904e\u5b66\u7fd2\u3059\u308b\u53ef\u80fd\u6027\u304c\u9ad8\u3044\u3002 \u8996\u70b9\u5ea7\u6a19 : \u30ab\u30e1\u30e9\u57fa\u6e96\u3067\u5ea7\u6a19\u8ef8\u3092\u6c7a\u3081\u308b\u3001\u30ad\u30cd\u30af\u30c8\u304c\u5411\u3044\u3066\u308b\u65b9\u304c+Z\u3002\u5b9f\u88c5\u304c\u697d\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 ShapeNet : \u5408\u6210\u3055\u308c\u305fCAD\u30c7\u30fc\u30bf\u3001\u52d5\u304b\u3057\u3066\u904a\u3079\u308b\u304c\u80cc\u666f\u304c\u306a\u3044\u306e\u3067\u73fe\u5b9f\u7684\u306a\u63a8\u8ad6\u306e\u8a13\u7df4\u306b\u306f\u5411\u304b\u306a\u3044\u3002\u4f55\u6545\u304b\u6905\u5b50\u3068\u8eca\u3068\u98db\u884c\u6a5f\u304c\u591a\u3044\u3002 Pix3D : \u73fe\u5b9f\u306eRGB\u3000\uff0b\u3000D\u306b\u306a\u3063\u3066\u304a\u308a\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u8a13\u7df4\u306b\u3082\u4f7f\u3048\u308b\u3002\u30c7\u30fc\u30bf\u6570\u306e\u5c11\u306a\u3055\u3068\u4f4d\u7f6e\u753b\u50cf\u306b\u3064\u304d1\u7269\u4f53\u3057\u304b\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3055\u308c\u3066\u306a\u3044\u3068\u3044\u3046\u5f31\u70b9\u3082\u3002","title":"\u307e\u3068\u3081\uff1a\u6c17\u3092\u3064\u3051\u308b\u3053\u30684\u3064\uff01"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#3d","text":"3D\u30c7\u30fc\u30bf\u306e\u8868\u3057\u65b9\u306f 1. Depth Map (H x W) \u5404\u30d4\u30af\u30bb\u30eb\u3068\u30ab\u30e1\u30e9\u3068\u306e\u8ddd\u96e2\u306e\u884c\u5217\uff08RGB-D image often called 2.5D\uff09\u3002 \u96a0\u308c\u305f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306b\u306f\u7121\u529b\u3002 Voxel Grid ( V x V x V) 3\u6b21\u5143\u884c\u5217\u3067\u5bfe\u5fdc\u3059\u308b\u4f4d\u7f6e\u306b\u7269\u4f53\u304c\u5b58\u5728\u3059\u308b\u304b\u3069\u3046\u304b\u3092\u793a\u3059\u30de\u30a4\u30f3\u30af\u30e9\u30d5\u30c8\u7684\u306a\u3082\u306e\u89e3\u50cf\u5ea6\u304c\u9ad8\u3044\u5fc5\u8981\u304c\u3042\u308a\u30e1\u30e2\u30ea\u30fc\u6d88\u8cbb\u3082\u6fc0\u3057\u3044\u305f\u3081 Oct-tree \u3084 Nested Shape Layers \u7b49\u306e\u6539\u826f\u7248\u304c\u3042\u308b Implicit surface (R^3 -> {0, 1}) \u5ea7\u6a19\u3092\u5909\u6570\u3068\u3057\u3066\u3001\u305d\u306e\u5ea7\u6a19\u306b\u7269\u4f53\u304c\u3042\u308b\u304b\u3092\u8fd4\u3059\u95a2\u6570\u3002\u63a8\u6e2c\u3059\u308b\u6642\u306f\u305d\u306e\u5ea7\u6a19\u306b\u7269\u4f53\u304c\u3042\u308b\u78ba\u7387\u3092\u8fd4\u3059\u8272\u3092\u63a8\u6e2c\u3059\u308b\u3002\u95a2\u6570\u306a\u306e\u3067\u3001\u4e00\u3064\u306e\uff13D\u30c7\u30fc\u30bf\u304b\u3089\u5ea7\u6a19\u3092\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8\u304c\u6b63\u3057\u3044\u304b\u5b66\u7fd2\u3059\u308b\u3068\u3044\u3046\u30b7\u30f3\u30d7\u30eb\u306a\u624b\u6cd5\u3067\u5b66\u7fd2\u3067\u304d\u308b PointCloud (P x 3) \u5ea7\u6a19\u306e\u96c6\u5408\u3002 \u30e1\u30e2\u30ea\u30fc\u6d88\u8cbb\u3092\u6291\u3048\u3064\u3064\u7269\u4f53\u306e\u5f62\u3092\u6349\u3048\u308b\u304c\u7269\u4f53\u306e\u8868\u9762\u304c\u30dc\u30b3\u30dc\u30b3\u3057\u3066\u308b\u3068\u3044\u3046\u5f31\u70b9\u304c\u3042\u308b\u3002 \u30b7\u30f3\u30d7\u30eb\u306b\u6271\u3048\u308b\u70ba\u5b66\u7fd2\u6642\u306f\u3053\u308c\u3092\u4f7f\u3046\u3053\u3068\u3082\u591a\u3044\u304c\u7269\u4f53\u3092\u8868\u73fe\u3059\u308b\u6642\u306f\u30dc\u30b3\u30dc\u30b3\u3092\u30ec\u30f3\u30c0\u30fc\u3059\u308b\u51e6\u7406\u304c\u5fc5\u8981 Mesh V\u500b\u306e\u70b9\u3092\u4e09\u89d2\u5f62\u3092\u69cb\u6210\u3059\u308b\u3088\u3046\u306b\u3064\u306a\u3044\u3060\u30b0\u30e9\u30d5\u3002 \u70b9\u306e\u6570\u3092\u5897\u3084\u3059\u3053\u3068\u3067\u7269\u4f53\u3092\u73fe\u5b9f\u306b\u8fd1\u3044\u5f62\u3067\u8868\u73fe\u3067\u304d\u308b\uff08\u3069\u3093\u3069\u3093\u8868\u9762\u304c\u30b9\u30e0\u30fc\u30ba\u306b\u306a\u308b\uff09\u3002 \u4e09\u89d2\u5f62\u306e\u90e8\u5206\u306b\u30c6\u30af\u30b9\u30c1\u30e3\u8cbc\u3063\u305f\u308a\u3067\u304d\u308b\u3002","title":"3D\u30c7\u30fc\u30bf\u69cb\u9020"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#_1","text":"","title":"\u30bf\u30b9\u30af"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#1-predicting-depth-maps","text":"RGB + Depth Map \u306e4\u30c1\u30e3\u30f3\u30cd\u30eb\u753b\u50cf\u3092\u5b66\u7fd2\u3057\u3001RGB\u30c1\u30e3\u30f3\u30cd\u30eb\u306e\u307f\u3067Depth Map\u3092\u63a8\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 \u6700\u521d\u306f\u5168\u7d50\u5408CNN\u3092\u4f7f\u3063\u3066\u3044\u305f\u304cRGB\u753b\u50cf\u5185\u306e \u5c0f\u3055\u3044\u7269\u4f53\u304c\u3001\u9060\u8fd1\u6cd5\u3067\u5c0f\u3055\u3044\u306e\u304b\u8fd1\u304f\u306b\u3042\u308b\u304c\u30de\u30b8\u3067\u5c0f\u3055\u3044\u306e\u304b\u308f\u304b\u3089\u306a\u3044 \u3068\u3044\u3046\u554f\u984c\u304c\u3042\u3063\u305f\u3002\u5bfe\u7b56\u3068\u3057\u3066\u8003\u3048\u3089\u308c\u305f\u306e\u304c \u30b9\u30b1\u30fc\u30eb\u4e0d\u5909\u6027\u3092\u6301\u3064RMS \u3092\u8aa4\u5dee\u95a2\u6570\u306b\u3059\u308b\u3053\u3068\u3060\u3063\u305f\u3002 \u3053\u306e\u30ed\u30b9\u3067\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u7269\u4f53\u306e\u8ddd\u96e2\u3092\u9593\u9055\u3048\u3066\u63a8\u5b9a\u3057\u305f\u3068\u3057\u3066\u3082\u3001 \u305d\u306e\u7269\u4f53\u306e\u7279\u5fb4\u304c\u4e00\u81f4\u3057\u3066\u3044\u308c\u3070\u8aa4\u5dee\u30920\u3068\u3057\u3066\u8a08\u7b97\u3059\u308b \u3002\u6b63\u78ba\u306b\u306f\u3001\u63a8\u6e2c\u3057\u305fdepthmap y \u3068grand truth\u3092 t \u3068\u3057\u3066 y * x = t where t is a scaler \u3092\u6e80\u305f\u3059\u30b9\u30ab\u30e9\u30fc x \u304c\u5b58\u5728\u3059\u308c\u3070\u6b63\u89e3\u3068\u3059\u308b\u3001\u591a\u5206\u753b\u50cf\u5185\u306e\u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u304c\u6b63\u3057\u3051\u308c\u3070\u304ak\u7684\u306a\u7406\u8ad6\u3063\u307d\u3044\u3002","title":"1. Predicting Depth Maps"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#1-2-predicting-surface-normals","text":"\u30d4\u30af\u30bb\u30eb\u6bce\u306b\u30ce\u30fc\u30de\u30eb\u30d9\u30af\u30bf\u30fc\u3092\u63a8\u5b9a\u3059\u308b\u30bf\u30b9\u30af\u3002\u7269\u4f53\u306e\u5411\u304d\u3084\u306d\u3058\u308c\u3092\u6b63\u78ba\u306b\u6349\u3048\u308b","title":"1-2. Predicting Surface Normals"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#2-classify-voxel-grid","text":"\u30dc\u30af\u30bb\u30eb\u30b0\u30ea\u30c3\u30c9\u304c\u3069\u3093\u306a\u7269\u4f53\u306a\u306e\u304b\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\u3002 \u30dc\u30af\u30bb\u30eb\u306e\u51e6\u7406\u306b\u306f\uff13DCNN\u3092\u4f7f\u3046\u30022D\u3067\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u30a2\u30d7\u30e9\u30a4\u3059\u308b\u3088\u3046\u306b\u3001\u4e09\u6b21\u5143\u7a7a\u9593\u306b\u9577\u65b9\u5f62\u306e\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u30a2\u30d7\u30e9\u30a4\u3057\u3066\u3044\u304f\u3002\u30c1\u30e3\u30f3\u30cd\u30eb\u6570\u304c\u5897\u3048\u308b\u6bce\u306b\u30dc\u30af\u30bb\u30eb\u304c\u5c0f\u3055\u304f\u306a\u3063\u3066\u3044\u304d\u3001\u6700\u5f8c\u306bFC\u5c64\u3067\u51e6\u7406\u3059\u308b\u3002","title":"2. Classify Voxel Grid"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#2-2-generating-voxel","text":"RGB\u753b\u50cf\u304b\u3089Voxel grid\u3092\u63a8\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 \u5f53\u521d\u306f2DCNN\u3067\u753b\u50cf\u3092\u30d5\u30e9\u30c3\u30c8\u306b\u30573DCNN\u3067\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u3044\u305f\u3002 3D\u306e\u7279\u5fb4\u91cf\u30de\u30c3\u30d7\u306f\u8a08\u7b97\u91cf\u304c\u3084\u3070\u304b\u3063\u305f\u70ba\u3001 Voxel TUbe \u3068\u3044\u3046\u30a2\u30c3\u30d7\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u30922DCNN\u3067\u4ee3\u66ff\u3059\u308b\u3059\u308b\u624b\u6cd5\u304c\u3067\u304d\u305f\u3002 \u3053\u306e\u624b\u6cd5\u3067\u306fH x W\u306e\u753b\u50cf\u3092\u63a8\u6e2c\u3057\u3066\u304b\u3089\u5404\u30d4\u30af\u30bb\u30eb\u6bce\u306b\u5965\u884c\u304d\u3092\u4f5c\u3063\u3066\u3044\u304f\u3002 \u3053\u3063\u3061\u306e\u65b9\u304c\u65e9\u3044\u304c \u76f8\u5bfe\u7684\u306a\u4f4d\u7f6e\u306f\u6b63\u3057\u3044\u304c\u7269\u4f53\u5168\u4f53\u304c\u30ba\u30ec\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b (\u7269\u4f53\u306e\u5f62\u306f\u6349\u3048\u308b\u304c\u3001\u30bb\u30f3\u30b5\u30fc\u304b\u3089\u3069\u308c\u304f\u3089\u3044\u96e2\u308c\u3066\u3044\u308b\u304b\u3092\u9593\u9055\u3048\u308b)\u3002","title":"2-2. Generating Voxel"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#3-classify-pointcloud-inputs","text":"PointCloud\u3067\u306f\u70b9\u306e\u9806\u756a\u306f\u95a2\u4fc2\u306a\u3044\u306e\u3067\u5404\u70b9\u3092MLP\u306b\u5165\u308c\u3066Affine\u3067\u5206\u985e\u3067\u304d\u308b\u3002 MLP\u306e\u51fa\u529b\u3092Maxpool\u3067\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u30af\u30e9\u30b9\u6570\u306e\u30d9\u30af\u30bf\u30fc\u306b\u843d\u3068\u3057\u8fbc\u3080\u3002","title":"3. Classify PointCloud Inputs"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#3-1-predicting-pointcloud-outputs","text":"PointCloud\u3092\u4e88\u6e2c\u3059\u308b\u70ba\u306b\u3001\u3075\u305f\u3064\u306ePointCloud\uff08\u51fa\u529b\u3068\u6559\u5e2b\u30c7\u30fc\u30bf\uff09\u3092\u6bd4\u3079\u308b\u30ed\u30b9\u95a2\u6570\u304c\u5fc5\u8981\u306b\u306a\u308b\u3002 Chamfer \u30ed\u30b9\u95a2\u6570\u3068\u3088\u3070\u308c\u3066\u3001\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u7684\u306b\u306f Y = predicted_point_cloud T = correct_point_cloud sum_A , sum_B = 0 , 0 for each_point in Y : y = the_point_in_Y x = the_closet_neighbour_of_y_in_T sum_A += square_of ( y - x ) # Do the same thing for each point in T # Add it to sum_B Loss = sum_A + sum_B Chamfer\u30ed\u30b9\u304c\u6700\u5c0f\u306b\u306a\u308b\u3088\u3046\u306b\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u8abf\u6574\u3057\u307e\u3057\u3087\u3046\u3002","title":"3-1. Predicting PointCloud Outputs"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#4-predicting-meshes","text":"RGB\u753b\u50cf\u304b\u3089\u7269\u4f53\u306e\u30e1\u30c3\u30b7\u30e5\u3092\u4f5c\u6210\u3059\u308b\u30bf\u30b9\u30af\u3002 \u7c98\u571f\u307f\u305f\u3044\u306a\u30e1\u30c3\u30b7\u30e5\u3092\u3061\u3087\u3063\u3068\u6574\u5f62\u3059\u308b\u306e\u3092\u7e70\u308a\u8fd4\u3057\u3066\u76ee\u6a19\u306b\u8fd1\u3065\u3051\u308b. \u30e1\u30c3\u30b7\u30e5\u3092\u51e6\u7406\u3059\u308b\u30ec\u30a4\u30e4\u30fc\u3068\u3057\u3066 \u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f \u304c\u3042\u308b\u3002 \u5404\u70b9\u306b\u5bfe\u3057\u3066\u305d\u3053\u304b\u3089\u751f\u3048\u308b\u5168\u3066\u306e\u30a8\u30c3\u30b8\u306b\u91cd\u307f\u3092\u304b\u3051\u3066\u30a8\u30c3\u30b8\u3092\u5909\u5316\u3055\u305b\u308b\uff08\u4f34\u3063\u3066\u70b9\u3082\u52d5\u304f\uff09\u3002\u3053\u308c\u3092\u7e70\u308a\u8fd4\u3059\u3068\u70b9\u304c\u52d5\u3044\u3066\u3044\u304f\u3002\u3053\u308c\u3092\u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u6570\u3060\u3051\u884c\u3044\u70b9\u3092\u5897\u3084\u3057\u3066\u304f\u3068\u3069\u3093\u3069\u3093\u7cbe\u5f69\u306a\uff13D\u30e2\u30c7\u30eb\u306b\u306a\u308b\u3002 \u51fa\u529b\u3055\u308c\u305f\u30e1\u30c3\u30b7\u30e5\u3068\u76ee\u6a19\u5024\u3092\u6bd4\u3079\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u65b0\u3057\u3044\u30ed\u30b9\u304c\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u3002 \u5f53\u521d\u306f\u30e1\u30c3\u30b7\u30e5\u306e\u8868\u9762\u304b\u3089\u3044\u304f\u3064\u304b\u306e\u70b9\u3092\u30b5\u30f3\u30d7\u30eb\u3057\u3001Chamfer\u30ed\u30b9\u3092\u8a08\u7b97\u3057\u3066\u3044\u305f\u3002 \u554f\u984c\u3068\u3057\u3066Chamfer\u30ed\u30b9\u304c\u5916\u308c\u5024\u306b\u654f\u611f\u306a\u306e\u3067Chamfer\u30ed\u30b9\u3092\u305d\u306e\u307e\u307e\u8a08\u7b97\u3059\u308b\u3068\u30ed\u30b9\u304c\u30c7\u30ab\u3059\u304e\u308b\u3053\u3068\u304c\u3042\u3063\u305f\u3002\u305d\u3053\u3067\u70b9\u3069\u3046\u3057\u306e\u6bd4\u8f03\u3092F1\u30ed\u30b9\u3067\u8a08\u7b97\u3059\u308b\u624b\u6cd5\u304c\u64ae\u3089\u308c\u3066\u3044\u308b\u3002","title":"4. Predicting Meshes"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/3dvision/#_2","text":"Mesh R-CNN \u304c\u304a\u3059\u3059\u3081\u3089\u3057\u3044\u3002\u69cb\u9020\u3068\u3057\u3066\u306fMask R-CNN\u30672D\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u3057\u3001\u305d\u308c\u3092\u5143\u306b\u5404\u533a\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u30e1\u30c3\u30b7\u30e5\u3092Predicting Meshes\u306e\u9805\u306e\u624b\u6cd5\u3067\u4f5c\u3063\u3066\u3044\u304f\u3002 \u5b9f\u306f\u30b0\u30e9\u30d5\u7573\u307f\u8fbc\u307f\u306b\u3088\u308b\u30e1\u30c3\u30b7\u30e5\u306e\u4f5c\u6210\u306f\u3001 \u30e1\u30c3\u30b7\u30e5\u306e\u521d\u671f\u72b6\u614b\u306b\u4f9d\u5b58\u3057\u3084\u3059\u3044 \u3068\u3044\u3046\u554f\u984c\u304c\u3042\u308a\u3001\u305d\u308c\u3092\u9632\u3050\u70ba\u306bGenerating Voxel\u306e\u9805\u306b\u66f8\u3044\u305f\u624b\u6cd5\u3067\u30e1\u30c3\u30b7\u30e5\u306e\u521d\u671f\u5316\u3092\u884c\u3046\u3002 [ Bounding_box , Category_label , Instance_segment ] = Mask_RCNN ( single_RGB_image ) initial_mesh = Voxel_prediction ( Bounding_box , Category_label , Instance_segment ) while not good : initial_mesh = Graph_Conv ( initial_mesh ) # \u6b63\u78ba\u306b\u306f\u4e0a\u8a18\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u5404\u30d0\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u6bce\u306b\u884c\u3046\u3002","title":"\u5f37\u3044\u30e2\u30c7\u30eb"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/","text":"\u3010\u3082\u3046\u3084\u308a\u305f\u304f\u306a\u3044\u3011RNN\u3068LSTM\u306e\u7406\u89e3\u3068NumPy\u306b\u3088\u308b\u5b9f\u88c5 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u305f\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3059\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u66f8\u304d\u307e\u3059\u3002\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5b66\u7fd2\u306b\u306f\u3001\u5358\u8a9e\u4e88\u6e2c\u3084\u5929\u6c17\u4e88\u6e2c\u306a\u3069\u69d8\u3005\u306a\u5fdc\u7528\u5148\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u6d41\u308c\u3067\u89e3\u8aac\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 - \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30fc\u5909\u6570\u306e\u8868\u73fe\u306e\u4ed5\u65b9 - RNN\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9 - LSTM\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9 - PyTorch\u3092\u4f7f\u3063\u305fLSTM\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9 \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u8868\u73fe\u306e\u4ed5\u65b9 \u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u529b\u3059\u308b\u306b\u306f\u3001\u4f55\u304b\u3057\u3089\u306e\u65b9\u6cd5\u3067\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u529b\u3067\u304d\u308b\u5f62\u306b\u8868\u73fe\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001one-hot encoding\u3092\u4f7f\u7528\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 \u5358\u8a9e\u306b\u5bfe\u3059\u308bone-hot encoding \u5358\u8a9e\u3092one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5358\u8a9e\u306e\u91cf\u304c\u81a8\u5927\u306b\u306a\u308b\u3068one-hot vector\u306e\u5927\u304d\u3055\u3082\u81a8\u5927\u306b\u306a\u308b\u306e\u3067\u3001\u5de5\u592b\u3092\u884c\u3044\u307e\u3059\u3002 \u4f7f\u7528\u983b\u5ea6\u306e\u9ad8\u3044k\u500b\u306e\u5358\u8a9e\u3092\u6b8b\u3057\u305d\u308c\u4ee5\u5916\u306e\u5358\u8a9e\u306fUNK\u3068\u3057\u3066\u3001one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002 \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u751f\u6210 a b a EOS, a a b b a a EOS, a a a a a b b b b b a a a a a EOS \u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002 EOS\u306f\u3001end of a sequence\u306e\u7565\u3067\u3059\u3002 ```Python: import numpy as np np.random.seed(42)#\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b def generate_dataset(num_sequences=2**8): \"\"\" \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\u95a2\u6570 num_sequences \u5468\u671f return \u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8 \"\"\" samples = [] for _ in range(num_sequences): num_tokens = np.random.randint(1, 6)#1\u304b\u30896\u307e\u3067\u306e\u6570\u30921\u3064\u751f\u6210 sample = ['a'] * num_tokens + ['b'] * num_tokens + ['a'] * num_tokens + ['EOS'] samples.append(sample) return samples sequences = generate_dataset() ## \u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u8abf\u3079\u308b one-hot encoding\u3092\u3059\u308b\u305f\u3081\u306b\u3001\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u683c\u7d0d\u3057\u3066\u3044\u308b\u8f9e\u66f8\u3092\u4f5c\u308a\u307e\u3059\u3002 defaultdict\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u8f9e\u66f8\u306evalue\u306e\u5024\u3092\u4efb\u610f\u306b\u521d\u671f\u5316\u3067\u304d\u308b\u307f\u305f\u3044\u3067\u3059\u3002 ```Python: from collections import defaultdict def sequences_to_dicts(sequences): \"\"\" \u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u683c\u7d0d\u3059\u308b\u8f9e\u66f8\u3092\u4f5c\u308b \"\"\" flatten = lambda l: [item for sublist in l for item in sublist]#list\u3092\u5168\u90e8\u3064\u306a\u3052\u308b all_words = flatten(sequences) word_count = defaultdict(int)#\u8f9e\u66f8\u306e\u521d\u671f\u5316 for word in flatten(sequences): #\u983b\u5ea6\u3092\u6570\u3048\u308b word_count[word] += 1 word_count = sorted(list(word_count.items()), key=lambda l: -l[1])#word_count\u306ekey\u3068value\u3092value\u306b\u57fa\u3065\u3044\u3066\u964d\u9806\u306b\u30bd\u30fc\u30c8 unique_words = [item[0] for item in word_count]#\u5358\u8a9e\u3092\u3068\u308b unique_words.append('UNK')#UNK\u3092\u8ffd\u52a0 num_sequences, vocab_size = len(sequences), len(unique_words) word_to_idx = defaultdict(lambda: vocab_size-1)#\u521d\u671f\u5024\u306e\u8a2d\u5b9a idx_to_word = defaultdict(lambda: 'UNK') for idx, word in enumerate(unique_words): #enumerate\u3067index\u3068\u8981\u7d20\u3092\u53d6\u5f97 #\u8f9e\u66f8\u306b\u5165\u308c\u308b word_to_idx[word] = idx idx_to_word[idx] = word return word_to_idx, idx_to_word, num_sequences, vocab_size word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences) \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u5272 \u7cfb\u5217\u30c7\u30fc\u30bf\u3092training, validation, test\u306b\u5206\u5272\u3057\u307e\u3059\u3002 \u305d\u308c\u305e\u308c\u300180%, 10%, 10%\u3067\u3059\u3002 \u7cfb\u5217\u30c7\u30fc\u30bfsequences\u306e\u5206\u5272\u306b\u306f\u3001\u30b9\u30e9\u30a4\u30b9\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002 \u30b9\u30e9\u30a4\u30b9\u3092\u4f7f\u3046\u3068\u3001 l[start:goal] \u3067l[start]\u304b\u3089l[goal-1]\u306e\u5024\u3092\u62bd\u51fa\u3067\u304d\u307e\u3059\u3002start\u3068goal\u306f\u534a\u958b\u533a\u9593\u306b\u306a\u3063\u3066\u304a\u308a\u3001l[goal]\u306f\u542b\u307e\u308c\u307e\u305b\u3093\u3002 start\u3068goal\u306f\u7701\u7565\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 l[:goal] \u306fl[0]\u304b\u3089l[goal-1]\u307e\u3067\u3001 l[start:] \u306fl[start]\u304b\u3089l[l.size()-1] (\u6700\u5f8c)\u307e\u3067\u62bd\u51fa\u3067\u304d\u307e\u3059\u3002 l[:] \u306f\u5168\u90e8\u62bd\u51fa\u3057\u307e\u3059\u3002 l[-n:] \u306f\u6700\u5f8c\u304b\u3089\u6570\u3048\u3066n\u500b\u306e\u8981\u7d20\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 l[:-n] \u306fl[0]\u304b\u3089\u62bd\u51fa\u3057\u307e\u3059\u304c\u3001\u6700\u5f8c\u306en\u500b\u306f\u62bd\u51fa\u3057\u307e\u305b\u3093\u3002 PyTorch\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: from torch.utils import data class Dataset(data.Dataset): def init (self, inputs, targets): self.intputs = inputs self.targets = targets def __len__(self): return len(self.targets) def __getitem__(self, index): X = self.inputs[index] y = self.targets[index] return X, y def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1): #\u5206\u5272\u3059\u308b\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9 num_train = int(len(sequences) p_train) num_val = int(len(sequences) p_val) num_test = int(len(sequences)*p_test) #\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u5206\u5272 #\u30b9\u30e9\u30a4\u30b9\u3092\u5229\u7528 sequences_train = sequences[:num_train] sequences_val = sequences[num_train:num_train+num_val] sequences_test = sequences[-num_test:] def get_inputs_targets_from_sequences(sequences): inputs, targets = [], [] #\u9577\u3055L\u306esequence\u304b\u3089EOS\u3092\u9664\u3044\u305fL-1 # targets\u306finputs\u306eground truth\u306e\u305f\u3081\u53f3\u306b1\u3064\u305a\u3089\u3059 for sequence in sequences: inputs.append(sequence[:-1]) targets.append(sequence[1:]) return inputs, targets #input\u3068target\u3092\u4f5c\u308b inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train) inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val) inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test) #\u5148\u307b\u3069\u5b9a\u7fa9\u3057\u305fclass\u3092\u7528\u3044\u3066dataset\u3092\u4f5c\u308b training_set = dataset_class(inputs_train, targets_train) validation_set = dataset_class(inputs_val, targets_val) test_set = dataset_class(inputs_test, targets_test) return training_set, validation_set, test_set training_set, validation_set, test_set = create_datasets(sequences, Dataset) ## one-hot vector\u5316 \u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u73fe\u308c\u308b\u5358\u8a9e\u3092\u983b\u5ea6\u306b\u57fa\u3065\u3044\u3066one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002 ```Python: def one_hot_encode(idx, vocab_size): \"\"\" one-hot vector\u5316\u3059\u308b\u3002 \"\"\" one_hot = np.zeros(vocab_size)#vocab_size = 4\u306a\u3089[0, 0, 0, 0] one_hot[idx] = 1.0#idx = 1\u306a\u3089[0, 1, 0, 0] return one_hot def one_hot_encode_sequence(sequence, vocab_size): \"\"\" return 3-D numpy array (num_words, vocab_size, 1) \"\"\" encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence]) #reshape encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1) return encoding RNN\u306e\u5c0e\u5165 Recurrent neural network (RNN)\u306f\u3001\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5206\u6790\u304c\u5f97\u610f\u3067\u3059\u3002RNN\u306f\u3001\u524d\u306e\u72b6\u614b\u3067\u4f7f\u3063\u305f\u8a08\u7b97\u7d50\u679c\u3092\u73fe\u5728\u306e\u72b6\u614b\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6982\u8981\u56f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002 x\u306f\u5165\u529b\u3067\u3042\u308b\u7cfb\u5217\u30c7\u30fc\u30bf U\u306f\u5165\u529b\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 V\u306f\u30e1\u30e2\u30ea\u30fc\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 W\u306f\u51fa\u529b\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u96a0\u308c\u72b6\u614b\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 h\u306f\u6642\u9593\u3054\u3068\u306e\u96a0\u308c\u72b6\u614b(\u30e1\u30e2\u30ea\u30fc) o\u306f\u51fa\u529b RNN\u306e\u5b9f\u88c5 NumPy\u3092\u4f7f\u3063\u3066\u3001RNN\u306e\u5b9f\u88c5\u3092forward pass, backward pass, optimization, training loop\u306e\u9806\u3067\u3084\u308a\u307e\u3059\u3002 RNN\u306e\u521d\u671f\u5316 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u521d\u671f\u5316\u3059\u308b\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: hidden_size = 50#\u96a0\u308c\u5c64(\u30e1\u30e2\u30ea\u30fc)\u306e\u6b21\u5143 vocab_size = len(word_to_idx) def init_orthogonal(param): \"\"\" \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u76f4\u4ea4\u5316\u3057\u3066\u521d\u671f\u5316 \"\"\" if param.ndim < 2: raise ValueError(\"Only parameters with 2 or more dimensions are supported.\") rows, cols = param.shape new_param = np.random.randn(rows, cols) if rows < cols: new_param = new_param.T q, r = np.linalg.qr(new_param) d = np.diag(r, 0) ph = np.sign(d) q *= ph if rows < cols: q = q.T new_param = q return new_param def init_rnn(hidden_size, vocab_size): \"\"\" RNN\u3092\u521d\u671f\u5316 \"\"\" U = np.zeros((hidden_size, vocab_size)) V = np.zeros((hidden_size, hidden_size)) W = np.zeros((vocab_size, hidden_size)) b_hidden = np.zeros((hidden_size, 1)) b_out = np.zeros((vocab_size, 1)) U = init_orthogonal(U) V = init_orthogonal(V) W = init_orthogonal(W) return U, V, W, b_hidden, b_out ``` \u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5 sigmoid,tanh, softmax\u306e\u5b9f\u88c5\u3092\u3057\u307e\u3057\u305f\u3002 \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\u306b\u5165\u529bx\u306b\u5fae\u5c11\u91cf\u3092\u8db3\u3057\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001backward pass\u7528\u306b\u5fae\u5206\u3082\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002 Python: def sigmoid(x, derivative=False): x_safe = x + 1e-12#\u5fae\u5c11\u91cf\u3092\u8db3\u3059 f = 1/(1 + np.exp(-x_safe)) if derivative: return f * (1 -f)#\u5fae\u5206\u3092\u8fd4\u3059 else: return f def tanh(x, derivative=False): x_safe = x + 1e-12 f = (np.exp(x_safe) - np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe)) if derivative: return 1-f**2 else: return f def softmax(x, derivative=False): x_safe = x + 1e-12 f = np.exp(x_safe)/np.sum(np.exp(x_safe)) if derivative: pass else: return f forward pass\u306e\u5b9f\u88c5 h = tanh(Ux + Vh + b_hidden) o = softmax(Wh + b_out) RNN\u306eforward pass\u306f\u4e0a\u5f0f\u3067\u8868\u3055\u308c\u308b\u306e\u3067\u3001\u5b9f\u88c5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002 ```Python: def forward_pass(inputs, hidden_state, params): U, V, W, b_hidden, b_out = params outputs, hidden_states = [], [] for t in range(len(inputs)): hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden) out = softmax(np.dot(W, hidden_state) + b_out) outputs.append(out) hidden_states.append(hidden_state.copy()) return outputs, hidden_states ``` backward pass\u306e\u5b9f\u88c5 forward pass\u3067\u640d\u5931\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u306e\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5(backpropagation)\u3092\u7528\u3044\u3066\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308bbackward pass\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002 \u52fe\u914d\u7206\u767a\u5bfe\u7b56\u7528\u306e\u52fe\u914d\u3092\u30af\u30ea\u30c3\u30d7\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308a\u307e\u3059\u3002 \u52fe\u914d\u306e\u5927\u304d\u3055\u304c\u4e0a\u9650\u5024\u3092\u8d85\u3048\u305f\u3089\u3001\u4e0a\u9650\u5024\u3067\u6b63\u898f\u5316\u3057\u307e\u3059\u3002 Python: def clip_gradient_norm(grads, max_norm=0.25): \"\"\" \u52fe\u914d\u7206\u767a\u5bfe\u7b56\u3067 \u52fe\u914d\u3092 g = (max_nrom/|g|)*g\u306b\u5909\u63db\u3059\u308b \"\"\" max_norm = float(max_norm) total_norm = 0 for grad in grads: grad_norm = np.sum(np.power(grad, 2)) total_norm += grad_norm total_norm = np.sqrt(total_norm) clip_coef = max_norm / (total_norm + 1e-6) if clip_coef < 1: for grad in grads: grad *= clip_coef return grads backward_pass\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308a\u307e\u3059\u3002\u640d\u5931\u3092\u6c42\u3081\u3066\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5\u3067\u305d\u308c\u305e\u308c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5fae\u5206\u3057\u305f\u640d\u5931\u306e\u52fe\u914d\u3092\u6c42\u3081\u307e\u3059\u3002 ```Python: def backward_pass(inputs, outputs, hidden_states, targets, params): U, V, W, b_hidden, b_out = params d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W) d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out) d_h_next = np.zeros_like(hidden_states[0]) loss = 0 for t in reversed(range(len(outputs))): #cross entropy loss\u3092\u8a08\u7b97 loss += -np.mean(np.log(outputs[t]+1e-12)*targets[t]) #backpropagate into output d_o = outputs[t].copy() d_o[np.argmax(targets[t])] -= -1 #backpropagate into W d_W += np.dot(d_o, hidden_states[t].T) d_b_out += d_o #backpropagate into h d_h = np.dot(W.T, d_o) + d_h_next #backpropagate through non-linearity d_f = tanh(hidden_states[t], derivative=True) * d_h d_b_hidden += d_f #backpropagate into U d_U += np.dot(d_f, inputs[t].T) #backpropagate into V d_V += np.dot(d_f, hidden_states[t-1].T) d_h_next = np.dot(V.T, d_f) grads = d_U, d_V, d_W, d_b_hidden, d_b_out grads = clip_gradient_norm(grads) return loss, grads ### optimization \u52fe\u914d\u964d\u4e0b\u6cd5\u3092\u7528\u3044\u3066\u3001RNN\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u4eca\u56de\u306f\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5(SGD)\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 ```Python: def update_paramaters(params, grads, lr=1e-3): for param, gras in zip(params, grads): #zip\u3067\u8907\u6570\u306e\u30ea\u30b9\u30c8\u306e\u8981\u7d20\u3092\u53d6\u5f97 param -= lr * grad return params \u5b66\u7fd2 \u5b9f\u88c5\u3057\u305fRNN\u306e\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(log_dir=\"./logs\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a num_epochs = 1000 \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5316 params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size) hidden_state = np.zeros((hidden_size, 1)) for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 hidde_state = np.zeros_like(hidden_state) #forward pass outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params) #backward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params) epoch_validation_loss += loss #training\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 hidde_state = np.zeros_like(hidden_state) #forward pass outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params) #backward pass training\u306a\u306e\u3067\u52fe\u914d\u3082\u8a08\u7b97 loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params) if np.isnan(loss): raise ValueError('Gradients have vanished') #network\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0 params = update_paramaters(params, grads) epoch_training_loss += loss writer.add_scalars(\"Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ![](http://namazu.tokyo/wp-content/uploads/2021/03/dd8661aaa3a8f79f8c27c79cff2db71f-300x200.png) Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u3042\u307e\u308a\u4e0a\u624b\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u96a0\u308c\u5c64\u306e\u6b21\u5143\u304c\u5c11\u306a\u3044\u3053\u3068\u3084\u30eb\u30fc\u30d7\u304c\u5c11\u306a\u3044\u3053\u3068\u3084\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5024\u304c\u5408\u3063\u3066\u306a\u3044\u3053\u3068\u304c\u539f\u56e0\u3067\u3057\u3087\u3046\u304b? ### \u30c6\u30b9\u30c8 \u5b66\u7fd2\u3057\u305fRNN\u306e\u30c6\u30b9\u30c8\u3092\u3057\u307e\u3059\u3002\u9069\u5f53\u306b\u6587\u7ae0\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u306b\u5bfe\u3057\u3066\u6b21\u306eword\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002 Python\u3067\u306f\u3001`list[-1]`\u3067\u4e00\u756a\u5f8c\u308d\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u307f\u305f\u3044\u3067\u3059\u3002 ```Python: def freestyle(params, sentence='', num_generate=10): sentence = sentence.split(' ')#\u7a7a\u767d\u3067\u533a\u5207\u308b sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size) hidden_state = np.zeros((hidden_size, 1)) outputs, hidden_states = forward_pass(sentence_one_hot, hidde_state, params) output_sentence = sentence word = idx_to_word[np.argmax(outputs[-1])] output_sentence.append(word) for i in range(num_generate): output = outputs[-1]#\u4e00\u756a\u5f8c\u308d\u306e\u5024\u3092\u53d6\u5f97 hidden_state = hidden_states[-1] output = output.reshape(1, output.shape[0], output.shape[1]) outputs, hidden_states = forward_pass(output, hidde_state, params) word = idx_to_word[np.argmax(outputs)] output_sentence.append(word) if word == \"EOS\": break return output_sentence test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n'] for i, test_example in enumerate(test_examples): print(f'Example {i}:', test_example) print('Predicted sequence:', freestyle(params, sentence=test_example), end='\\n\\n') \u4e0a\u624b\u304f\u5b66\u7fd2\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u30c6\u30b9\u30c8\u3082\u4e0a\u624b\u304f\u3044\u3063\u3066\u306a\u3044\u3053\u3068\u304c\u7d50\u679c\u304b\u3089\u308f\u304b\u308a\u307e\u3059\u3002 \u5168\u3066Unknown\u3068\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u3002 ```Shell:\u30c6\u30b9\u30c8\u7d50\u679c Example 0: a a b Predicted sequence: ['a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 1: a a a a b Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 2: a a a a a a b Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 3: a Predicted sequence: ['a', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 4: r n n Predicted sequence: ['r', 'n', 'n', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] ## LSTM\u306e\u5c0e\u5165 RNN\u306f\u3001\u30ae\u30e3\u30c3\u30d7\u304c\u5927\u304d\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u60c5\u5831\u3092\u95a2\u9023\u3065\u3051\u3066\u5b66\u7fd2\u3059\u308b\u306e\u304c\u96e3\u3057\u304f\u306a\u308a\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u9577\u671f\u4f9d\u5b58\u6027\u3092\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u305f\u306e\u304c\u3001Long Short Term Memory(LSTM)\u3067\u3059\u3002LSTM\u306f\u3001RNN\u306e\u6d3e\u751f\u3067\u540c\u3058\u3088\u3046\u306b\u7e70\u308a\u8fd4\u3057\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/93056c52056f15d863e0d4fb0cc89948-300x113.png) ### LSTM\u306e\u4ed5\u7d44\u307f LSTM\u306f\u3001\u5fd8\u5374\u30b2\u30fc\u30c8\u5c64\u3001\u5165\u529b\u30b2\u30fc\u30c8\u5c64\u3001\u51fa\u529b\u30b2\u30fc\u30c8\u5c64\u306e3\u3064\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002 LSTM\u306f\u30bb\u30eb\u3068\u547c\u3070\u308c\u308b\u30e1\u30e2\u30ea\u30fc\u304c\u60c5\u5831\u3092\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\u3002C\u304c\u30bb\u30eb\u3001x\u304c\u5165\u529b\u3001h\u304c\u51fa\u529b\u3001W\u304c\u91cd\u307f\u3001b\u304c\u30d0\u30a4\u30a2\u30b9\u3067\u3059\u3002 \u307e\u305a\u3001\u5fd8\u5374\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u30bb\u30eb\u72b6\u614b\u304b\u3089\u6368\u3066\u308b\u60c5\u5831\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002\u73fe\u5728\u306e\u5165\u529b\u3068\u30011\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u3044\u308c\u307e\u3059\u30020\u304b\u30891\u306e\u9593\u306e\u6570\u5024\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u30020\u304c\u5b8c\u5168\u306b\u6368\u3066\u308b\u3092\u8868\u3057\u30011\u304c\u5b8c\u5168\u306b\u7dad\u6301\u3092\u8868\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/7188d6629f92af532044727371464615-300x93.png) \u6b21\u306b\u3001\u5165\u529b\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u5165\u529b\u306b\u5bfe\u3057\u3066\u3069\u306e\u5024\u3092\u66f4\u65b0\u3059\u308b\u304b\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002tanh\u5c64\u3067\u30bb\u30eb\u72b6\u614b\u306b\u52a0\u3048\u3089\u308c\u308b\u65b0\u305f\u306a\u5019\u88dc\u5024\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/eaed8d1ee52ddcefbf9ef8b3aadb5fc8-300x93.png) \u30bb\u30eb\u3092\u66f4\u65b0\u3057\u307e\u3059\u30021\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5fd8\u5374\u6e08\u307f\u306e\u30bb\u30eb\u3068\u66f4\u65b0\u3059\u308b\u5024\u3092\u8db3\u3057\u5408\u308f\u305b\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/196a34321e796c678b1fc2d400977997-300x93.png) \u6700\u5f8c\u306b\u3001\u51fa\u529b\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u30bb\u30eb\u72b6\u614b\u306b\u57fa\u3065\u3044\u3066\u51fa\u529b\u3059\u308b\u3082\u306e\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/dea185317c8a1cbc486670eb6aae4ef5-300x93.png) ## LSTM\u306e\u5b9f\u88c5 NumPy\u3092\u4f7f\u3063\u3066\u3001LSTM\u306e\u5b9f\u88c5\u3092forward pass, backward pass, optimization, training loop\u306e\u9806\u3067\u3084\u308a\u307e\u3059\u3002 ### LSTM\u306e\u521d\u671f\u5316 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u521d\u671f\u5316\u3059\u308b\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: z_size = hidden_size + vocab_size def init_lstm(hidden_size, vocab_size, z_size): \"\"\" LSTM\u306e\u521d\u671f\u5316 \"\"\" W_f = np.random.randn(hidden_size, z_size) b_f = np.zeros((hidden_size, 1)) W_i = np.random.randn(hidden_size, z_size) b_i = np.zeros((hidden_size, 1)) W_g = np.random.randn(hidden_size, z_size) b_g = np.zeros((hidden_size, 1)) W_o = np.random.randn(hidden_size, z_size) b_o = np.zeros((hidden_size, 1)) W_v = np.random.randn(vocab_size, hidden_size) b_v = np.zeros((vocab_size, 1)) W_f = init_orthogonal(W_f) W_i = init_orthogonal(W_i) W_g = init_orthogonal(W_g) W_o = init_orthogonal(W_o) W_v = init_orthogonal(W_v) return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v forward pass\u306e\u5b9f\u88c5 LSTM\u306e\u4ed5\u7d44\u307f\u306b\u3042\u308b\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u901a\u308a\u306b\u5b9f\u88c5\u3057\u307e\u3059\u3002 ```Python: def forward(inputs, h_prev, C_prev, p): \"\"\" inputs:\u73fe\u5728\u306e\u5165\u529b h_prev:1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b C_prev:1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u30bb\u30eb p:LSTM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf return \u5404\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u72b6\u614b\u3068\u51fa\u529b \"\"\" assert h_prev.shape == (hidden_size, 1) assert C_prev.shape == (hidden_size, 1) W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p x_s, z_s, f_s, i_s = [], [], [], [] g_s, C_s, o_s, h_s = [], [], [], [] v_s, output_s = [], [] h_s.append(h_prev) C_s.append(C_prev) for x in inputs: #\u5165\u529b\u30681\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u3092\u7d50\u5408 z = np.row_stack((h_prev, x)) z_s.append(z) #\u5fd8\u5374\u30b2\u30fc\u30c8 f = sigmoid(np.dot(W_f, z) + b_f) f_s.append(f) #\u5165\u529b\u30b2\u30fc\u30c8 i = sigmoid(np.dot(W_i, z) + b_i) i_s.append(i) #\u73fe\u5728\u306e\u5165\u529b\u306b\u5bfe\u3057\u3066\u30bb\u30eb\u306b\u52a0\u3048\u308b\u5019\u88dc g = tanh(np.dot(W_g, z) + b_g) g_s.append(g) #\u30bb\u30eb\u306e\u66f4\u65b0 C_prev = f * C_prev + i * g C_s.append(C_prev) #\u51fa\u529b\u30b2\u30fc\u30c8 o = sigmoid(np.dot(W_o, z) + b_o) o_s.append(o) #\u51fa\u529b\u3059\u308b h_prev = o * tanh(C_prev) h_s.append(h_prev) v = np.dot(W_v, h_prev) + b_v v_s.append(v) output = softmax(v) output_s.append(output) return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s ``` backward pass\u306e\u5b9f\u88c5 \u640d\u5931\u3092\u6c42\u3081\u3066\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5\u3067\u305d\u308c\u305e\u308c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5fae\u5206\u3057\u305f\u640d\u5931\u306e\u52fe\u914d\u3092\u6c42\u3081\u307e\u3059\u3002 Python: def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params): W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p #\u52fe\u914d\u3092\u521d\u671f\u5316 W_f_d = np.zeros_like(W_f) b_f_d = np.zeros_like(b_f) W_i_d = np.zeros_like(W_i) b_i_d = np.zeros_like(b_i) W_g_d = np.zeros_like(W_g) b_g_d = np.zeros_like(b_g) W_o_d = np.zeros_like(W_o) b_o_d = np.zeros_like(b_o) W_v_d = np.zeros_like(W_v) b_v_d = np.zeros_like(b_v) #\u6b21\u306e\u30bb\u30eb\u3068\u96a0\u308c\u72b6\u614b\u3092\u521d\u671f\u5316 dh_next = np.zeros_like(h[0]) dC_next = np.zeros_like(C[0]) loss = 0 for t in reversed(range(len(outputs))): #\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30ed\u30b9\u3092\u8a08\u7b97 loss += -np.mean(np.log(outputs[t]) * targets[t]) #\u524d\u306e\u30bb\u30eb\u3092\u66f4\u65b0 C_prev = C[t-1] dv = np.copy(outputs[t]) dv[np.argmax(targets[t])] -= 1 W_v_d += np.dot(dv, h[t].T) b_v_d += dv dh = np.dot(W_v.T, dv) dh += dh_next do = dh * tanh(C[t]) do = sigmoid(o[t], derivative=True)*do W_o_d += np.dot(do, z[t].T) b_o_d += do dC = np.copy(dC_next) dC += dh * o[t] * tanh(tanh(C[t]), derivative=True) dg = dC * i[t] dg = tanh(g[t], derivative=True) * dg W_g_d += np.dot(dg, z[t].T) b_g_d += dg di = dC * g[t] di = sigmoid(i[t], True) * di W_i_d += np.dot(di, z[t].T) b_i_d += di df = dC * C_prev df = sigmoid(f[t]) * df W_f_d += np.dot(df, z[t].T) b_f_d += df dz = (np.dot(W_f.T, df) + np.dot(W_i.T, di) + np.dot(W_g.T, dg) + np.dot(W_o.T, do)) dh_prev = dz[:hidden_size, :] dC_prev = f[t] * dC grads = W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d grads = clip_gradient_norm(grads) return loss, grads \u5b66\u7fd2 \u5b9f\u88c5\u3057\u305fLSTM\u306e\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: writer = SummaryWriter(log_dir=\"./logs/lstm\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a num_epochs = 200#\u30a8\u30dd\u30c3\u30af\u6570 LSTM\u306e\u521d\u671f\u5316 z_size = hidden_size + vocab_size params = init_lstm(hidden_size, vocab_size, z_size) \u96a0\u308c\u5c64\u306e\u521d\u671f\u5316 hidden_state = np.zeros((hidden_size, 1)) for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 h = np.zeros((hidden_size, 1)) c = np.zeros((hidden_size, 1)) #forward pass z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params) #backward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params) epoch_validation_loss += loss #train\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 h = np.zeros((hidden_size, 1)) c = np.zeros((hidden_size, 1)) #forward pass z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params) #backward pass \u4eca\u306ftraining\u306a\u306e\u3067Loss\u3068\u52fe\u914d\u3092\u8a08\u7b97 loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params) #LSTM\u306e\u66f4\u65b0 params = update_paramaters(params, grads, lr=1e-1) epoch_training_loss += loss writer.add_scalars(\"LSTM Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ![](http://namazu.tokyo/wp-content/uploads/2021/03/38636fedc882953549a18b2520823905-300x202.png) Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 RNN\u3068\u6bd4\u8f03\u3059\u308b\u3068\u3001\u5b66\u7fd2\u304c\u9032\u3080\u306b\u3064\u308c\u3066Loss\u304c\u3057\u3063\u304b\u308a\u3068\u4e0b\u304c\u3063\u3066\u3044\u308b\u306e\u3067\u5b89\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002 ## PyTorch\u3092\u7528\u3044\u305fLSTM\u306e\u5b9f\u88c5 \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066LSTM\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3059\u3002 ### LSTM\u306e\u5b9a\u7fa9 \u307e\u305a\u3001LSTM\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: import torch import torch.nn as nn import torch.nn.functional as F class MyLSTM(nn.Module): def __init__(self): super(MyLSTM, self).__init__() self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=50, num_layers=1, bidirectional=False) self.l_out = nn.Linear(in_features=50, out_features=vocab_size, bias=False) def forward(self, x): x, (h, c) = self.lstm(x) x = x.view(-1, self.lstm.hidden_size) x = self.l_out(x) return x \u5b66\u7fd2 \u5b66\u7fd2\u3059\u308b\u305f\u3081\u306e\u30eb\u30fc\u30d7\u3092\u66f8\u304d\u307e\u3059\u3002 \u30ed\u30b9\u95a2\u6570\u306f\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u3001optimizer\u306fSGD\u3092\u7528\u3044\u307e\u3057\u305f\u3002 numpy\u3092\u7528\u3044\u305f\u3068\u304d\u3068\u540c\u69d8\u3067\u3059\u3002 PyTorch\u3067\u306f\u3001\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u7528\u3044\u308b\u3068\u304d\u3001target\u306fone-hot vector\u306b\u3059\u308b\u306e\u3067\u306f\u306a\u304f1\u3067\u3042\u308b\u7b87\u6240(\u6b63\u89e3\u306e\u7b87\u6240)\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u6e21\u3059\u3060\u3051\u3067\u3088\u3044\u3067\u3059\u3002 Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: num_epochs = 200#\u30a8\u30dd\u30c3\u30af\u6570 net = MyLSTM()#LSTM\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210 net = net.double()#\u578b\u3092float\u304b\u3089double\u306b\u5909\u63db criterion = nn.CrossEntropyLoss()#\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u4f7f\u7528 optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)#optimizer\u3092\u8a2d\u5b9a writer = SummaryWriter(log_dir=\"./logs/lstm_pytorch\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 net.eval()#\u30c6\u30b9\u30c8\u30e2\u30fc\u30c9 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_idx = [word_to_idx[word] for word in targets] inputs_one_hot = torch.from_numpy(inputs_one_hot) inputs_one_hot = inputs_one_hot.permute(0, 2, 1) targets_idx = torch.LongTensor(targets_idx) #forward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 outputs = net(inputs_one_hot) loss = criterion(outputs, targets_idx) epoch_validation_loss += loss.item() net.train()#\u8a13\u7df4\u30e2\u30fc\u30c9 #train\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: optimizer.zero_grad()#\u52fe\u914d\u306e\u521d\u671f\u5316 #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_idx = [word_to_idx[word] for word in targets] inputs_one_hot = torch.from_numpy(inputs_one_hot) inputs_one_hot = inputs_one_hot.permute(0, 2, 1) targets_idx = torch.LongTensor(targets_idx) #forward pass outputs = net(inputs_one_hot) #loss\u306e\u8a08\u7b97 loss = criterion(outputs, targets_idx) #backward pass \u4eca\u306ftraining\u306a\u306e\u3067\u52fe\u914d\u3092\u8a08\u7b97 loss.backward() #LSTM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0 optimizer.step() epoch_training_loss += loss.item() writer.add_scalars(\"LSTM PyTorch Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ``` Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u5148\u307b\u3069\u306enumpy\u3067\u5b9f\u88c5\u3057\u305fLSTM\u3088\u308a\u3001\u30ed\u30b9\u304c\u3057\u3063\u304b\u308a\u3068\u4e0b\u304c\u3063\u3066\u3044\u307e\u3059\u3002\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u305f\u65b9\u304c\u3088\u3044\u3067\u3059\u306d\u3002 \u307e\u3068\u3081 \u4eca\u56de\u306fRNN\u3068LSTM\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001numpy\u306e\u5b9f\u88c5\u3092\u3057\u3066\u8efd\u3044\u5b9f\u9a13\u3092\u884c\u3044\u307e\u3057\u305f\u3002\u307e\u305f\u3001PyTorch\u3092\u7528\u3044\u3066LSTM\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3057\u305f\u3002 \u53c2\u8003\u6587\u732e https://masamunetogetoge.com/gradient-vanish https://qiita.com/naoaki0802/items/7a11cded96f3a6165d01 http://kento1109.hatenablog.com/entry/2019/07/06/182247 https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca https://qiita.com/t_Signull/items/21b82be280b46f467d1b https://qiita.com/tanuk1647/items/276d2be36f5abb8ea52e","title":"\u3010\u3082\u3046\u3084\u308a\u305f\u304f\u306a\u3044\u3011RNN\u3068LSTM\u306e\u7406\u89e3\u3068NumPy\u306b\u3088\u308b\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#rnnlstmnumpy","text":"\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u7528\u3044\u305f\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u3059\u308b\u65b9\u6cd5\u306b\u3064\u3044\u3066\u66f8\u304d\u307e\u3059\u3002\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5b66\u7fd2\u306b\u306f\u3001\u5358\u8a9e\u4e88\u6e2c\u3084\u5929\u6c17\u4e88\u6e2c\u306a\u3069\u69d8\u3005\u306a\u5fdc\u7528\u5148\u304c\u3042\u308a\u307e\u3059\u3002 \u3053\u306e\u6d41\u308c\u3067\u89e3\u8aac\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 - \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u306f\u3001\u30ab\u30c6\u30b4\u30ea\u30fc\u5909\u6570\u306e\u8868\u73fe\u306e\u4ed5\u65b9 - RNN\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9 - LSTM\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9 - PyTorch\u3092\u4f7f\u3063\u305fLSTM\u306e\u5b9f\u88c5\u306e\u4ed5\u65b9","title":"\u3010\u3082\u3046\u3084\u308a\u305f\u304f\u306a\u3044\u3011RNN\u3068LSTM\u306e\u7406\u89e3\u3068NumPy\u306b\u3088\u308b\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_1","text":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u529b\u3059\u308b\u306b\u306f\u3001\u4f55\u304b\u3057\u3089\u306e\u65b9\u6cd5\u3067\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u5165\u529b\u3067\u304d\u308b\u5f62\u306b\u8868\u73fe\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u3001one-hot encoding\u3092\u4f7f\u7528\u3057\u3066\u3044\u304d\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002","title":"\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u8868\u73fe\u306e\u4ed5\u65b9"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#one-hot-encoding","text":"\u5358\u8a9e\u3092one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u5358\u8a9e\u306e\u91cf\u304c\u81a8\u5927\u306b\u306a\u308b\u3068one-hot vector\u306e\u5927\u304d\u3055\u3082\u81a8\u5927\u306b\u306a\u308b\u306e\u3067\u3001\u5de5\u592b\u3092\u884c\u3044\u307e\u3059\u3002 \u4f7f\u7528\u983b\u5ea6\u306e\u9ad8\u3044k\u500b\u306e\u5358\u8a9e\u3092\u6b8b\u3057\u305d\u308c\u4ee5\u5916\u306e\u5358\u8a9e\u306fUNK\u3068\u3057\u3066\u3001one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002","title":"\u5358\u8a9e\u306b\u5bfe\u3059\u308bone-hot encoding"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_2","text":"a b a EOS, a a b b a a EOS, a a a a a b b b b b a a a a a EOS \u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3092\u8003\u3048\u307e\u3059\u3002 EOS\u306f\u3001end of a sequence\u306e\u7565\u3067\u3059\u3002 ```Python: import numpy as np np.random.seed(42)#\u4e71\u6570\u3092\u56fa\u5b9a\u3059\u308b def generate_dataset(num_sequences=2**8): \"\"\" \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u751f\u6210\u3059\u308b\u95a2\u6570 num_sequences \u5468\u671f return \u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u30ea\u30b9\u30c8 \"\"\" samples = [] for _ in range(num_sequences): num_tokens = np.random.randint(1, 6)#1\u304b\u30896\u307e\u3067\u306e\u6570\u30921\u3064\u751f\u6210 sample = ['a'] * num_tokens + ['b'] * num_tokens + ['a'] * num_tokens + ['EOS'] samples.append(sample) return samples sequences = generate_dataset() ## \u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u8abf\u3079\u308b one-hot encoding\u3092\u3059\u308b\u305f\u3081\u306b\u3001\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u683c\u7d0d\u3057\u3066\u3044\u308b\u8f9e\u66f8\u3092\u4f5c\u308a\u307e\u3059\u3002 defaultdict\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u8f9e\u66f8\u306evalue\u306e\u5024\u3092\u4efb\u610f\u306b\u521d\u671f\u5316\u3067\u304d\u308b\u307f\u305f\u3044\u3067\u3059\u3002 ```Python: from collections import defaultdict def sequences_to_dicts(sequences): \"\"\" \u5358\u8a9e\u3068\u305d\u306e\u51fa\u73fe\u983b\u5ea6\u3092\u683c\u7d0d\u3059\u308b\u8f9e\u66f8\u3092\u4f5c\u308b \"\"\" flatten = lambda l: [item for sublist in l for item in sublist]#list\u3092\u5168\u90e8\u3064\u306a\u3052\u308b all_words = flatten(sequences) word_count = defaultdict(int)#\u8f9e\u66f8\u306e\u521d\u671f\u5316 for word in flatten(sequences): #\u983b\u5ea6\u3092\u6570\u3048\u308b word_count[word] += 1 word_count = sorted(list(word_count.items()), key=lambda l: -l[1])#word_count\u306ekey\u3068value\u3092value\u306b\u57fa\u3065\u3044\u3066\u964d\u9806\u306b\u30bd\u30fc\u30c8 unique_words = [item[0] for item in word_count]#\u5358\u8a9e\u3092\u3068\u308b unique_words.append('UNK')#UNK\u3092\u8ffd\u52a0 num_sequences, vocab_size = len(sequences), len(unique_words) word_to_idx = defaultdict(lambda: vocab_size-1)#\u521d\u671f\u5024\u306e\u8a2d\u5b9a idx_to_word = defaultdict(lambda: 'UNK') for idx, word in enumerate(unique_words): #enumerate\u3067index\u3068\u8981\u7d20\u3092\u53d6\u5f97 #\u8f9e\u66f8\u306b\u5165\u308c\u308b word_to_idx[word] = idx idx_to_word[idx] = word return word_to_idx, idx_to_word, num_sequences, vocab_size word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)","title":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u751f\u6210"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_3","text":"\u7cfb\u5217\u30c7\u30fc\u30bf\u3092training, validation, test\u306b\u5206\u5272\u3057\u307e\u3059\u3002 \u305d\u308c\u305e\u308c\u300180%, 10%, 10%\u3067\u3059\u3002 \u7cfb\u5217\u30c7\u30fc\u30bfsequences\u306e\u5206\u5272\u306b\u306f\u3001\u30b9\u30e9\u30a4\u30b9\u3092\u4f7f\u3063\u3066\u3044\u307e\u3059\u3002 \u30b9\u30e9\u30a4\u30b9\u3092\u4f7f\u3046\u3068\u3001 l[start:goal] \u3067l[start]\u304b\u3089l[goal-1]\u306e\u5024\u3092\u62bd\u51fa\u3067\u304d\u307e\u3059\u3002start\u3068goal\u306f\u534a\u958b\u533a\u9593\u306b\u306a\u3063\u3066\u304a\u308a\u3001l[goal]\u306f\u542b\u307e\u308c\u307e\u305b\u3093\u3002 start\u3068goal\u306f\u7701\u7565\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 l[:goal] \u306fl[0]\u304b\u3089l[goal-1]\u307e\u3067\u3001 l[start:] \u306fl[start]\u304b\u3089l[l.size()-1] (\u6700\u5f8c)\u307e\u3067\u62bd\u51fa\u3067\u304d\u307e\u3059\u3002 l[:] \u306f\u5168\u90e8\u62bd\u51fa\u3057\u307e\u3059\u3002 l[-n:] \u306f\u6700\u5f8c\u304b\u3089\u6570\u3048\u3066n\u500b\u306e\u8981\u7d20\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002 l[:-n] \u306fl[0]\u304b\u3089\u62bd\u51fa\u3057\u307e\u3059\u304c\u3001\u6700\u5f8c\u306en\u500b\u306f\u62bd\u51fa\u3057\u307e\u305b\u3093\u3002 PyTorch\u3092\u7528\u3044\u3066\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: from torch.utils import data class Dataset(data.Dataset): def init (self, inputs, targets): self.intputs = inputs self.targets = targets def __len__(self): return len(self.targets) def __getitem__(self, index): X = self.inputs[index] y = self.targets[index] return X, y def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1): #\u5206\u5272\u3059\u308b\u30b5\u30a4\u30ba\u3092\u5b9a\u7fa9 num_train = int(len(sequences) p_train) num_val = int(len(sequences) p_val) num_test = int(len(sequences)*p_test) #\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u5206\u5272 #\u30b9\u30e9\u30a4\u30b9\u3092\u5229\u7528 sequences_train = sequences[:num_train] sequences_val = sequences[num_train:num_train+num_val] sequences_test = sequences[-num_test:] def get_inputs_targets_from_sequences(sequences): inputs, targets = [], [] #\u9577\u3055L\u306esequence\u304b\u3089EOS\u3092\u9664\u3044\u305fL-1 # targets\u306finputs\u306eground truth\u306e\u305f\u3081\u53f3\u306b1\u3064\u305a\u3089\u3059 for sequence in sequences: inputs.append(sequence[:-1]) targets.append(sequence[1:]) return inputs, targets #input\u3068target\u3092\u4f5c\u308b inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train) inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val) inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test) #\u5148\u307b\u3069\u5b9a\u7fa9\u3057\u305fclass\u3092\u7528\u3044\u3066dataset\u3092\u4f5c\u308b training_set = dataset_class(inputs_train, targets_train) validation_set = dataset_class(inputs_val, targets_val) test_set = dataset_class(inputs_test, targets_test) return training_set, validation_set, test_set training_set, validation_set, test_set = create_datasets(sequences, Dataset) ## one-hot vector\u5316 \u7cfb\u5217\u30c7\u30fc\u30bf\u306b\u73fe\u308c\u308b\u5358\u8a9e\u3092\u983b\u5ea6\u306b\u57fa\u3065\u3044\u3066one-hot vector\u306b\u5909\u63db\u3057\u307e\u3059\u3002 ```Python: def one_hot_encode(idx, vocab_size): \"\"\" one-hot vector\u5316\u3059\u308b\u3002 \"\"\" one_hot = np.zeros(vocab_size)#vocab_size = 4\u306a\u3089[0, 0, 0, 0] one_hot[idx] = 1.0#idx = 1\u306a\u3089[0, 1, 0, 0] return one_hot def one_hot_encode_sequence(sequence, vocab_size): \"\"\" return 3-D numpy array (num_words, vocab_size, 1) \"\"\" encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence]) #reshape encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1) return encoding","title":"\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5206\u5272"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#rnn","text":"Recurrent neural network (RNN)\u306f\u3001\u7cfb\u5217\u30c7\u30fc\u30bf\u306e\u5206\u6790\u304c\u5f97\u610f\u3067\u3059\u3002RNN\u306f\u3001\u524d\u306e\u72b6\u614b\u3067\u4f7f\u3063\u305f\u8a08\u7b97\u7d50\u679c\u3092\u73fe\u5728\u306e\u72b6\u614b\u306b\u5229\u7528\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u6982\u8981\u56f3\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002 x\u306f\u5165\u529b\u3067\u3042\u308b\u7cfb\u5217\u30c7\u30fc\u30bf U\u306f\u5165\u529b\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 V\u306f\u30e1\u30e2\u30ea\u30fc\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 W\u306f\u51fa\u529b\u3092\u8a08\u7b97\u3059\u308b\u305f\u3081\u306e\u96a0\u308c\u72b6\u614b\u306b\u5bfe\u3059\u308b\u91cd\u307f\u884c\u5217 h\u306f\u6642\u9593\u3054\u3068\u306e\u96a0\u308c\u72b6\u614b(\u30e1\u30e2\u30ea\u30fc) o\u306f\u51fa\u529b","title":"RNN\u306e\u5c0e\u5165"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#rnn_1","text":"NumPy\u3092\u4f7f\u3063\u3066\u3001RNN\u306e\u5b9f\u88c5\u3092forward pass, backward pass, optimization, training loop\u306e\u9806\u3067\u3084\u308a\u307e\u3059\u3002","title":"RNN\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#rnn_2","text":"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u521d\u671f\u5316\u3059\u308b\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: hidden_size = 50#\u96a0\u308c\u5c64(\u30e1\u30e2\u30ea\u30fc)\u306e\u6b21\u5143 vocab_size = len(word_to_idx) def init_orthogonal(param): \"\"\" \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u76f4\u4ea4\u5316\u3057\u3066\u521d\u671f\u5316 \"\"\" if param.ndim < 2: raise ValueError(\"Only parameters with 2 or more dimensions are supported.\") rows, cols = param.shape new_param = np.random.randn(rows, cols) if rows < cols: new_param = new_param.T q, r = np.linalg.qr(new_param) d = np.diag(r, 0) ph = np.sign(d) q *= ph if rows < cols: q = q.T new_param = q return new_param def init_rnn(hidden_size, vocab_size): \"\"\" RNN\u3092\u521d\u671f\u5316 \"\"\" U = np.zeros((hidden_size, vocab_size)) V = np.zeros((hidden_size, hidden_size)) W = np.zeros((vocab_size, hidden_size)) b_hidden = np.zeros((hidden_size, 1)) b_out = np.zeros((vocab_size, 1)) U = init_orthogonal(U) V = init_orthogonal(V) W = init_orthogonal(W) return U, V, W, b_hidden, b_out ```","title":"RNN\u306e\u521d\u671f\u5316"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_4","text":"sigmoid,tanh, softmax\u306e\u5b9f\u88c5\u3092\u3057\u307e\u3057\u305f\u3002 \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\u306b\u5165\u529bx\u306b\u5fae\u5c11\u91cf\u3092\u8db3\u3057\u3066\u3044\u307e\u3059\u3002 \u307e\u305f\u3001backward pass\u7528\u306b\u5fae\u5206\u3082\u8a08\u7b97\u3057\u3066\u3044\u307e\u3059\u3002 Python: def sigmoid(x, derivative=False): x_safe = x + 1e-12#\u5fae\u5c11\u91cf\u3092\u8db3\u3059 f = 1/(1 + np.exp(-x_safe)) if derivative: return f * (1 -f)#\u5fae\u5206\u3092\u8fd4\u3059 else: return f def tanh(x, derivative=False): x_safe = x + 1e-12 f = (np.exp(x_safe) - np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe)) if derivative: return 1-f**2 else: return f def softmax(x, derivative=False): x_safe = x + 1e-12 f = np.exp(x_safe)/np.sum(np.exp(x_safe)) if derivative: pass else: return f","title":"\u6d3b\u6027\u5316\u95a2\u6570\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#forward-pass","text":"h = tanh(Ux + Vh + b_hidden) o = softmax(Wh + b_out) RNN\u306eforward pass\u306f\u4e0a\u5f0f\u3067\u8868\u3055\u308c\u308b\u306e\u3067\u3001\u5b9f\u88c5\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059\u3002 ```Python: def forward_pass(inputs, hidden_state, params): U, V, W, b_hidden, b_out = params outputs, hidden_states = [], [] for t in range(len(inputs)): hidden_state = tanh(np.dot(U, inputs[t]) + np.dot(V, hidden_state) + b_hidden) out = softmax(np.dot(W, hidden_state) + b_out) outputs.append(out) hidden_states.append(hidden_state.copy()) return outputs, hidden_states ```","title":"forward pass\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#backward-pass","text":"forward pass\u3067\u640d\u5931\u306e\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308b\u306e\u306f\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5(backpropagation)\u3092\u7528\u3044\u3066\u52fe\u914d\u3092\u8a08\u7b97\u3059\u308bbackward pass\u3092\u5b9f\u88c5\u3057\u307e\u3059\u3002 \u52fe\u914d\u7206\u767a\u5bfe\u7b56\u7528\u306e\u52fe\u914d\u3092\u30af\u30ea\u30c3\u30d7\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308a\u307e\u3059\u3002 \u52fe\u914d\u306e\u5927\u304d\u3055\u304c\u4e0a\u9650\u5024\u3092\u8d85\u3048\u305f\u3089\u3001\u4e0a\u9650\u5024\u3067\u6b63\u898f\u5316\u3057\u307e\u3059\u3002 Python: def clip_gradient_norm(grads, max_norm=0.25): \"\"\" \u52fe\u914d\u7206\u767a\u5bfe\u7b56\u3067 \u52fe\u914d\u3092 g = (max_nrom/|g|)*g\u306b\u5909\u63db\u3059\u308b \"\"\" max_norm = float(max_norm) total_norm = 0 for grad in grads: grad_norm = np.sum(np.power(grad, 2)) total_norm += grad_norm total_norm = np.sqrt(total_norm) clip_coef = max_norm / (total_norm + 1e-6) if clip_coef < 1: for grad in grads: grad *= clip_coef return grads backward_pass\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3092\u4f5c\u308a\u307e\u3059\u3002\u640d\u5931\u3092\u6c42\u3081\u3066\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5\u3067\u305d\u308c\u305e\u308c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5fae\u5206\u3057\u305f\u640d\u5931\u306e\u52fe\u914d\u3092\u6c42\u3081\u307e\u3059\u3002 ```Python: def backward_pass(inputs, outputs, hidden_states, targets, params): U, V, W, b_hidden, b_out = params d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W) d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out) d_h_next = np.zeros_like(hidden_states[0]) loss = 0 for t in reversed(range(len(outputs))): #cross entropy loss\u3092\u8a08\u7b97 loss += -np.mean(np.log(outputs[t]+1e-12)*targets[t]) #backpropagate into output d_o = outputs[t].copy() d_o[np.argmax(targets[t])] -= -1 #backpropagate into W d_W += np.dot(d_o, hidden_states[t].T) d_b_out += d_o #backpropagate into h d_h = np.dot(W.T, d_o) + d_h_next #backpropagate through non-linearity d_f = tanh(hidden_states[t], derivative=True) * d_h d_b_hidden += d_f #backpropagate into U d_U += np.dot(d_f, inputs[t].T) #backpropagate into V d_V += np.dot(d_f, hidden_states[t-1].T) d_h_next = np.dot(V.T, d_f) grads = d_U, d_V, d_W, d_b_hidden, d_b_out grads = clip_gradient_norm(grads) return loss, grads ### optimization \u52fe\u914d\u964d\u4e0b\u6cd5\u3092\u7528\u3044\u3066\u3001RNN\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0\u3057\u307e\u3059\u3002\u4eca\u56de\u306f\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5(SGD)\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 ```Python: def update_paramaters(params, grads, lr=1e-3): for param, gras in zip(params, grads): #zip\u3067\u8907\u6570\u306e\u30ea\u30b9\u30c8\u306e\u8981\u7d20\u3092\u53d6\u5f97 param -= lr * grad return params","title":"backward pass\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_5","text":"\u5b9f\u88c5\u3057\u305fRNN\u306e\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(log_dir=\"./logs\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a num_epochs = 1000","title":"\u5b66\u7fd2"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_6","text":"params = init_rnn(hidden_size=hidden_size, vocab_size=vocab_size) hidden_state = np.zeros((hidden_size, 1)) for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 hidde_state = np.zeros_like(hidden_state) #forward pass outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params) #backward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 loss, _ = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params) epoch_validation_loss += loss #training\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 hidde_state = np.zeros_like(hidden_state) #forward pass outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, params) #backward pass training\u306a\u306e\u3067\u52fe\u914d\u3082\u8a08\u7b97 loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, params) if np.isnan(loss): raise ValueError('Gradients have vanished') #network\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0 params = update_paramaters(params, grads) epoch_training_loss += loss writer.add_scalars(\"Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ![](http://namazu.tokyo/wp-content/uploads/2021/03/dd8661aaa3a8f79f8c27c79cff2db71f-300x200.png) Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u3042\u307e\u308a\u4e0a\u624b\u304f\u5b66\u7fd2\u3067\u304d\u3066\u3044\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002\u96a0\u308c\u5c64\u306e\u6b21\u5143\u304c\u5c11\u306a\u3044\u3053\u3068\u3084\u30eb\u30fc\u30d7\u304c\u5c11\u306a\u3044\u3053\u3068\u3084\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5024\u304c\u5408\u3063\u3066\u306a\u3044\u3053\u3068\u304c\u539f\u56e0\u3067\u3057\u3087\u3046\u304b? ### \u30c6\u30b9\u30c8 \u5b66\u7fd2\u3057\u305fRNN\u306e\u30c6\u30b9\u30c8\u3092\u3057\u307e\u3059\u3002\u9069\u5f53\u306b\u6587\u7ae0\u3092\u751f\u6210\u3057\u3001\u305d\u308c\u306b\u5bfe\u3057\u3066\u6b21\u306eword\u3092\u4e88\u6e2c\u3057\u307e\u3059\u3002 Python\u3067\u306f\u3001`list[-1]`\u3067\u4e00\u756a\u5f8c\u308d\u306e\u5024\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u307f\u305f\u3044\u3067\u3059\u3002 ```Python: def freestyle(params, sentence='', num_generate=10): sentence = sentence.split(' ')#\u7a7a\u767d\u3067\u533a\u5207\u308b sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size) hidden_state = np.zeros((hidden_size, 1)) outputs, hidden_states = forward_pass(sentence_one_hot, hidde_state, params) output_sentence = sentence word = idx_to_word[np.argmax(outputs[-1])] output_sentence.append(word) for i in range(num_generate): output = outputs[-1]#\u4e00\u756a\u5f8c\u308d\u306e\u5024\u3092\u53d6\u5f97 hidden_state = hidden_states[-1] output = output.reshape(1, output.shape[0], output.shape[1]) outputs, hidden_states = forward_pass(output, hidde_state, params) word = idx_to_word[np.argmax(outputs)] output_sentence.append(word) if word == \"EOS\": break return output_sentence test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n'] for i, test_example in enumerate(test_examples): print(f'Example {i}:', test_example) print('Predicted sequence:', freestyle(params, sentence=test_example), end='\\n\\n') \u4e0a\u624b\u304f\u5b66\u7fd2\u3057\u3066\u3044\u306a\u3044\u306e\u3067\u3001\u30c6\u30b9\u30c8\u3082\u4e0a\u624b\u304f\u3044\u3063\u3066\u306a\u3044\u3053\u3068\u304c\u7d50\u679c\u304b\u3089\u308f\u304b\u308a\u307e\u3059\u3002 \u5168\u3066Unknown\u3068\u4e88\u6e2c\u3057\u3066\u3044\u307e\u3059\u3002 ```Shell:\u30c6\u30b9\u30c8\u7d50\u679c Example 0: a a b Predicted sequence: ['a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 1: a a a a b Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 2: a a a a a a b Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 3: a Predicted sequence: ['a', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] Example 4: r n n Predicted sequence: ['r', 'n', 'n', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK'] ## LSTM\u306e\u5c0e\u5165 RNN\u306f\u3001\u30ae\u30e3\u30c3\u30d7\u304c\u5927\u304d\u304f\u306a\u308b\u306b\u3064\u308c\u3066\u60c5\u5831\u3092\u95a2\u9023\u3065\u3051\u3066\u5b66\u7fd2\u3059\u308b\u306e\u304c\u96e3\u3057\u304f\u306a\u308a\u307e\u3059\u3002 \u3053\u306e\u3088\u3046\u306a\u9577\u671f\u4f9d\u5b58\u6027\u3092\u5b66\u7fd2\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u305f\u306e\u304c\u3001Long Short Term Memory(LSTM)\u3067\u3059\u3002LSTM\u306f\u3001RNN\u306e\u6d3e\u751f\u3067\u540c\u3058\u3088\u3046\u306b\u7e70\u308a\u8fd4\u3057\u30e2\u30b8\u30e5\u30fc\u30eb\u306b\u306a\u3063\u3066\u3044\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/93056c52056f15d863e0d4fb0cc89948-300x113.png) ### LSTM\u306e\u4ed5\u7d44\u307f LSTM\u306f\u3001\u5fd8\u5374\u30b2\u30fc\u30c8\u5c64\u3001\u5165\u529b\u30b2\u30fc\u30c8\u5c64\u3001\u51fa\u529b\u30b2\u30fc\u30c8\u5c64\u306e3\u3064\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002 LSTM\u306f\u30bb\u30eb\u3068\u547c\u3070\u308c\u308b\u30e1\u30e2\u30ea\u30fc\u304c\u60c5\u5831\u3092\u4fdd\u6301\u3057\u3066\u3044\u307e\u3059\u3002C\u304c\u30bb\u30eb\u3001x\u304c\u5165\u529b\u3001h\u304c\u51fa\u529b\u3001W\u304c\u91cd\u307f\u3001b\u304c\u30d0\u30a4\u30a2\u30b9\u3067\u3059\u3002 \u307e\u305a\u3001\u5fd8\u5374\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u30bb\u30eb\u72b6\u614b\u304b\u3089\u6368\u3066\u308b\u60c5\u5831\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002\u73fe\u5728\u306e\u5165\u529b\u3068\u30011\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u3092\u30b7\u30b0\u30e2\u30a4\u30c9\u95a2\u6570\u306b\u3044\u308c\u307e\u3059\u30020\u304b\u30891\u306e\u9593\u306e\u6570\u5024\u304c\u51fa\u529b\u3055\u308c\u307e\u3059\u30020\u304c\u5b8c\u5168\u306b\u6368\u3066\u308b\u3092\u8868\u3057\u30011\u304c\u5b8c\u5168\u306b\u7dad\u6301\u3092\u8868\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/7188d6629f92af532044727371464615-300x93.png) \u6b21\u306b\u3001\u5165\u529b\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u5165\u529b\u306b\u5bfe\u3057\u3066\u3069\u306e\u5024\u3092\u66f4\u65b0\u3059\u308b\u304b\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002tanh\u5c64\u3067\u30bb\u30eb\u72b6\u614b\u306b\u52a0\u3048\u3089\u308c\u308b\u65b0\u305f\u306a\u5019\u88dc\u5024\u306e\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/eaed8d1ee52ddcefbf9ef8b3aadb5fc8-300x93.png) \u30bb\u30eb\u3092\u66f4\u65b0\u3057\u307e\u3059\u30021\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u5fd8\u5374\u6e08\u307f\u306e\u30bb\u30eb\u3068\u66f4\u65b0\u3059\u308b\u5024\u3092\u8db3\u3057\u5408\u308f\u305b\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/196a34321e796c678b1fc2d400977997-300x93.png) \u6700\u5f8c\u306b\u3001\u51fa\u529b\u30b2\u30fc\u30c8\u5c64\u3067\u3001\u30bb\u30eb\u72b6\u614b\u306b\u57fa\u3065\u3044\u3066\u51fa\u529b\u3059\u308b\u3082\u306e\u3092\u5224\u5b9a\u3057\u307e\u3059\u3002 ![](http://namazu.tokyo/wp-content/uploads/2021/03/dea185317c8a1cbc486670eb6aae4ef5-300x93.png) ## LSTM\u306e\u5b9f\u88c5 NumPy\u3092\u4f7f\u3063\u3066\u3001LSTM\u306e\u5b9f\u88c5\u3092forward pass, backward pass, optimization, training loop\u306e\u9806\u3067\u3084\u308a\u307e\u3059\u3002 ### LSTM\u306e\u521d\u671f\u5316 \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u521d\u671f\u5316\u3059\u308b\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: z_size = hidden_size + vocab_size def init_lstm(hidden_size, vocab_size, z_size): \"\"\" LSTM\u306e\u521d\u671f\u5316 \"\"\" W_f = np.random.randn(hidden_size, z_size) b_f = np.zeros((hidden_size, 1)) W_i = np.random.randn(hidden_size, z_size) b_i = np.zeros((hidden_size, 1)) W_g = np.random.randn(hidden_size, z_size) b_g = np.zeros((hidden_size, 1)) W_o = np.random.randn(hidden_size, z_size) b_o = np.zeros((hidden_size, 1)) W_v = np.random.randn(vocab_size, hidden_size) b_v = np.zeros((vocab_size, 1)) W_f = init_orthogonal(W_f) W_i = init_orthogonal(W_i) W_g = init_orthogonal(W_g) W_o = init_orthogonal(W_o) W_v = init_orthogonal(W_v) return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v","title":"\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u521d\u671f\u5316"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#forward-pass_1","text":"LSTM\u306e\u4ed5\u7d44\u307f\u306b\u3042\u308b\u30c7\u30fc\u30bf\u306e\u6d41\u308c\u901a\u308a\u306b\u5b9f\u88c5\u3057\u307e\u3059\u3002 ```Python: def forward(inputs, h_prev, C_prev, p): \"\"\" inputs:\u73fe\u5728\u306e\u5165\u529b h_prev:1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b C_prev:1\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u30bb\u30eb p:LSTM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf return \u5404\u30e2\u30b8\u30e5\u30fc\u30eb\u306e\u72b6\u614b\u3068\u51fa\u529b \"\"\" assert h_prev.shape == (hidden_size, 1) assert C_prev.shape == (hidden_size, 1) W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p x_s, z_s, f_s, i_s = [], [], [], [] g_s, C_s, o_s, h_s = [], [], [], [] v_s, output_s = [], [] h_s.append(h_prev) C_s.append(C_prev) for x in inputs: #\u5165\u529b\u30681\u30b9\u30c6\u30c3\u30d7\u524d\u306e\u51fa\u529b\u3092\u7d50\u5408 z = np.row_stack((h_prev, x)) z_s.append(z) #\u5fd8\u5374\u30b2\u30fc\u30c8 f = sigmoid(np.dot(W_f, z) + b_f) f_s.append(f) #\u5165\u529b\u30b2\u30fc\u30c8 i = sigmoid(np.dot(W_i, z) + b_i) i_s.append(i) #\u73fe\u5728\u306e\u5165\u529b\u306b\u5bfe\u3057\u3066\u30bb\u30eb\u306b\u52a0\u3048\u308b\u5019\u88dc g = tanh(np.dot(W_g, z) + b_g) g_s.append(g) #\u30bb\u30eb\u306e\u66f4\u65b0 C_prev = f * C_prev + i * g C_s.append(C_prev) #\u51fa\u529b\u30b2\u30fc\u30c8 o = sigmoid(np.dot(W_o, z) + b_o) o_s.append(o) #\u51fa\u529b\u3059\u308b h_prev = o * tanh(C_prev) h_s.append(h_prev) v = np.dot(W_v, h_prev) + b_v v_s.append(v) output = softmax(v) output_s.append(output) return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s ```","title":"forward pass\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#backward-pass_1","text":"\u640d\u5931\u3092\u6c42\u3081\u3066\u3001\u9006\u8aa4\u5dee\u4f1d\u64ad\u6cd5\u3067\u305d\u308c\u305e\u308c\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u5fae\u5206\u3057\u305f\u640d\u5931\u306e\u52fe\u914d\u3092\u6c42\u3081\u307e\u3059\u3002 Python: def backward(z, f, i, g, C, o, h, v, outputs, targets, p = params): W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p #\u52fe\u914d\u3092\u521d\u671f\u5316 W_f_d = np.zeros_like(W_f) b_f_d = np.zeros_like(b_f) W_i_d = np.zeros_like(W_i) b_i_d = np.zeros_like(b_i) W_g_d = np.zeros_like(W_g) b_g_d = np.zeros_like(b_g) W_o_d = np.zeros_like(W_o) b_o_d = np.zeros_like(b_o) W_v_d = np.zeros_like(W_v) b_v_d = np.zeros_like(b_v) #\u6b21\u306e\u30bb\u30eb\u3068\u96a0\u308c\u72b6\u614b\u3092\u521d\u671f\u5316 dh_next = np.zeros_like(h[0]) dC_next = np.zeros_like(C[0]) loss = 0 for t in reversed(range(len(outputs))): #\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u30ed\u30b9\u3092\u8a08\u7b97 loss += -np.mean(np.log(outputs[t]) * targets[t]) #\u524d\u306e\u30bb\u30eb\u3092\u66f4\u65b0 C_prev = C[t-1] dv = np.copy(outputs[t]) dv[np.argmax(targets[t])] -= 1 W_v_d += np.dot(dv, h[t].T) b_v_d += dv dh = np.dot(W_v.T, dv) dh += dh_next do = dh * tanh(C[t]) do = sigmoid(o[t], derivative=True)*do W_o_d += np.dot(do, z[t].T) b_o_d += do dC = np.copy(dC_next) dC += dh * o[t] * tanh(tanh(C[t]), derivative=True) dg = dC * i[t] dg = tanh(g[t], derivative=True) * dg W_g_d += np.dot(dg, z[t].T) b_g_d += dg di = dC * g[t] di = sigmoid(i[t], True) * di W_i_d += np.dot(di, z[t].T) b_i_d += di df = dC * C_prev df = sigmoid(f[t]) * df W_f_d += np.dot(df, z[t].T) b_f_d += df dz = (np.dot(W_f.T, df) + np.dot(W_i.T, di) + np.dot(W_g.T, dg) + np.dot(W_o.T, do)) dh_prev = dz[:hidden_size, :] dC_prev = f[t] * dC grads = W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d grads = clip_gradient_norm(grads) return loss, grads","title":"backward pass\u306e\u5b9f\u88c5"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_7","text":"\u5b9f\u88c5\u3057\u305fLSTM\u306e\u5b66\u7fd2\u3092\u884c\u3044\u307e\u3059\u3002Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: writer = SummaryWriter(log_dir=\"./logs/lstm\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a num_epochs = 200#\u30a8\u30dd\u30c3\u30af\u6570","title":"\u5b66\u7fd2"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#lstm","text":"z_size = hidden_size + vocab_size params = init_lstm(hidden_size, vocab_size, z_size)","title":"LSTM\u306e\u521d\u671f\u5316"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_8","text":"hidden_state = np.zeros((hidden_size, 1)) for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 h = np.zeros((hidden_size, 1)) c = np.zeros((hidden_size, 1)) #forward pass z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params) #backward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 loss, _ = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params) epoch_validation_loss += loss #train\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_one_hot = one_hot_encode_sequence(targets, vocab_size) #\u521d\u671f\u5316 h = np.zeros((hidden_size, 1)) c = np.zeros((hidden_size, 1)) #forward pass z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params) #backward pass \u4eca\u306ftraining\u306a\u306e\u3067Loss\u3068\u52fe\u914d\u3092\u8a08\u7b97 loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params) #LSTM\u306e\u66f4\u65b0 params = update_paramaters(params, grads, lr=1e-1) epoch_training_loss += loss writer.add_scalars(\"LSTM Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ![](http://namazu.tokyo/wp-content/uploads/2021/03/38636fedc882953549a18b2520823905-300x202.png) Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 RNN\u3068\u6bd4\u8f03\u3059\u308b\u3068\u3001\u5b66\u7fd2\u304c\u9032\u3080\u306b\u3064\u308c\u3066Loss\u304c\u3057\u3063\u304b\u308a\u3068\u4e0b\u304c\u3063\u3066\u3044\u308b\u306e\u3067\u5b89\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002 ## PyTorch\u3092\u7528\u3044\u305fLSTM\u306e\u5b9f\u88c5 \u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u3066LSTM\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3059\u3002 ### LSTM\u306e\u5b9a\u7fa9 \u307e\u305a\u3001LSTM\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5b9a\u7fa9\u3057\u307e\u3059\u3002 ```Python: import torch import torch.nn as nn import torch.nn.functional as F class MyLSTM(nn.Module): def __init__(self): super(MyLSTM, self).__init__() self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=50, num_layers=1, bidirectional=False) self.l_out = nn.Linear(in_features=50, out_features=vocab_size, bias=False) def forward(self, x): x, (h, c) = self.lstm(x) x = x.view(-1, self.lstm.hidden_size) x = self.l_out(x) return x","title":"\u96a0\u308c\u5c64\u306e\u521d\u671f\u5316"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_9","text":"\u5b66\u7fd2\u3059\u308b\u305f\u3081\u306e\u30eb\u30fc\u30d7\u3092\u66f8\u304d\u307e\u3059\u3002 \u30ed\u30b9\u95a2\u6570\u306f\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u3001optimizer\u306fSGD\u3092\u7528\u3044\u307e\u3057\u305f\u3002 numpy\u3092\u7528\u3044\u305f\u3068\u304d\u3068\u540c\u69d8\u3067\u3059\u3002 PyTorch\u3067\u306f\u3001\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u7528\u3044\u308b\u3068\u304d\u3001target\u306fone-hot vector\u306b\u3059\u308b\u306e\u3067\u306f\u306a\u304f1\u3067\u3042\u308b\u7b87\u6240(\u6b63\u89e3\u306e\u7b87\u6240)\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u6e21\u3059\u3060\u3051\u3067\u3088\u3044\u3067\u3059\u3002 Loss\u306e\u30b0\u30e9\u30d5\u306fTensorBoard\u3092\u4f7f\u7528\u3057\u3066\u63cf\u753b\u3057\u307e\u3057\u305f\u3002 ```Python: num_epochs = 200#\u30a8\u30dd\u30c3\u30af\u6570 net = MyLSTM()#LSTM\u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210 net = net.double()#\u578b\u3092float\u304b\u3089double\u306b\u5909\u63db criterion = nn.CrossEntropyLoss()#\u30af\u30ed\u30b9\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\u3092\u4f7f\u7528 optimizer = torch.optim.SGD(net.parameters(), lr=1e-1)#optimizer\u3092\u8a2d\u5b9a writer = SummaryWriter(log_dir=\"./logs/lstm_pytorch\")#SummaryWriter \u306e\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u3092\u751f\u6210 \u4fdd\u5b58\u3059\u308b\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3082\u6307\u5b9a for i in range(num_epochs): epoch_training_loss = 0 epoch_validation_loss = 0 net.eval()#\u30c6\u30b9\u30c8\u30e2\u30fc\u30c9 #validation\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in validation_set: #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_idx = [word_to_idx[word] for word in targets] inputs_one_hot = torch.from_numpy(inputs_one_hot) inputs_one_hot = inputs_one_hot.permute(0, 2, 1) targets_idx = torch.LongTensor(targets_idx) #forward pass \u4eca\u306fvalidation\u306a\u306e\u3067Loss\u306e\u307f\u3092\u8a08\u7b97 outputs = net(inputs_one_hot) loss = criterion(outputs, targets_idx) epoch_validation_loss += loss.item() net.train()#\u8a13\u7df4\u30e2\u30fc\u30c9 #train\u306e\u30eb\u30fc\u30d7 sentence\u3054\u3068\u306b\u30eb\u30fc\u30d7\u3092\u56de\u3059 for inputs, targets in training_set: optimizer.zero_grad()#\u52fe\u914d\u306e\u521d\u671f\u5316 #one-hot vector\u5316 inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size) targets_idx = [word_to_idx[word] for word in targets] inputs_one_hot = torch.from_numpy(inputs_one_hot) inputs_one_hot = inputs_one_hot.permute(0, 2, 1) targets_idx = torch.LongTensor(targets_idx) #forward pass outputs = net(inputs_one_hot) #loss\u306e\u8a08\u7b97 loss = criterion(outputs, targets_idx) #backward pass \u4eca\u306ftraining\u306a\u306e\u3067\u52fe\u914d\u3092\u8a08\u7b97 loss.backward() #LSTM\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u66f4\u65b0 optimizer.step() epoch_training_loss += loss.item() writer.add_scalars(\"LSTM PyTorch Loss\", {\"val\":epoch_validation_loss/len(validation_set), \"train\":epoch_training_loss/len(training_set)}, i) writer.close() ``` Loss\u306e\u30b0\u30e9\u30d5\u3067\u3059\u3002\u7dba\u9e97\u306b\u30d7\u30ed\u30c3\u30c8\u3067\u304d\u3066\u3044\u307e\u3059\u3002\u8d64\u304ctrain, \u9752\u304cval\u3092\u8868\u3057\u3066\u3044\u307e\u3059\u3002 \u5148\u307b\u3069\u306enumpy\u3067\u5b9f\u88c5\u3057\u305fLSTM\u3088\u308a\u3001\u30ed\u30b9\u304c\u3057\u3063\u304b\u308a\u3068\u4e0b\u304c\u3063\u3066\u3044\u307e\u3059\u3002\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3092\u4f7f\u3063\u305f\u65b9\u304c\u3088\u3044\u3067\u3059\u306d\u3002","title":"\u5b66\u7fd2"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_10","text":"\u4eca\u56de\u306fRNN\u3068LSTM\u3092\u7406\u89e3\u3059\u308b\u305f\u3081\u306b\u3001numpy\u306e\u5b9f\u88c5\u3092\u3057\u3066\u8efd\u3044\u5b9f\u9a13\u3092\u884c\u3044\u307e\u3057\u305f\u3002\u307e\u305f\u3001PyTorch\u3092\u7528\u3044\u3066LSTM\u306e\u5b9f\u88c5\u3092\u884c\u3044\u307e\u3057\u305f\u3002","title":"\u307e\u3068\u3081"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/lstm/#_11","text":"https://masamunetogetoge.com/gradient-vanish https://qiita.com/naoaki0802/items/7a11cded96f3a6165d01 http://kento1109.hatenablog.com/entry/2019/07/06/182247 https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca https://qiita.com/t_Signull/items/21b82be280b46f467d1b https://qiita.com/tanuk1647/items/276d2be36f5abb8ea52e","title":"\u53c2\u8003\u6587\u732e"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/video_task/","text":"\u52d5\u753b\u3092\u4f7f\u3063\u305f\u6df1\u5c64\u5b66\u7fd2 \u30df\u30b7\u30ac\u30f3\u5927\u5b66\u306e\u516c\u958b\u8b1b\u7fa9\u306b\u3068\u3066\u3082\u826f\u3044\u30af\u30e9\u30b9\u304c\u3042\u3063\u305f\u306e\u3067\u7ffb\u8a33and\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002 Lecture video \u30bf\u30b9\u30af \u52d5\u753b\u306f\u753b\u50cf\u3092\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3068\u3057\u305f (T x 3 x H x W) \u306e\u30c6\u30f3\u30b5\u30fc\u3067\u6271\u3046\u3002 (3 x T x H x W) \u306e\u6642\u3082\u3042\u308b\u3002 \u753b\u50cf\u7cfb\u306e\u30bf\u30b9\u30af\u306f\u4e3b\u306b Classification (whole image) Semantic Segmentation (Pixelwise Classification) Object Detection (Find bounding box) Instance Segmentation (Predict shape) \u304c\u3042\u308a\u3001\u52d5\u753b\u30c7\u30fc\u30bf\u306f\u3053\u308c\u3092\u9023\u7d9a\u7684\u306b\u884c\u3046\u3002 1. Video Classification \u52d5\u753b\u3092\u30af\u30e9\u30b9\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\u3001\u753b\u50cf\u3068\u9055\u3044\u7269\u4f53\u304c\u4f55\u304b\u5f53\u3066\u308b\u4ee5\u5916\u306b\u3082\u4e00\u9023\u306e\u52d5\u304d\u304b\u3089\u4eba\u9593\u306e\u52d5\u4f5c\u3092\u5206\u985e\u3057\u305f\u308a\u3059\u308b\u3002 \u52d5\u753b\u306e\u5168\u822c\u7684\u306a\u554f\u984c\u3068\u3057\u3066 \u30c7\u30fc\u30bf\u304c\u5927\u304d\u3059\u304e\u308b \u3002\u7121\u5727\u7e2e\u3060\u30681\u30a8\u30f3\u30c8\u30ea\u30fc1 byte\u3068\u3057\u3066HD\u52d5\u753b\uff11\u5206\u306710GB\u304f\u3089\u3044\u3002\u306a\u306e\u3067FPS\u3068\u89e3\u50cf\u5ea6\u3092\u4e0b\u3052\u305f\u52d5\u753b\u306e\u4e00\u90e8\u5206\u3060\u3051\u3092\u5b66\u7fd2\u3055\u305b\u308b\u3002\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u306f\u52d5\u753b\u306e\u4e00\u90e8\u3092\u30aa\u30fc\u30d0\u30fc\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u5e73\u5747\u7cbe\u5ea6\u3092\u53d6\u308b\u3053\u3068\u3067\u52d5\u753b\u5168\u4f53\u3078\u306e\u7cbe\u5ea6\u304c\u308f\u304b\u308b\u3002 1-1. Single Frame CNN \u5404\u30d5\u30ec\u30fc\u30e0\u3092\u5225\u3005\u306e\u753b\u50cf\u3068\u3057\u3066\u72ec\u7acb\u3057\u305fCNN\u3067\u5206\u985e\u3059\u308b\u624b\u6cd5\u3002\u9577\u3081\u306e\u52d5\u753b\u3092\u5b66\u7fd2\u3055\u305b\u3066\u6b63\u7b54\u7387\u3092\u30d5\u30ec\u30fc\u30e0\u6570\u3067\u5e73\u5747\u3057\u305f\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u3066\u3044\u304f\u3002\u753b\u50cf\u540c\u58eb\u306e\u95a2\u4fc2\u3092\u8003\u616e\u3057\u306a\u3044\u306e\u3067\u4e00\u898b\u3059\u308b\u3068\u30c0\u30e1\u306a\u30e2\u30c7\u30eb\u3063\u307d\u3044\u304c\u5b9f\u306f\u666e\u901a\u306b\u5b9f\u7528\u30ec\u30d9\u30eb\u3067\u6027\u80fd\u304c\u826f\u3044\u3089\u3057\u3044\u3002\u52d5\u753b\u5206\u985e\u3092\u3059\u308b\u6642\u306f\u3053\u308c\u3092\u30d9\u30fc\u30b9\u306b\u3084\u308b\u3068\u3044\u3044\u3002 img src=../imgs/3.png width=400> ### 1-2. Late Fusion \u4e0a\u306e\u30e2\u30c7\u30eb\u3092MLP\u306b\u63a5\u7d9a\u3057\u305f\u30e2\u30c7\u30eb\u3002\u30d5\u30ec\u30fc\u30e0\u6bce\u306e\u7279\u5fb4\u91cf\u3092\u4e00\u3064\u306b\u7d71\u5408\u3059\u308b\u306e\u3067\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3067\u5b66\u7fd2\u3067\u304d\u3066\u30d5\u30ec\u30fc\u30e0\u540c\u58eb\u306e\u95a2\u4fc2\u3082\u5b66\u7fd2\u3067\u304d\u308b\u304cFC\u5c64\u3067\u904e\u5b66\u7fd2\u3092\u8d77\u3053\u3057\u3084\u3059\u3044\u3002\u5bfe\u7b56\u3068\u3057\u3066\u7279\u5fb4\u91cf\u306e\u7d71\u5408\u6642\u306b`T, H, W`\u3092Pooling\u3057\u3066`D`\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u843d\u3068\u3057\u8fbc\u3080\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u3082\u30b7\u30f3\u30d7\u30eb\u306a\u306e\u3067\u307e\u305a\u8a66\u3057\u305f\u3044\u30e2\u30c7\u30eb\u3002 \u3057\u304b\u3057\u7279\u5fb4\u91cf\u3092\u7d71\u5408\u3059\u308b\u306e\u304c\u9045\u3044\u306e\u3067\u3001**\u52d5\u753b\u306e\u5c0f\u3055\u3044\u52d5\u304d\u3092\u5b66\u7fd2\u3067\u304d\u306a\u3044**\u3002\u4f8b\u3048\u3070\u3001\u4eba\u304c\u6b69\u3044\u3066\u308b\u304b\u7acb\u3063\u3066\u3044\u308b\u304b\u306f\u8db3\u306e\u5c0f\u3055\u306a\u52d5\u304d\u3067\u5224\u65ad\u3059\u308b\u304c\u3001FC\u5c64\u306b\u6765\u308b\u524d\u306b\u3053\u308c\u3089\u306e\u5c0f\u3055\u3044\u7279\u5fb4\u91cf\u306f\u5931\u308f\u308c\u3084\u3059\u3044\u3002 ### 1-3. Early Fusion \u5148\u306b\u5404\u30d5\u30ec\u30fc\u30e0\u306e\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001`(3 x T x H x W)`\u306e\u52d5\u753b\u3092`(3T x H x W)`\u306b\u306a\u308b\u3088\u3046\u306b\u7573\u307f\u8fbc\u307f\uff08\u7279\u5fb4\u91cf\u306e\u7d71\u5408\uff09\u3057\u3066\u304b\u3089\u30013\u6b21\u5143\u30c6\u30f3\u30b5\u30fc\u3068\u3057\u3066\u304b\u30892DCNN\u3067\u5b66\u7fd2\u3059\u308b\u30e2\u30c7\u30eb\u3002\u30d4\u30af\u30bb\u30eb\u306e\u7d30\u304b\u3044\u5909\u5316\u3082\uff12DCNN\u304c\u5b66\u7fd2\u3067\u304d\u308b\u304c\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u4e00\u56de\u306e\u7573\u307f\u8fbc\u307f\u3067\u51e6\u7406\u3059\u308b\u306e\u3067\u305d\u3053\u3089\u8fba\u306e\u7279\u5fb4\u91cf\u306f\u4e0a\u624b\u304f\u8868\u73fe\u3055\u308c\u306a\u3044\u3002 ### 1-3. Slow Fusion Early Fusion\u3067\u306f\u7e26\u6a2a\u3067\u306e\u7573\u307f\u8fbc\u307f\u3057\u304b\u884c\u308f\u306a\u305a\u6642\u7cfb\u5217\u306e\u60c5\u5831\u306f\u5931\u308f\u308c\u3066\u3044\u305f\u304c\u3001\u7e26\u6a2a\u306b\u52a0\u3048\u6642\u9593\u8ef8\u3082\u52a0\u3048\u305f3\u6b21\u5143\u3067\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u3001\u6642\u9593\u3068\u7a7a\u9593\u306e\u7279\u5fb4\u3092\u5f90\u3005\u306b\u7d71\u5408\u3057\u3066\u3044\u304f\u624b\u6cd5\u3002 ### 1-4.\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 * **Sport-1M**: YouTube\u306e100\u4e07\u679a\u306e\u52d5\u753b\u306b487\u306e\u30b9\u30dd\u30fc\u30c4\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b --- ## \u5404\u624b\u6cd5\u306e\u6bd4\u8f03 * Single Frame \u30b7\u30f3\u30d7\u30eb\u3060\u304c\u7d71\u5408\u7cfb\u306b\u52a3\u3089\u306a\u3044\u6027\u80fd\u3001\u3068\u308a\u3042\u3048\u305a\u3053\u308c\u3092\u6700\u521d\u306b\u8a66\u305d\u3046\u3002 * Early \u7e26\u6a2a\u3067\u7573\u307f\u8fbc\u3080\u70ba\u753b\u50cf\u3068\u3057\u3066\u306e\u5b66\u7fd2\u306f\u5f37\u3044\u304c\u3001**Time shift invavriant**\u3058\u3083\u306a\u3044\u3068\u3044\u3046\u5f31\u70b9\u3082\u3042\u308b\u3002 \u3053\u308c\u306f\u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u5965\u884c\u304d\u304c\u305d\u306e\u30d4\u30af\u30bb\u30eb\u306e\u6642\u9593\u306b\u3088\u308b\u5909\u5316\u3092\u5b66\u7fd2\u3057\u3061\u3083\u3046\u306e\u304c\u539f\u56e0\u3067\u3001\u4f8b\u3048\u3070\u6700\u521d\u306e\u65b9\u3067\u4eba\u304c\u7acb\u3061\u4e0a\u304c\u308b\u52d5\u753b\u3092\u5b66\u7fd2\u3057\u305f\u3068\u3057\u3066\u3001\u305d\u306e\u52d5\u753b\u3092\u6570\u30d5\u30ec\u30fc\u30e0\u305a\u3089\u3057\u305f\u5834\u5408\u30d5\u30a3\u30eb\u30bf\u30fc\u3082\u5965\u306b\u305d\u306e\u30d5\u30ec\u30fc\u30e0\u5206\u30ba\u30ec\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 * Slow \u7e26\u6a2a\u3068\u6642\u9593\u8ef8\u3067\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u52d5\u304b\u3059\u306e\u3067time shift-invariant\u306b\u306a\u308b\u3002\u5165\u529b`(Cin x T x H x W)`\u3092`(Cin x Cout x 3 x 3 x 3)`\u30d5\u30a3\u30eb\u30bf\u30fc\u3067\u51e6\u7406\u3059\u308c\u3070(\u4e09\u3064\u306e\uff13\u306fTHW\u7528)\u540c\u3058\u5f62\u3067\u51fa\u529b\u3059\u308b\u306e\u3067\u51fa\u529b\u3082\u898b\u3048\u308b\u5316\u3067\u304d\u308b\u30023DCNN\u306f\u7d76\u8cdb\u9032\u5316\u4e2d\u306a\u6a21\u69d8\u3002Conv2D\u3092Conv3D\u306b\u3057\u305fVGG**C3D**\u304c\u9ad8\u6027\u80fd\u3060\u304c\u8a08\u7b97\u91cf\u306f\u975e\u5e38\u306b\u591a\u3044\u30023D-ResNet\u3067\u4e00\u6c17\u306b\u30c7\u30a3\u30fc\u30d7\u306b\u3002 ## 2. \u884c\u52d5\u8a8d\u8b58 \u4eba\u306e\u52d5\u304d\u304b\u3089\u4f55\u3092\u3057\u3066\u3044\u308b\u304b\u4e88\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 ### 2-1. Optical Flow \u5404\u30d5\u30ec\u30fc\u30e0\u306e\u753b\u50cf\u7279\u5fb4\u91cf\u306b\u8ffd\u52a0\u3067\u9023\u7d9a\u3057\u305f2\u30d5\u30ec\u30fc\u30e0\u9593\u306e\u7269\u4f53\u306e\u52d5\u304d\u306e\u898b\u3048\u65b9\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u3059\u308b\uff08\u30d4\u30af\u30bb\u30eb\u306e\u52d5\u304d\u3068\u8a00\u3046\u3079\u304d\u304b\uff09\u3002\u524d\u8005\u3092Spatial stream, \u5f8c\u8005\u3092Temporal stream\u3068\u3044\u3046\u5225\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b66\u7fd2\u3057\u30af\u30e9\u30b9\u30b9\u30b3\u30a2\u3092\u5e73\u5747\u3067\u7d71\u5408\u3059\u308b\u3002 \u5c64\u304c\u6df1\u3044\u30e2\u30c7\u30eb\u304c\u3053\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u306b\u5bfe\u5fdc\u3057\u305f\u3089\u3084\u3070\u3044\u3089\u3057\u3044\u3002","title":"\u52d5\u753b\u3092\u4f7f\u3063\u305f\u6df1\u5c64\u5b66\u7fd2"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/video_task/#_1","text":"\u30df\u30b7\u30ac\u30f3\u5927\u5b66\u306e\u516c\u958b\u8b1b\u7fa9\u306b\u3068\u3066\u3082\u826f\u3044\u30af\u30e9\u30b9\u304c\u3042\u3063\u305f\u306e\u3067\u7ffb\u8a33and\u307e\u3068\u3081\u3066\u304a\u304d\u307e\u3059\u3002 Lecture video","title":"\u52d5\u753b\u3092\u4f7f\u3063\u305f\u6df1\u5c64\u5b66\u7fd2"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/video_task/#_2","text":"\u52d5\u753b\u306f\u753b\u50cf\u3092\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3068\u3057\u305f (T x 3 x H x W) \u306e\u30c6\u30f3\u30b5\u30fc\u3067\u6271\u3046\u3002 (3 x T x H x W) \u306e\u6642\u3082\u3042\u308b\u3002 \u753b\u50cf\u7cfb\u306e\u30bf\u30b9\u30af\u306f\u4e3b\u306b Classification (whole image) Semantic Segmentation (Pixelwise Classification) Object Detection (Find bounding box) Instance Segmentation (Predict shape) \u304c\u3042\u308a\u3001\u52d5\u753b\u30c7\u30fc\u30bf\u306f\u3053\u308c\u3092\u9023\u7d9a\u7684\u306b\u884c\u3046\u3002","title":"\u30bf\u30b9\u30af"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/video_task/#1-video-classification","text":"\u52d5\u753b\u3092\u30af\u30e9\u30b9\u5206\u985e\u3059\u308b\u30bf\u30b9\u30af\u3001\u753b\u50cf\u3068\u9055\u3044\u7269\u4f53\u304c\u4f55\u304b\u5f53\u3066\u308b\u4ee5\u5916\u306b\u3082\u4e00\u9023\u306e\u52d5\u304d\u304b\u3089\u4eba\u9593\u306e\u52d5\u4f5c\u3092\u5206\u985e\u3057\u305f\u308a\u3059\u308b\u3002 \u52d5\u753b\u306e\u5168\u822c\u7684\u306a\u554f\u984c\u3068\u3057\u3066 \u30c7\u30fc\u30bf\u304c\u5927\u304d\u3059\u304e\u308b \u3002\u7121\u5727\u7e2e\u3060\u30681\u30a8\u30f3\u30c8\u30ea\u30fc1 byte\u3068\u3057\u3066HD\u52d5\u753b\uff11\u5206\u306710GB\u304f\u3089\u3044\u3002\u306a\u306e\u3067FPS\u3068\u89e3\u50cf\u5ea6\u3092\u4e0b\u3052\u305f\u52d5\u753b\u306e\u4e00\u90e8\u5206\u3060\u3051\u3092\u5b66\u7fd2\u3055\u305b\u308b\u3002\u30e2\u30c7\u30eb\u306e\u8a55\u4fa1\u306f\u52d5\u753b\u306e\u4e00\u90e8\u3092\u30aa\u30fc\u30d0\u30fc\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0\u3057\u3066\u5e73\u5747\u7cbe\u5ea6\u3092\u53d6\u308b\u3053\u3068\u3067\u52d5\u753b\u5168\u4f53\u3078\u306e\u7cbe\u5ea6\u304c\u308f\u304b\u308b\u3002","title":"1. Video Classification"},{"location":"%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/video_task/#1-1-single-frame-cnn","text":"\u5404\u30d5\u30ec\u30fc\u30e0\u3092\u5225\u3005\u306e\u753b\u50cf\u3068\u3057\u3066\u72ec\u7acb\u3057\u305fCNN\u3067\u5206\u985e\u3059\u308b\u624b\u6cd5\u3002\u9577\u3081\u306e\u52d5\u753b\u3092\u5b66\u7fd2\u3055\u305b\u3066\u6b63\u7b54\u7387\u3092\u30d5\u30ec\u30fc\u30e0\u6570\u3067\u5e73\u5747\u3057\u305f\u7cbe\u5ea6\u3092\u5411\u4e0a\u3055\u305b\u3066\u3044\u304f\u3002\u753b\u50cf\u540c\u58eb\u306e\u95a2\u4fc2\u3092\u8003\u616e\u3057\u306a\u3044\u306e\u3067\u4e00\u898b\u3059\u308b\u3068\u30c0\u30e1\u306a\u30e2\u30c7\u30eb\u3063\u307d\u3044\u304c\u5b9f\u306f\u666e\u901a\u306b\u5b9f\u7528\u30ec\u30d9\u30eb\u3067\u6027\u80fd\u304c\u826f\u3044\u3089\u3057\u3044\u3002\u52d5\u753b\u5206\u985e\u3092\u3059\u308b\u6642\u306f\u3053\u308c\u3092\u30d9\u30fc\u30b9\u306b\u3084\u308b\u3068\u3044\u3044\u3002 img src=../imgs/3.png width=400> ### 1-2. Late Fusion \u4e0a\u306e\u30e2\u30c7\u30eb\u3092MLP\u306b\u63a5\u7d9a\u3057\u305f\u30e2\u30c7\u30eb\u3002\u30d5\u30ec\u30fc\u30e0\u6bce\u306e\u7279\u5fb4\u91cf\u3092\u4e00\u3064\u306b\u7d71\u5408\u3059\u308b\u306e\u3067\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u3067\u5b66\u7fd2\u3067\u304d\u3066\u30d5\u30ec\u30fc\u30e0\u540c\u58eb\u306e\u95a2\u4fc2\u3082\u5b66\u7fd2\u3067\u304d\u308b\u304cFC\u5c64\u3067\u904e\u5b66\u7fd2\u3092\u8d77\u3053\u3057\u3084\u3059\u3044\u3002\u5bfe\u7b56\u3068\u3057\u3066\u7279\u5fb4\u91cf\u306e\u7d71\u5408\u6642\u306b`T, H, W`\u3092Pooling\u3057\u3066`D`\u6b21\u5143\u30c7\u30fc\u30bf\u306b\u843d\u3068\u3057\u8fbc\u3080\u3002\u3053\u308c\u3089\u306e\u624b\u6cd5\u3082\u30b7\u30f3\u30d7\u30eb\u306a\u306e\u3067\u307e\u305a\u8a66\u3057\u305f\u3044\u30e2\u30c7\u30eb\u3002 \u3057\u304b\u3057\u7279\u5fb4\u91cf\u3092\u7d71\u5408\u3059\u308b\u306e\u304c\u9045\u3044\u306e\u3067\u3001**\u52d5\u753b\u306e\u5c0f\u3055\u3044\u52d5\u304d\u3092\u5b66\u7fd2\u3067\u304d\u306a\u3044**\u3002\u4f8b\u3048\u3070\u3001\u4eba\u304c\u6b69\u3044\u3066\u308b\u304b\u7acb\u3063\u3066\u3044\u308b\u304b\u306f\u8db3\u306e\u5c0f\u3055\u306a\u52d5\u304d\u3067\u5224\u65ad\u3059\u308b\u304c\u3001FC\u5c64\u306b\u6765\u308b\u524d\u306b\u3053\u308c\u3089\u306e\u5c0f\u3055\u3044\u7279\u5fb4\u91cf\u306f\u5931\u308f\u308c\u3084\u3059\u3044\u3002 ### 1-3. Early Fusion \u5148\u306b\u5404\u30d5\u30ec\u30fc\u30e0\u306e\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u306e\u3067\u306f\u306a\u304f\u3001`(3 x T x H x W)`\u306e\u52d5\u753b\u3092`(3T x H x W)`\u306b\u306a\u308b\u3088\u3046\u306b\u7573\u307f\u8fbc\u307f\uff08\u7279\u5fb4\u91cf\u306e\u7d71\u5408\uff09\u3057\u3066\u304b\u3089\u30013\u6b21\u5143\u30c6\u30f3\u30b5\u30fc\u3068\u3057\u3066\u304b\u30892DCNN\u3067\u5b66\u7fd2\u3059\u308b\u30e2\u30c7\u30eb\u3002\u30d4\u30af\u30bb\u30eb\u306e\u7d30\u304b\u3044\u5909\u5316\u3082\uff12DCNN\u304c\u5b66\u7fd2\u3067\u304d\u308b\u304c\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u4e00\u56de\u306e\u7573\u307f\u8fbc\u307f\u3067\u51e6\u7406\u3059\u308b\u306e\u3067\u305d\u3053\u3089\u8fba\u306e\u7279\u5fb4\u91cf\u306f\u4e0a\u624b\u304f\u8868\u73fe\u3055\u308c\u306a\u3044\u3002 ### 1-3. Slow Fusion Early Fusion\u3067\u306f\u7e26\u6a2a\u3067\u306e\u7573\u307f\u8fbc\u307f\u3057\u304b\u884c\u308f\u306a\u305a\u6642\u7cfb\u5217\u306e\u60c5\u5831\u306f\u5931\u308f\u308c\u3066\u3044\u305f\u304c\u3001\u7e26\u6a2a\u306b\u52a0\u3048\u6642\u9593\u8ef8\u3082\u52a0\u3048\u305f3\u6b21\u5143\u3067\u7573\u307f\u8fbc\u307f\u3092\u884c\u3044\u3001\u6642\u9593\u3068\u7a7a\u9593\u306e\u7279\u5fb4\u3092\u5f90\u3005\u306b\u7d71\u5408\u3057\u3066\u3044\u304f\u624b\u6cd5\u3002 ### 1-4.\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8 * **Sport-1M**: YouTube\u306e100\u4e07\u679a\u306e\u52d5\u753b\u306b487\u306e\u30b9\u30dd\u30fc\u30c4\u306e\u30ab\u30c6\u30b4\u30ea\u30fc\u304c\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b --- ## \u5404\u624b\u6cd5\u306e\u6bd4\u8f03 * Single Frame \u30b7\u30f3\u30d7\u30eb\u3060\u304c\u7d71\u5408\u7cfb\u306b\u52a3\u3089\u306a\u3044\u6027\u80fd\u3001\u3068\u308a\u3042\u3048\u305a\u3053\u308c\u3092\u6700\u521d\u306b\u8a66\u305d\u3046\u3002 * Early \u7e26\u6a2a\u3067\u7573\u307f\u8fbc\u3080\u70ba\u753b\u50cf\u3068\u3057\u3066\u306e\u5b66\u7fd2\u306f\u5f37\u3044\u304c\u3001**Time shift invavriant**\u3058\u3083\u306a\u3044\u3068\u3044\u3046\u5f31\u70b9\u3082\u3042\u308b\u3002 \u3053\u308c\u306f\u30d5\u30a3\u30eb\u30bf\u30fc\u306e\u5965\u884c\u304d\u304c\u305d\u306e\u30d4\u30af\u30bb\u30eb\u306e\u6642\u9593\u306b\u3088\u308b\u5909\u5316\u3092\u5b66\u7fd2\u3057\u3061\u3083\u3046\u306e\u304c\u539f\u56e0\u3067\u3001\u4f8b\u3048\u3070\u6700\u521d\u306e\u65b9\u3067\u4eba\u304c\u7acb\u3061\u4e0a\u304c\u308b\u52d5\u753b\u3092\u5b66\u7fd2\u3057\u305f\u3068\u3057\u3066\u3001\u305d\u306e\u52d5\u753b\u3092\u6570\u30d5\u30ec\u30fc\u30e0\u305a\u3089\u3057\u305f\u5834\u5408\u30d5\u30a3\u30eb\u30bf\u30fc\u3082\u5965\u306b\u305d\u306e\u30d5\u30ec\u30fc\u30e0\u5206\u30ba\u30ec\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 * Slow \u7e26\u6a2a\u3068\u6642\u9593\u8ef8\u3067\u30d5\u30a3\u30eb\u30bf\u30fc\u3092\u52d5\u304b\u3059\u306e\u3067time shift-invariant\u306b\u306a\u308b\u3002\u5165\u529b`(Cin x T x H x W)`\u3092`(Cin x Cout x 3 x 3 x 3)`\u30d5\u30a3\u30eb\u30bf\u30fc\u3067\u51e6\u7406\u3059\u308c\u3070(\u4e09\u3064\u306e\uff13\u306fTHW\u7528)\u540c\u3058\u5f62\u3067\u51fa\u529b\u3059\u308b\u306e\u3067\u51fa\u529b\u3082\u898b\u3048\u308b\u5316\u3067\u304d\u308b\u30023DCNN\u306f\u7d76\u8cdb\u9032\u5316\u4e2d\u306a\u6a21\u69d8\u3002Conv2D\u3092Conv3D\u306b\u3057\u305fVGG**C3D**\u304c\u9ad8\u6027\u80fd\u3060\u304c\u8a08\u7b97\u91cf\u306f\u975e\u5e38\u306b\u591a\u3044\u30023D-ResNet\u3067\u4e00\u6c17\u306b\u30c7\u30a3\u30fc\u30d7\u306b\u3002 ## 2. \u884c\u52d5\u8a8d\u8b58 \u4eba\u306e\u52d5\u304d\u304b\u3089\u4f55\u3092\u3057\u3066\u3044\u308b\u304b\u4e88\u6e2c\u3059\u308b\u30bf\u30b9\u30af\u3002 ### 2-1. Optical Flow \u5404\u30d5\u30ec\u30fc\u30e0\u306e\u753b\u50cf\u7279\u5fb4\u91cf\u306b\u8ffd\u52a0\u3067\u9023\u7d9a\u3057\u305f2\u30d5\u30ec\u30fc\u30e0\u9593\u306e\u7269\u4f53\u306e\u52d5\u304d\u306e\u898b\u3048\u65b9\u306e\u30d1\u30bf\u30fc\u30f3\u3092\u5b66\u7fd2\u3059\u308b\uff08\u30d4\u30af\u30bb\u30eb\u306e\u52d5\u304d\u3068\u8a00\u3046\u3079\u304d\u304b\uff09\u3002\u524d\u8005\u3092Spatial stream, \u5f8c\u8005\u3092Temporal stream\u3068\u3044\u3046\u5225\u306e\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3067\u5b66\u7fd2\u3057\u30af\u30e9\u30b9\u30b9\u30b3\u30a2\u3092\u5e73\u5747\u3067\u7d71\u5408\u3059\u308b\u3002 \u5c64\u304c\u6df1\u3044\u30e2\u30c7\u30eb\u304c\u3053\u306e\u30a2\u30fc\u30ad\u30c6\u30af\u30c1\u30e3\u30fc\u306b\u5bfe\u5fdc\u3057\u305f\u3089\u3084\u3070\u3044\u3089\u3057\u3044\u3002","title":"1-1. Single Frame CNN"}]}